# åˆçº§DevOpså·¥ç¨‹å¸ˆå®Œæ•´å­¦ä¹ æŒ‡å—

> åŸºäº20å¹´DevOpså®æˆ˜ç»éªŒæ•´ç†çš„ç³»ç»Ÿæ€§å­¦ä¹ è·¯å¾„

## ğŸ“š ç›®å½•

- [å­¦ä¹ è·¯å¾„æ¦‚è§ˆ](#å­¦ä¹ è·¯å¾„æ¦‚è§ˆ)
- [é˜¶æ®µä¸€ï¼šåŸºç¡€ç¯å¢ƒæ­å»º](#é˜¶æ®µä¸€åŸºç¡€ç¯å¢ƒæ­å»º)
- [é˜¶æ®µäºŒï¼šå·¥å…·åŸºç¡€æŒæ¡](#é˜¶æ®µäºŒå·¥å…·åŸºç¡€æŒæ¡)
- [é˜¶æ®µä¸‰ï¼šå®è·µåº”ç”¨éƒ¨ç½²](#é˜¶æ®µä¸‰å®è·µåº”ç”¨éƒ¨ç½²)
- [é˜¶æ®µå››ï¼šéƒ¨ç½²è¿ç»´è‡ªåŠ¨åŒ–](#é˜¶æ®µå››éƒ¨ç½²è¿ç»´è‡ªåŠ¨åŒ–)
- [é˜¶æ®µäº”ï¼šé«˜çº§è¿ç»´ç›‘æ§](#é˜¶æ®µäº”é«˜çº§è¿ç»´ç›‘æ§)
- [å­¦ä¹ æ—¶é—´è§„åˆ’](#å­¦ä¹ æ—¶é—´è§„åˆ’)
- [å®è·µé¡¹ç›®å»ºè®®](#å®è·µé¡¹ç›®å»ºè®®)

---

## å­¦ä¹ è·¯å¾„æ¦‚è§ˆ

### ä¼˜å…ˆçº§åˆ†çº§è¯´æ˜

| ä¼˜å…ˆçº§ | é˜¶æ®µ | å†…å®¹ | é‡è¦æ€§ | é¢„ä¼°æ—¶é—´ |
|--------|------|------|--------|----------|
| **P1** | åŸºç¡€ç¯å¢ƒ | ç³»ç»Ÿå®‰è£…é…ç½® | â­â­â­â­â­ | 1-2å‘¨ |
| **P2** | å·¥å…·åŸºç¡€ | æ ¸å¿ƒå·¥å…·æŒæ¡ | â­â­â­â­â­ | 2-3å‘¨ |
| **P3** | å®è·µåº”ç”¨ | åº”ç”¨éƒ¨ç½²éªŒè¯ | â­â­â­â­ | 2-3å‘¨ |
| **P4** | éƒ¨ç½²è¿ç»´ | è‡ªåŠ¨åŒ–æµç¨‹ | â­â­â­â­ | 3-4å‘¨ |
| **P5** | é«˜çº§è¿ç»´ | ç›‘æ§å‘Šè­¦ | â­â­â­ | 2-3å‘¨ |

### æ ¸å¿ƒæŠ€èƒ½æ ‘

```
DevOpså·¥ç¨‹å¸ˆ
â”œâ”€â”€ åŸºç¡€æŠ€èƒ½
â”‚   â”œâ”€â”€ Linuxç³»ç»Ÿç®¡ç†
â”‚   â”œâ”€â”€ ç½‘ç»œé…ç½®
â”‚   â””â”€â”€ Shellè„šæœ¬
â”œâ”€â”€ å®¹å™¨åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ Docker
â”‚   â”œâ”€â”€ Kubernetes
â”‚   â””â”€â”€ Helm
â”œâ”€â”€ CI/CDæµç¨‹
â”‚   â”œâ”€â”€ Jenkins
â”‚   â”œâ”€â”€ GitLab CI
â”‚   â””â”€â”€ GitHub Actions
â”œâ”€â”€ ç›‘æ§è¿ç»´
â”‚   â”œâ”€â”€ Prometheus
â”‚   â”œâ”€â”€ Grafana
â”‚   â””â”€â”€ ELK Stack
â””â”€â”€ è‡ªåŠ¨åŒ–è„šæœ¬
    â”œâ”€â”€ Bash
    â”œâ”€â”€ Python
    â””â”€â”€ Ansible
```

---

## é˜¶æ®µä¸€ï¼šåŸºç¡€ç¯å¢ƒæ­å»º

> **ç›®æ ‡**: æ­å»ºç¨³å®šçš„å­¦ä¹ å’Œå®è·µç¯å¢ƒ
> **æ—¶é—´**: 1-2å‘¨
> **ä¼˜å…ˆçº§**: P1 â­â­â­â­â­

### 1.1 æ¨èå­¦ä¹ ç¯å¢ƒ

#### ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®è¦æ±‚

**æœ€ä½é…ç½®:**
- CPU: 4æ ¸å¿ƒ
- å†…å­˜: 8GB RAM
- å­˜å‚¨: 100GB SSD
- ç½‘ç»œ: ç¨³å®šäº’è”ç½‘è¿æ¥

**æ¨èé…ç½®:**
- CPU: 8æ ¸å¿ƒ
- å†…å­˜: 16GB+ RAM
- å­˜å‚¨: 250GB+ SSD
- ç½‘ç»œ: åƒå…†ç½‘ç»œ

#### ğŸ—ï¸ è™šæ‹ŸåŒ–å¹³å°é€‰æ‹©

**æ–¹æ¡ˆä¸€: æœ¬åœ°è™šæ‹Ÿæœº (æ¨èåˆå­¦è€…)**
```bash
# è™šæ‹ŸåŒ–è½¯ä»¶é€‰æ‹©
VMware Workstation Pro    # åŠŸèƒ½æœ€å…¨ï¼Œæ€§èƒ½æœ€ä½³
VirtualBox (å…è´¹)         # å¼€æºå…è´¹ï¼ŒåŠŸèƒ½å¤Ÿç”¨
Hyper-V (Windows)         # Windowså†…ç½®ï¼Œé›†æˆåº¦é«˜
```

**æ–¹æ¡ˆäºŒ: äº‘å¹³å° (æ¨èè¿›é˜¶å­¦ä¹ )**
```bash
# äº‘æœåŠ¡å•†æ¨è
AWS Free Tier            # 12ä¸ªæœˆå…è´¹é¢åº¦
Google Cloud Platform    # $300å…è´¹è¯•ç”¨
Azure Student           # å­¦ç”Ÿå…è´¹è´¦å·
è…¾è®¯äº‘/é˜¿é‡Œäº‘             # å›½å†…è®¿é—®é€Ÿåº¦å¿«
```

#### ğŸ›ï¸ æ¶æ„è®¾è®¡

**åŸºç¡€ç»ƒä¹ æ¶æ„ (3èŠ‚ç‚¹):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MasterèŠ‚ç‚¹  â”‚    â”‚ WorkerèŠ‚ç‚¹  â”‚    â”‚ MonitorèŠ‚ç‚¹ â”‚
â”‚ 192.168.1.10â”‚    â”‚ 192.168.1.11â”‚    â”‚ 192.168.1.12â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ â€¢ K8s Masterâ”‚    â”‚ â€¢ K8s Workerâ”‚    â”‚ â€¢ Prometheusâ”‚
â”‚ â€¢ Jenkins   â”‚    â”‚ â€¢ Docker    â”‚    â”‚ â€¢ Grafana   â”‚
â”‚ â€¢ Git       â”‚    â”‚ â€¢ Apps      â”‚    â”‚ â€¢ ELK Stack â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ“ä½œç³»ç»Ÿå®‰è£…æµç¨‹

#### ğŸ“¥ ç³»ç»Ÿé€‰æ‹©ä¸ä¸‹è½½

**æ¨èç³»ç»Ÿ: Ubuntu Server 22.04 LTS**

```bash
# ä¸‹è½½é“¾æ¥
https://ubuntu.com/download/server

# éªŒè¯ISOæ–‡ä»¶
sha256sum ubuntu-22.04.3-live-server-amd64.iso
```

#### ğŸ› ï¸ è¯¦ç»†å®‰è£…æ­¥éª¤

**Step 1: åˆ›å»ºè™šæ‹Ÿæœº**
```bash
# VMwareé…ç½®
åç§°: devops-master
å†…å­˜: 4GB (æ¨è8GB)
ç¡¬ç›˜: 50GB (åŠ¨æ€åˆ†é…)
ç½‘ç»œ: æ¡¥æ¥æ¨¡å¼
```

**Step 2: å¯åŠ¨å®‰è£…ç¨‹åº**
```bash
1. é€‰æ‹©è¯­è¨€: English
2. é€‰æ‹©é”®ç›˜å¸ƒå±€: English (US)
3. ç½‘ç»œé…ç½®: é…ç½®é™æ€IP
   - IP: 192.168.1.10/24
   - Gateway: 192.168.1.1
   - DNS: 8.8.8.8, 1.1.1.1
```

**Step 3: ç£ç›˜åˆ†åŒº**
```bash
# æ¨èåˆ†åŒºæ–¹æ¡ˆ
/         30GB  (æ ¹åˆ†åŒº)
/var      15GB  (æ—¥å¿—å’Œæ•°æ®)
/home     3GB   (ç”¨æˆ·ç›®å½•)
swap      2GB   (äº¤æ¢åˆ†åŒº)
```

**Step 4: ç”¨æˆ·é…ç½®**
```bash
ç”¨æˆ·å: devops
å¯†ç : è®¾ç½®å¼ºå¯†ç 
æœåŠ¡: âœ“ Install OpenSSH server
```

**Step 5: è½¯ä»¶åŒ…é€‰æ‹©**
```bash
é€‰æ‹©: Ubuntu Server (minimal)
# åŸºç¡€å®‰è£…ï¼Œåç»­æ‰‹åŠ¨å®‰è£…éœ€è¦çš„è½¯ä»¶
```

### 1.3 ç³»ç»Ÿåˆå§‹åŒ–é…ç½®

#### ğŸ” åŸºç¡€å®‰å…¨é…ç½®

**æ›´æ–°ç³»ç»Ÿ:**
```bash
sudo apt update && sudo apt upgrade -y
```

**é…ç½®é˜²ç«å¢™:**
```bash
# å¯ç”¨UFWé˜²ç«å¢™
sudo ufw enable

# å¼€æ”¾å¿…è¦ç«¯å£
sudo ufw allow ssh
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw allow 8080/tcp  # Jenkins
sudo ufw allow 3000/tcp  # Grafana
sudo ufw allow 9090/tcp  # Prometheus

# æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€
sudo ufw status
```

**SSHå®‰å…¨é…ç½®:**
```bash
# ç¼–è¾‘SSHé…ç½®
sudo vim /etc/ssh/sshd_config

# ä¿®æ”¹ä»¥ä¸‹é…ç½®
PermitRootLogin no
PasswordAuthentication no  # ä½¿ç”¨å¯†é’¥è®¤è¯
Port 22
MaxAuthTries 3

# é‡å¯SSHæœåŠ¡
sudo systemctl restart ssh
```

**åˆ›å»ºSSHå¯†é’¥å¯¹:**
```bash
# ç”Ÿæˆå¯†é’¥å¯¹
ssh-keygen -t rsa -b 4096 -C "devops@company.com"

# å¤åˆ¶å…¬é’¥åˆ°æœåŠ¡å™¨
ssh-copy-id devops@192.168.1.10
```

#### ğŸ‘¤ ç”¨æˆ·æƒé™é…ç½®

```bash
# å°†ç”¨æˆ·åŠ å…¥sudoç»„
sudo usermod -aG sudo devops

# é…ç½®sudoå…å¯†ç 
echo "devops ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/devops

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/devops-workspace/{projects,scripts,configs}
```

#### ğŸŒ ç½‘ç»œå’Œä¸»æœºåé…ç½®

```bash
# è®¾ç½®ä¸»æœºå
sudo hostnamectl set-hostname devops-master

# é…ç½®hostsæ–‡ä»¶
sudo tee -a /etc/hosts << EOF
192.168.1.10 devops-master
192.168.1.11 devops-worker
192.168.1.12 devops-monitor
EOF

# è®¾ç½®æ—¶åŒº
sudo timedatectl set-timezone Asia/Shanghai

# éªŒè¯ç½‘ç»œé…ç½®
ip addr show
ping -c 3 google.com
```

#### âœ… é˜¶æ®µä¸€æ£€æŸ¥æ¸…å•

- [ ] è™šæ‹Ÿæœºåˆ›å»ºå®Œæˆ
- [ ] Ubuntu Server 22.04 LTSå®‰è£…æˆåŠŸ
- [ ] ç½‘ç»œé…ç½®æ­£ç¡®ï¼Œèƒ½è®¿é—®äº’è”ç½‘
- [ ] SSHæœåŠ¡æ­£å¸¸ï¼Œå¯†é’¥è®¤è¯ç”Ÿæ•ˆ
- [ ] é˜²ç«å¢™é…ç½®å®Œæˆ
- [ ] ç”¨æˆ·æƒé™é…ç½®æ­£ç¡®
- [ ] ä¸»æœºåå’Œhostsæ–‡ä»¶é…ç½®å®Œæˆ

---

## é˜¶æ®µäºŒï¼šå·¥å…·åŸºç¡€æŒæ¡

> **ç›®æ ‡**: å®‰è£…å’Œé…ç½®æ ¸å¿ƒDevOpså·¥å…·
> **æ—¶é—´**: 2-3å‘¨
> **ä¼˜å…ˆçº§**: P2 â­â­â­â­â­

### 2.1 å¿…è¦åº”ç”¨å®‰è£…

#### ğŸ³ Dockerå®¹å™¨å¹³å°

**å®‰è£…Docker:**
```bash
# å¸è½½æ—§ç‰ˆæœ¬
sudo apt remove docker docker-engine docker.io containerd runc

# æ›´æ–°åŒ…ç´¢å¼•
sudo apt update

# å®‰è£…ä¾èµ–
sudo apt install -y \
  apt-transport-https \
  ca-certificates \
  curl \
  gnupg \
  lsb-release

# æ·»åŠ Dockerå®˜æ–¹GPGå¯†é’¥
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# æ·»åŠ Dockerä»“åº“
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# å®‰è£…Docker
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# å¯åŠ¨DockeræœåŠ¡
sudo systemctl start docker
sudo systemctl enable docker

# å°†ç”¨æˆ·åŠ å…¥dockerç»„
sudo usermod -aG docker $USER

# é‡æ–°ç™»å½•ä½¿ç»„æƒé™ç”Ÿæ•ˆ
newgrp docker
```

**å®‰è£…Docker Compose:**
```bash
# ä¸‹è½½Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.21.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

# æ·»åŠ æ‰§è¡Œæƒé™
sudo chmod +x /usr/local/bin/docker-compose

# éªŒè¯å®‰è£…
docker --version
docker-compose --version
```

#### âš“ Kuberneteså®¹å™¨ç¼–æ’

**å®‰è£…K3s (è½»é‡çº§Kubernetes):**
```bash
# å®‰è£…K3s MasterèŠ‚ç‚¹
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644

# é…ç½®kubectl
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config

# éªŒè¯å®‰è£…
kubectl get nodes
kubectl get pods --all-namespaces
```

**å®‰è£…kubectl:**
```bash
# ä¸‹è½½kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# å®‰è£…kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# éªŒè¯å®‰è£…
kubectl version --client
```

**å®‰è£…Helm:**
```bash
# ä¸‹è½½Helmå®‰è£…è„šæœ¬
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# éªŒè¯å®‰è£…
helm version
```

#### ğŸ”§ ç‰ˆæœ¬æ§åˆ¶å’ŒCI/CDå·¥å…·

**å®‰è£…Git:**
```bash
# å®‰è£…Git
sudo apt install -y git

# é…ç½®Gitç”¨æˆ·ä¿¡æ¯
git config --global user.name "DevOps Engineer"
git config --global user.email "devops@company.com"
git config --global init.defaultBranch main

# é…ç½®Gitåˆ«å
git config --global alias.st status
git config --global alias.co checkout
git config --global alias.br branch
git config --global alias.ci commit
```

**å®‰è£…Jenkins:**
```bash
# æ·»åŠ Jenkinsä»“åº“å¯†é’¥
wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -

# æ·»åŠ Jenkinsä»“åº“
sudo sh -c 'echo deb https://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'

# å®‰è£…Java (Jenkinsä¾èµ–)
sudo apt update
sudo apt install -y openjdk-11-jdk

# å®‰è£…Jenkins
sudo apt install -y jenkins

# å¯åŠ¨JenkinsæœåŠ¡
sudo systemctl start jenkins
sudo systemctl enable jenkins

# è·å–åˆå§‹ç®¡ç†å‘˜å¯†ç 
sudo cat /var/lib/jenkins/secrets/initialAdminPassword
```

#### ğŸ“Š ç›‘æ§å’Œæ—¥å¿—å·¥å…·

**å®‰è£…åŸºç¡€å·¥å…·:**
```bash
# å®‰è£…ç³»ç»Ÿå·¥å…·
sudo apt install -y \
  curl wget vim htop tree jq unzip zip \
  net-tools nmap telnet tcpdump \
  python3-pip nodejs npm \
  software-properties-common

# å®‰è£…Node Exporter (PrometheusæŒ‡æ ‡æ”¶é›†)
cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
tar xzf node_exporter-1.6.1.linux-amd64.tar.gz
sudo mv node_exporter-1.6.1.linux-amd64/node_exporter /usr/local/bin/

# åˆ›å»ºnode_exporteræœåŠ¡
sudo tee /etc/systemd/system/node_exporter.service << EOF
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

# åˆ›å»ºç”¨æˆ·å¹¶å¯åŠ¨æœåŠ¡
sudo useradd --no-create-home --shell /bin/false node_exporter
sudo systemctl daemon-reload
sudo systemctl start node_exporter
sudo systemctl enable node_exporter
```

### 2.2 åº”ç”¨é…ç½®è¯¦è§£

#### ğŸ³ Dockeré«˜çº§é…ç½®

**é…ç½®Docker daemon:**
```bash
# åˆ›å»ºDockeré…ç½®æ–‡ä»¶
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json << EOF
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn"
  ],
  "storage-driver": "overlay2",
  "data-root": "/var/lib/docker"
}
EOF

# é‡å¯DockeræœåŠ¡
sudo systemctl restart docker

# éªŒè¯é…ç½®
docker info
```

**é…ç½®Dockerç½‘ç»œ:**
```bash
# åˆ›å»ºè‡ªå®šä¹‰ç½‘ç»œ
docker network create --driver bridge devops-network

# æŸ¥çœ‹ç½‘ç»œåˆ—è¡¨
docker network ls

# æŸ¥çœ‹ç½‘ç»œè¯¦æƒ…
docker network inspect devops-network
```

#### âš“ Kubernetesé›†ç¾¤é…ç½®

**é…ç½®K3sé›†ç¾¤:**
```bash
# åœ¨MasterèŠ‚ç‚¹è·å–node token
sudo cat /var/lib/rancher/k3s/server/node-token

# åœ¨WorkerèŠ‚ç‚¹åŠ å…¥é›†ç¾¤ (åœ¨devops-workerä¸Šæ‰§è¡Œ)
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.1.10:6443 K3S_TOKEN=<node-token> sh -

# éªŒè¯é›†ç¾¤çŠ¶æ€
kubectl get nodes -o wide
```

**é…ç½®Ingress Controller:**
```bash
# å®‰è£…Nginx Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml

# ç­‰å¾…Ingress Controllerå¯åŠ¨
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s
```

#### ğŸ”§ Jenkinsè¯¦ç»†é…ç½®

**Jenkinsåˆå§‹åŒ–é…ç½®:**
```bash
# 1. è®¿é—® http://192.168.1.10:8080
# 2. è¾“å…¥åˆå§‹å¯†ç 
# 3. é€‰æ‹© "Install suggested plugins"
# 4. åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·

# å®‰è£…é¢å¤–æ’ä»¶ (åœ¨Jenkins Webç•Œé¢)
# - Docker Pipeline
# - Kubernetes
# - Git Parameter
# - Blue Ocean
# - Slack Notification
```

**é…ç½®Jenkinsç³»ç»Ÿ:**
```bash
# å°†jenkinsç”¨æˆ·åŠ å…¥dockerç»„
sudo usermod -aG docker jenkins

# é‡å¯JenkinsæœåŠ¡
sudo systemctl restart jenkins

# é…ç½®Jenkinså·¥ä½œç›®å½•æƒé™
sudo chown -R jenkins:jenkins /var/lib/jenkins
```

#### âœ… é˜¶æ®µäºŒæ£€æŸ¥æ¸…å•

- [ ] Dockerå®‰è£…æˆåŠŸï¼Œèƒ½æ­£å¸¸è¿è¡Œå®¹å™¨
- [ ] Docker Composeå®‰è£…æˆåŠŸ
- [ ] K3sé›†ç¾¤éƒ¨ç½²æˆåŠŸï¼Œkubectlèƒ½æ­£å¸¸ä½¿ç”¨
- [ ] Helmå®‰è£…æˆåŠŸ
- [ ] Gité…ç½®å®Œæˆ
- [ ] Jenkinså®‰è£…å¹¶å®Œæˆåˆå§‹é…ç½®
- [ ] Node Exporteræ­£å¸¸è¿è¡Œ
- [ ] æ‰€æœ‰æœåŠ¡éƒ½è®¾ç½®ä¸ºå¼€æœºè‡ªå¯

---

## é˜¶æ®µä¸‰ï¼šå®è·µåº”ç”¨éƒ¨ç½²

> **ç›®æ ‡**: æ­å»ºå®Œæ•´çš„åº”ç”¨éƒ¨ç½²ç¯å¢ƒ
> **æ—¶é—´**: 2-3å‘¨
> **ä¼˜å…ˆçº§**: P3 â­â­â­â­

### 3.1 åº”ç”¨æ­å»ºå’Œäº¤äº’æµç¨‹

#### ğŸš€ åˆ›å»ºç¤ºä¾‹åº”ç”¨

**å‡†å¤‡é¡¹ç›®ç»“æ„:**
```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/devops-workspace/demo-app/{app,docker,k8s,monitoring,scripts}
cd ~/devops-workspace/demo-app
```

**åˆ›å»ºNode.jsç¤ºä¾‹åº”ç”¨:**
```bash
# åˆ›å»ºpackage.json
cat > app/package.json << EOF
{
  "name": "devops-demo-app",
  "version": "1.0.0",
  "description": "DevOps Demo Application",
  "main": "app.js",
  "scripts": {
    "start": "node app.js",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "dependencies": {
    "express": "^4.18.2",
    "redis": "^4.6.7"
  }
}
EOF

# åˆ›å»ºä¸»åº”ç”¨æ–‡ä»¶
cat > app/app.js << 'EOF'
const express = require('express');
const redis = require('redis');
const app = express();
const port = process.env.PORT || 3000;

// Redisé…ç½®
const redisClient = redis.createClient({
  host: process.env.REDIS_HOST || 'localhost',
  port: process.env.REDIS_PORT || 6379
});

redisClient.on('error', (err) => {
  console.log('Redis Client Error', err);
});

// è¿æ¥Redis
async function connectRedis() {
  try {
    await redisClient.connect();
    console.log('Connected to Redis');
  } catch (err) {
    console.log('Failed to connect to Redis:', err);
  }
}

connectRedis();

// ä¸­é—´ä»¶
app.use(express.json());

// è·¯ç”±
app.get('/', (req, res) => {
  res.json({
    message: 'Hello DevOps!',
    version: '1.0.0',
    timestamp: new Date().toISOString(),
    hostname: require('os').hostname()
  });
});

app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    uptime: process.uptime(),
    timestamp: new Date().toISOString()
  });
});

app.get('/redis-test', async (req, res) => {
  try {
    await redisClient.set('test-key', 'Hello from Redis!');
    const value = await redisClient.get('test-key');
    res.json({
      status: 'success',
      value: value,
      timestamp: new Date().toISOString()
    });
  } catch (err) {
    res.status(500).json({
      status: 'error',
      message: err.message
    });
  }
});

// å¯åŠ¨æœåŠ¡å™¨
app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`);
  console.log(`Health check: http://localhost:${port}/health`);
});

// ä¼˜é›…å…³é—­
process.on('SIGTERM', async () => {
  console.log('SIGTERM received, shutting down gracefully');
  await redisClient.quit();
  process.exit(0);
});
EOF

# åˆ›å»ºDockerfile
cat > docker/Dockerfile << 'EOF'
FROM node:18-alpine

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶packageæ–‡ä»¶
COPY app/package*.json ./

# å®‰è£…ä¾èµ–
RUN npm install --only=production

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY app/ .

# åˆ›å»ºérootç”¨æˆ·
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001
USER nextjs

# æš´éœ²ç«¯å£
EXPOSE 3000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3000/health || exit 1

# å¯åŠ¨åº”ç”¨
CMD ["npm", "start"]
EOF
```

#### ğŸ³ Dockerå®¹å™¨åŒ–éƒ¨ç½²

**æ„å»ºå’Œæµ‹è¯•Dockeré•œåƒ:**
```bash
# æ„å»ºé•œåƒ
docker build -t devops-demo-app:v1.0.0 -f docker/Dockerfile .

# è¿è¡ŒRediså®¹å™¨
docker run -d \
  --name redis-server \
  --network devops-network \
  -p 6379:6379 \
  redis:alpine

# è¿è¡Œåº”ç”¨å®¹å™¨
docker run -d \
  --name demo-app \
  --network devops-network \
  -p 3000:3000 \
  -e REDIS_HOST=redis-server \
  devops-demo-app:v1.0.0

# æµ‹è¯•åº”ç”¨
curl http://localhost:3000/
curl http://localhost:3000/health
curl http://localhost:3000/redis-test
```

**åˆ›å»ºDocker Composeé…ç½®:**
```bash
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - redis
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
      - app-network
    restart: unless-stopped
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - app-network
    restart: unless-stopped

networks:
  app-network:
    driver: bridge

volumes:
  redis-data:
EOF

# åˆ›å»ºNginxé…ç½®æ–‡ä»¶
cat > nginx.conf << 'EOF'
events {
    worker_connections 1024;
}

http {
    upstream app {
        server app:3000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /health {
            proxy_pass http://app/health;
            access_log off;
        }
    }
}
EOF
```

#### âš“ Kuberneteséƒ¨ç½²é…ç½®

**åˆ›å»ºKubernetesèµ„æºæ¸…å•:**
```bash
# åˆ›å»ºNamespace
cat > k8s/namespace.yaml << 'EOF'
apiVersion: v1
kind: Namespace
metadata:
  name: demo-app
  labels:
    name: demo-app
EOF

# åˆ›å»ºRediséƒ¨ç½²
cat > k8s/redis.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        command: ["redis-server", "--appendonly", "yes"]
        volumeMounts:
        - name: redis-data
          mountPath: /data
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      volumes:
      - name: redis-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: demo-app
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP
EOF

# åˆ›å»ºåº”ç”¨éƒ¨ç½²
cat > k8s/app.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
  namespace: demo-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
    spec:
      containers:
      - name: app
        image: devops-demo-app:v1.0.0
        ports:
        - containerPort: 3000
        env:
        - name: REDIS_HOST
          value: "redis-service"
        - name: REDIS_PORT
          value: "6379"
        - name: NODE_ENV
          value: "production"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: demo-app-service
  namespace: demo-app
spec:
  selector:
    app: demo-app
  ports:
  - port: 80
    targetPort: 3000
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: demo-app-ingress
  namespace: demo-app
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: demo-app.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: demo-app-service
            port:
              number: 80
EOF
```

### 3.2 åº”ç”¨éªŒè¯æµç¨‹

#### ğŸ” Dockerç¯å¢ƒéªŒè¯

**éªŒè¯è„šæœ¬:**
```bash
#!/bin/bash
# scripts/verify-docker.sh

set -e

echo "=== Dockerç¯å¢ƒéªŒè¯ ==="

# éªŒè¯DockeræœåŠ¡
echo "1. æ£€æŸ¥DockeræœåŠ¡çŠ¶æ€..."
if systemctl is-active --quiet docker; then
    echo "âœ… DockeræœåŠ¡æ­£å¸¸è¿è¡Œ"
else
    echo "âŒ DockeræœåŠ¡å¼‚å¸¸"
    exit 1
fi

# éªŒè¯Docker Compose
echo "2. å¯åŠ¨åº”ç”¨æ ˆ..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "3. ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
echo "4. æ£€æŸ¥å®¹å™¨çŠ¶æ€..."
docker-compose ps

# åº”ç”¨åŠŸèƒ½æµ‹è¯•
echo "5. åº”ç”¨åŠŸèƒ½æµ‹è¯•..."
if curl -f http://localhost/ > /dev/null 2>&1; then
    echo "âœ… åº”ç”¨ä¸»é¡µè®¿é—®æ­£å¸¸"
else
    echo "âŒ åº”ç”¨ä¸»é¡µè®¿é—®å¤±è´¥"
fi

if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "âœ… å¥åº·æ£€æŸ¥æ­£å¸¸"
else
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
fi

if curl -f http://localhost/redis-test > /dev/null 2>&1; then
    echo "âœ… Redisè¿æ¥æ­£å¸¸"
else
    echo "âŒ Redisè¿æ¥å¤±è´¥"
fi

echo "=== DockeréªŒè¯å®Œæˆ ==="
```

#### âš“ Kubernetesç¯å¢ƒéªŒè¯

**éªŒè¯è„šæœ¬:**
```bash
#!/bin/bash
# scripts/verify-k8s.sh

set -e

echo "=== Kubernetesç¯å¢ƒéªŒè¯ ==="

# éƒ¨ç½²åº”ç”¨
echo "1. éƒ¨ç½²åº”ç”¨åˆ°Kubernetes..."
kubectl apply -f k8s/

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "2. ç­‰å¾…éƒ¨ç½²å®Œæˆ..."
kubectl wait --for=condition=available --timeout=300s deployment/demo-app -n demo-app
kubectl wait --for=condition=available --timeout=300s deployment/redis -n demo-app

# æ£€æŸ¥PodçŠ¶æ€
echo "3. æ£€æŸ¥PodçŠ¶æ€..."
kubectl get pods -n demo-app

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "4. æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
kubectl get services -n demo-app

# ç«¯å£è½¬å‘æµ‹è¯•
echo "5. å¯åŠ¨ç«¯å£è½¬å‘..."
kubectl port-forward service/demo-app-service 8080:80 -n demo-app &
PORT_FORWARD_PID=$!

# ç­‰å¾…ç«¯å£è½¬å‘ç”Ÿæ•ˆ
sleep 10

# åº”ç”¨åŠŸèƒ½æµ‹è¯•
echo "6. åº”ç”¨åŠŸèƒ½æµ‹è¯•..."
if curl -f http://localhost:8080/ > /dev/null 2>&1; then
    echo "âœ… åº”ç”¨ä¸»é¡µè®¿é—®æ­£å¸¸"
else
    echo "âŒ åº”ç”¨ä¸»é¡µè®¿é—®å¤±è´¥"
fi

if curl -f http://localhost:8080/health > /dev/null 2>&1; then
    echo "âœ… å¥åº·æ£€æŸ¥æ­£å¸¸"
else
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
fi

# æ¸…ç†ç«¯å£è½¬å‘
kill $PORT_FORWARD_PID

echo "=== KuberneteséªŒè¯å®Œæˆ ==="
```

#### ğŸ“Š ç›‘æ§æŒ‡æ ‡éªŒè¯

**åˆ›å»ºç›‘æ§é…ç½®:**
```bash
# ä¸ºåº”ç”¨æ·»åŠ Prometheusç›‘æ§
cat >> app/app.js << 'EOF'

// æ·»åŠ Prometheusç›‘æ§ç«¯ç‚¹
const promClient = require('prom-client');

// åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
const collectDefaultMetrics = promClient.collectDefaultMetrics;
collectDefaultMetrics();

// è‡ªå®šä¹‰æŒ‡æ ‡
const httpRequestsTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status']
});

const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route']
});

// ä¸­é—´ä»¶ï¼šè®°å½•HTTPè¯·æ±‚æŒ‡æ ‡
app.use((req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestsTotal.inc({
      method: req.method,
      route: req.route?.path || req.path,
      status: res.statusCode
    });
    httpRequestDuration.observe({
      method: req.method,
      route: req.route?.path || req.path
    }, duration);
  });
  
  next();
});

// PrometheusæŒ‡æ ‡ç«¯ç‚¹
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', promClient.register.contentType);
  res.end(await promClient.register.metrics());
});
EOF
```

#### âœ… é˜¶æ®µä¸‰æ£€æŸ¥æ¸…å•

- [ ] ç¤ºä¾‹åº”ç”¨åˆ›å»ºå®Œæˆ
- [ ] Dockeré•œåƒæ„å»ºæˆåŠŸ
- [ ] Docker Composeé…ç½®æ­£ç¡®ï¼Œèƒ½æ­£å¸¸å¯åŠ¨æ‰€æœ‰æœåŠ¡
- [ ] Kubernetesèµ„æºæ¸…å•é…ç½®å®Œæˆ
- [ ] åº”ç”¨éƒ¨ç½²åˆ°KubernetesæˆåŠŸ
- [ ] æ‰€æœ‰éªŒè¯è„šæœ¬è¿è¡Œæ­£å¸¸
- [ ] åº”ç”¨åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] ç›‘æ§æŒ‡æ ‡ç«¯ç‚¹æ­£å¸¸å·¥ä½œ

---

## é˜¶æ®µå››ï¼šéƒ¨ç½²è¿ç»´è‡ªåŠ¨åŒ–

> **ç›®æ ‡**: å®ç°å®Œæ•´çš„CI/CDè‡ªåŠ¨åŒ–æµç¨‹
> **æ—¶é—´**: 3-4å‘¨
> **ä¼˜å…ˆçº§**: P4 â­â­â­â­

### 4.1 åº”ç”¨éƒ¨ç½²ç­–ç•¥

#### ğŸš€ å¤šç¯å¢ƒéƒ¨ç½²é…ç½®

**åˆ›å»ºç¯å¢ƒé…ç½®:**
```bash
# åˆ›å»ºç¯å¢ƒç›®å½•
mkdir -p ~/devops-workspace/demo-app/environments/{dev,staging,prod}

# å¼€å‘ç¯å¢ƒé…ç½®
cat > environments/dev/docker-compose.override.yml << 'EOF'
version: '3.8'

services:
  app:
    build:
      context: ../..
      dockerfile: docker/Dockerfile
    environment:
      - NODE_ENV=development
      - DEBUG=true
    volumes:
      - ../../app:/app
    command: npm run dev
    
  redis:
    ports:
      - "6379:6379"
EOF

# æµ‹è¯•ç¯å¢ƒé…ç½®
cat > environments/staging/values.yaml << 'EOF'
app:
  name: demo-app-staging
  namespace: staging
  image:
    repository: devops-demo-app
    tag: staging
    pullPolicy: Always
  
  replicas: 2
  
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "256Mi"
      cpu: "200m"

  service:
    type: ClusterIP
    port: 80

  ingress:
    enabled: true
    host: staging.demo-app.local

redis:
  enabled: true
  persistence:
    enabled: false
EOF

# ç”Ÿäº§ç¯å¢ƒé…ç½®
cat > environments/prod/values.yaml << 'EOF'
app:
  name: demo-app-prod
  namespace: production
  image:
    repository: devops-demo-app
    tag: v1.0.0
    pullPolicy: IfNotPresent
  
  replicas: 5
  
  resources:
    requests:
      memory: "256Mi"
      cpu: "200m"
    limits:
      memory: "512Mi"
      cpu: "500m"

  service:
    type: ClusterIP
    port: 80

  ingress:
    enabled: true
    host: demo-app.example.com
    tls:
      enabled: true

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70

redis:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi
  
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"
EOF
```

#### ğŸ“¦ Helm Chartåˆ›å»º

**åˆ›å»ºHelm Chartç»“æ„:**
```bash
# åˆ›å»ºHelm Chart
mkdir -p helm/demo-app/{templates,charts}

# Chart.yaml
cat > helm/demo-app/Chart.yaml << 'EOF'
apiVersion: v2
name: demo-app
description: A DevOps Demo Application Helm Chart
type: application
version: 1.0.0
appVersion: "1.0.0"

dependencies:
  - name: redis
    version: "17.11.3"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
EOF

# values.yaml (é»˜è®¤é…ç½®)
cat > helm/demo-app/values.yaml << 'EOF'
app:
  name: demo-app
  namespace: default
  
  image:
    repository: devops-demo-app
    tag: latest
    pullPolicy: IfNotPresent
  
  replicas: 3
  
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "256Mi"
      cpu: "200m"

  service:
    type: ClusterIP
    port: 80
    targetPort: 3000

  ingress:
    enabled: false
    host: demo-app.local
    tls:
      enabled: false

  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80

redis:
  enabled: true
  architecture: standalone
  auth:
    enabled: false
  master:
    persistence:
      enabled: false
EOF

# Deploymentæ¨¡æ¿
cat > helm/demo-app/templates/deployment.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.app.name }}
  namespace: {{ .Values.app.namespace }}
  labels:
    app: {{ .Values.app.name }}
    version: {{ .Values.app.image.tag }}
spec:
  replicas: {{ .Values.app.replicas }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: {{ .Values.app.name }}
  template:
    metadata:
      labels:
        app: {{ .Values.app.name }}
        version: {{ .Values.app.image.tag }}
    spec:
      containers:
      - name: {{ .Values.app.name }}
        image: "{{ .Values.app.image.repository }}:{{ .Values.app.image.tag }}"
        imagePullPolicy: {{ .Values.app.image.pullPolicy }}
        ports:
        - containerPort: {{ .Values.app.service.targetPort }}
        env:
        - name: NODE_ENV
          value: "production"
        {{- if .Values.redis.enabled }}
        - name: REDIS_HOST
          value: "{{ .Release.Name }}-redis-master"
        - name: REDIS_PORT
          value: "6379"
        {{- end }}
        livenessProbe:
          httpGet:
            path: /health
            port: {{ .Values.app.service.targetPort }}
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: {{ .Values.app.service.targetPort }}
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          {{- toYaml .Values.app.resources | nindent 10 }}
EOF

# Serviceæ¨¡æ¿
cat > helm/demo-app/templates/service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.app.name }}-service
  namespace: {{ .Values.app.namespace }}
  labels:
    app: {{ .Values.app.name }}
spec:
  type: {{ .Values.app.service.type }}
  ports:
  - port: {{ .Values.app.service.port }}
    targetPort: {{ .Values.app.service.targetPort }}
    protocol: TCP
  selector:
    app: {{ .Values.app.name }}
EOF

# HPAæ¨¡æ¿
cat > helm/demo-app/templates/hpa.yaml << 'EOF'
{{- if .Values.app.autoscaling.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Values.app.name }}-hpa
  namespace: {{ .Values.app.namespace }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Values.app.name }}
  minReplicas: {{ .Values.app.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.app.autoscaling.maxReplicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.app.autoscaling.targetCPUUtilizationPercentage }}
{{- end }}
EOF
```

#### ğŸ¯ è“ç»¿éƒ¨ç½²å®ç°

**è“ç»¿éƒ¨ç½²è„šæœ¬:**
```bash
#!/bin/bash
# scripts/blue-green-deploy.sh

set -e

NAMESPACE=${1:-production}
APP_NAME=${2:-demo-app}
NEW_VERSION=${3}
TIMEOUT=${4:-300}

if [ -z "$NEW_VERSION" ]; then
    echo "ç”¨æ³•: $0 <namespace> <app-name> <new-version> [timeout]"
    echo "ç¤ºä¾‹: $0 production demo-app v1.1.0 300"
    exit 1
fi

echo "=== è“ç»¿éƒ¨ç½²å¼€å§‹ ==="
echo "å‘½åç©ºé—´: $NAMESPACE"
echo "åº”ç”¨åç§°: $APP_NAME"
echo "æ–°ç‰ˆæœ¬: $NEW_VERSION"

# è·å–å½“å‰æ´»è·ƒé¢œè‰²
CURRENT_COLOR=$(kubectl get service $APP_NAME-service -n $NAMESPACE -o jsonpath='{.spec.selector.color}' 2>/dev/null || echo "blue")
NEW_COLOR=$([ "$CURRENT_COLOR" = "blue" ] && echo "green" || echo "blue")

echo "å½“å‰æ´»è·ƒç‰ˆæœ¬: $CURRENT_COLOR"
echo "æ–°ç‰ˆæœ¬éƒ¨ç½²åˆ°: $NEW_COLOR"

# éƒ¨ç½²æ–°ç‰ˆæœ¬
echo "1. éƒ¨ç½²æ–°ç‰ˆæœ¬åˆ° $NEW_COLOR ç¯å¢ƒ..."
helm upgrade --install $APP_NAME-$NEW_COLOR ./helm/demo-app \
  --namespace $NAMESPACE \
  --set app.name=$APP_NAME-$NEW_COLOR \
  --set app.namespace=$NAMESPACE \
  --set app.image.tag=$NEW_VERSION \
  --set app.service.selector.color=$NEW_COLOR \
  --wait --timeout=${TIMEOUT}s

# ç­‰å¾…æ–°ç‰ˆæœ¬å°±ç»ª
echo "2. ç­‰å¾…æ–°ç‰ˆæœ¬å°±ç»ª..."
kubectl wait --for=condition=available \
  --timeout=${TIMEOUT}s \
  deployment/$APP_NAME-$NEW_COLOR \
  -n $NAMESPACE

# å¥åº·æ£€æŸ¥
echo "3. æ‰§è¡Œå¥åº·æ£€æŸ¥..."
kubectl port-forward service/$APP_NAME-$NEW_COLOR-service 9000:80 -n $NAMESPACE &
PORT_FORWARD_PID=$!
sleep 10

HEALTH_CHECK_PASSED=false
for i in {1..10}; do
    if curl -f http://localhost:9000/health > /dev/null 2>&1; then
        echo "âœ… å¥åº·æ£€æŸ¥é€šè¿‡ (å°è¯• $i/10)"
        HEALTH_CHECK_PASSED=true
        break
    else
        echo "â³ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œé‡è¯•ä¸­... (å°è¯• $i/10)"
        sleep 10
    fi
done

kill $PORT_FORWARD_PID 2>/dev/null || true

if [ "$HEALTH_CHECK_PASSED" = false ]; then
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå›æ»šéƒ¨ç½²"
    helm uninstall $APP_NAME-$NEW_COLOR -n $NAMESPACE
    exit 1
fi

# åˆ‡æ¢æµé‡
echo "4. åˆ‡æ¢æµé‡åˆ°æ–°ç‰ˆæœ¬..."
kubectl patch service $APP_NAME-service -n $NAMESPACE \
  -p '{"spec":{"selector":{"color":"'$NEW_COLOR'"}}}'

echo "5. éªŒè¯æµé‡åˆ‡æ¢..."
sleep 30

# æ¸…ç†æ—§ç‰ˆæœ¬
echo "6. æ¸…ç†æ—§ç‰ˆæœ¬..."
helm uninstall $APP_NAME-$CURRENT_COLOR -n $NAMESPACE 2>/dev/null || true

echo "=== è“ç»¿éƒ¨ç½²å®Œæˆ ==="
echo "æ–°ç‰ˆæœ¬ $NEW_VERSION å·²æˆåŠŸéƒ¨ç½²åˆ° $NEW_COLOR ç¯å¢ƒ"
```

### 4.2 CI/CDè‡ªåŠ¨åŒ–æµç¨‹

#### ğŸ”„ Jenkins Pipelineé…ç½®

**åˆ›å»ºJenkinsfile:**
```bash
cat > Jenkinsfile << 'EOF'
pipeline {
    agent any
    
    environment {
        DOCKER_REGISTRY = 'localhost:5000'
        APP_NAME = 'devops-demo-app'
        DOCKER_IMAGE = "${DOCKER_REGISTRY}/${APP_NAME}"
        KUBECONFIG = credentials('kubeconfig')
        SLACK_CHANNEL = '#devops'
    }
    
    parameters {
        choice(
            name: 'DEPLOY_ENV',
            choices: ['staging', 'production'],
            description: 'é€‰æ‹©éƒ¨ç½²ç¯å¢ƒ'
        )
        booleanParam(
            name: 'SKIP_TESTS',
            defaultValue: false,
            description: 'è·³è¿‡æµ‹è¯•'
        )
        string(
            name: 'VERSION_TAG',
            defaultValue: '',
            description: 'ç‰ˆæœ¬æ ‡ç­¾ (ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ)'
        )
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    env.VERSION_TAG = params.VERSION_TAG ?: "v${BUILD_NUMBER}-${GIT_COMMIT.take(7)}"
                    env.IMAGE_TAG = "${DOCKER_IMAGE}:${env.VERSION_TAG}"
                }
            }
        }
        
        stage('Install Dependencies') {
            steps {
                dir('app') {
                    sh 'npm ci'
                }
            }
        }
        
        stage('Run Tests') {
            when {
                not { params.SKIP_TESTS }
            }
            steps {
                dir('app') {
                    sh 'npm test'
                    sh 'npm run lint || true'
                }
            }
            post {
                always {
                    publishTestResults testResultsPattern: 'app/test-results.xml'
                }
            }
        }
        
        stage('Security Scan') {
            steps {
                dir('app') {
                    sh 'npm audit --audit-level moderate'
                }
            }
        }
        
        stage('Build Docker Image') {
            steps {
                script {
                    def image = docker.build(env.IMAGE_TAG, "-f docker/Dockerfile .")
                    image.push()
                    image.push("${DOCKER_IMAGE}:latest")
                }
            }
        }
        
        stage('Deploy to Staging') {
            when {
                anyOf {
                    branch 'develop'
                    params.DEPLOY_ENV == 'staging'
                }
            }
            steps {
                script {
                    sh """
                        helm upgrade --install demo-app-staging ./helm/demo-app \
                          --namespace staging \
                          --create-namespace \
                          --set app.namespace=staging \
                          --set app.image.tag=${env.VERSION_TAG} \
                          --set app.replicas=2 \
                          --wait --timeout=300s
                    """
                }
            }
        }
        
        stage('Integration Tests') {
            when {
                anyOf {
                    branch 'develop'
                    params.DEPLOY_ENV == 'staging'
                }
            }
            steps {
                sh './scripts/integration-tests.sh staging'
            }
        }
        
        stage('Deploy to Production') {
            when {
                anyOf {
                    branch 'main'
                    params.DEPLOY_ENV == 'production'
                }
            }
            steps {
                script {
                    input message: 'ç¡®è®¤éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ?', ok: 'éƒ¨ç½²'
                    
                    sh """
                        ./scripts/blue-green-deploy.sh production demo-app ${env.VERSION_TAG}
                    """
                }
            }
        }
        
        stage('Production Health Check') {
            when {
                anyOf {
                    branch 'main'
                    params.DEPLOY_ENV == 'production'
                }
            }
            steps {
                sh './scripts/health-check.sh production'
            }
        }
    }
    
    post {
        success {
            slackSend(
                channel: env.SLACK_CHANNEL,
                color: 'good',
                message: "âœ… éƒ¨ç½²æˆåŠŸ: ${env.APP_NAME} ${env.VERSION_TAG} åˆ° ${params.DEPLOY_ENV}"
            )
        }
        failure {
            slackSend(
                channel: env.SLACK_CHANNEL,
                color: 'danger',
                message: "âŒ éƒ¨ç½²å¤±è´¥: ${env.APP_NAME} ${env.VERSION_TAG} åˆ° ${params.DEPLOY_ENV}\nè¯¦æƒ…: ${BUILD_URL}"
            )
        }
        always {
            cleanWs()
        }
    }
}
EOF
```

#### ğŸ± GitHub Actionså·¥ä½œæµ

**åˆ›å»ºGitHub Actionsé…ç½®:**
```bash
mkdir -p .github/workflows

cat > .github/workflows/ci-cd.yml << 'EOF'
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: app/package-lock.json
    
    - name: Install dependencies
      run: |
        cd app
        npm ci
    
    - name: Run linting
      run: |
        cd app
        npm run lint
    
    - name: Run tests
      run: |
        cd app
        npm test
    
    - name: Run security audit
      run: |
        cd app
        npm audit --audit-level moderate

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    permissions:
      contents: read
      packages: write
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBECONFIG }}
    
    - name: Install Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'
    
    - name: Deploy to staging
      run: |
        helm upgrade --install demo-app-staging ./helm/demo-app \
          --namespace staging \
          --create-namespace \
          --set app.namespace=staging \
          --set app.image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
          --set app.image.tag=${{ github.sha }} \
          --set app.replicas=2 \
          --wait --timeout=300s
    
    - name: Run integration tests
      run: ./scripts/integration-tests.sh staging

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBECONFIG }}
    
    - name: Install Helm
      uses: azure/setup-helm@v3
      with:
        version: '3.12.0'
    
    - name: Deploy to production
      run: |
        ./scripts/blue-green-deploy.sh production demo-app ${{ github.sha }}
    
    - name: Production health check
      run: ./scripts/health-check.sh production
    
    - name: Notify success
      if: success()
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#devops'
        text: 'âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æˆåŠŸ: ${{ github.sha }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Notify failure
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#devops'
        text: 'âŒ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å¤±è´¥: ${{ github.sha }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
EOF
```

#### ğŸ¦Š GitLab CI/CDé…ç½®

**åˆ›å»ºGitLab CIé…ç½®:**
```bash
cat > .gitlab-ci.yml << 'EOF'
stages:
  - test
  - build
  - deploy-staging
  - deploy-production

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  APP_NAME: "devops-demo-app"
  REGISTRY: $CI_REGISTRY
  IMAGE: $CI_REGISTRY_IMAGE

# ç¼“å­˜é…ç½®
cache:
  paths:
    - app/node_modules/

# æµ‹è¯•é˜¶æ®µ
test:
  stage: test
  image: node:18
  script:
    - cd app
    - npm ci
    - npm run lint
    - npm test
    - npm audit --audit-level moderate
  coverage: '/Statements\s*:\s*([^%]+)/'
  artifacts:
    reports:
      junit: app/test-results.xml
      coverage_report:
        coverage_format: cobertura
        path: app/coverage/cobertura-coverage.xml

# æ„å»ºé˜¶æ®µ
build:
  stage: build
  image: docker:24.0.5
  services:
    - docker:24.0.5-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - |
      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        tag=""
        echo "Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'"
      else
        tag=":$CI_COMMIT_REF_SLUG"
        echo "Running on branch '$CI_COMMIT_BRANCH': tag = $tag"
      fi
    - docker build -t $IMAGE:$CI_COMMIT_SHA -f docker/Dockerfile .
    - docker push $IMAGE:$CI_COMMIT_SHA
    - docker tag $IMAGE:$CI_COMMIT_SHA $IMAGE$tag
    - docker push $IMAGE$tag
  only:
    - main
    - develop

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy-staging:
  stage: deploy-staging
  image: alpine/helm:3.12.0
  before_script:
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl
    - mv kubectl /usr/local/bin/
    - echo $KUBECONFIG | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
  script:
    - |
      helm upgrade --install demo-app-staging ./helm/demo-app \
        --namespace staging \
        --create-namespace \
        --set app.namespace=staging \
        --set app.image.repository=$IMAGE \
        --set app.image.tag=$CI_COMMIT_SHA \
        --set app.replicas=2 \
        --wait --timeout=300s
    - ./scripts/integration-tests.sh staging
  environment:
    name: staging
    url: https://staging.demo-app.local
  only:
    - develop

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy-production:
  stage: deploy-production
  image: alpine/helm:3.12.0
  before_script:
    - apk add --no-cache curl bash
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl
    - mv kubectl /usr/local/bin/
    - echo $KUBECONFIG | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
  script:
    - ./scripts/blue-green-deploy.sh production demo-app $CI_COMMIT_SHA
    - ./scripts/health-check.sh production
  environment:
    name: production
    url: https://demo-app.example.com
  when: manual
  only:
    - main

# é€šçŸ¥é…ç½®
.notify_success: &notify_success
  - |
    curl -X POST -H 'Content-type: application/json' \
      --data "{\"text\":\"âœ… éƒ¨ç½²æˆåŠŸ: $APP_NAME $CI_COMMIT_SHA åˆ° $CI_ENVIRONMENT_NAME\"}" \
      $SLACK_WEBHOOK_URL

.notify_failure: &notify_failure
  - |
    curl -X POST -H 'Content-type: application/json' \
      --data "{\"text\":\"âŒ éƒ¨ç½²å¤±è´¥: $APP_NAME $CI_COMMIT_SHA åˆ° $CI_ENVIRONMENT_NAME\nè¯¦æƒ…: $CI_PIPELINE_URL\"}" \
      $SLACK_WEBHOOK_URL

# æˆåŠŸåé€šçŸ¥
notify-success:
  stage: .post
  image: alpine:latest
  before_script:
    - apk add --no-cache curl
  script: *notify_success
  when: on_success
  only:
    - main
    - develop

# å¤±è´¥åé€šçŸ¥
notify-failure:
  stage: .post
  image: alpine:latest
  before_script:
    - apk add --no-cache curl
  script: *notify_failure
  when: on_failure
  only:
    - main
    - develop
EOF
```

#### âœ… é˜¶æ®µå››æ£€æŸ¥æ¸…å•

- [ ] å¤šç¯å¢ƒé…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ
- [ ] Helm Charté…ç½®æ­£ç¡®
- [ ] è“ç»¿éƒ¨ç½²è„šæœ¬æµ‹è¯•æˆåŠŸ
- [ ] Jenkins Pipelineé…ç½®å®Œæˆ
- [ ] GitHub Actionså·¥ä½œæµé…ç½®å®Œæˆ
- [ ] GitLab CI/CDé…ç½®å®Œæˆ
- [ ] æ‰€æœ‰éƒ¨ç½²è„šæœ¬èƒ½æ­£å¸¸æ‰§è¡Œ
- [ ] CI/CDæµç¨‹ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡

---

## é˜¶æ®µäº”ï¼šé«˜çº§è¿ç»´ç›‘æ§

> **ç›®æ ‡**: å®ç°å®Œæ•´çš„ç›‘æ§ã€å‘Šè­¦å’Œè‡ªåŠ¨åŒ–è¿ç»´
> **æ—¶é—´**: 2-3å‘¨
> **ä¼˜å…ˆçº§**: P5 â­â­â­

### 5.1 ç›‘æ§ä½“ç³»æ­å»º

#### ğŸ“Š Prometheusç›‘æ§é…ç½®

**éƒ¨ç½²Prometheus Stack:**
```bash
# åˆ›å»ºç›‘æ§å‘½åç©ºé—´
kubectl create namespace monitoring

# æ·»åŠ Prometheus Helmä»“åº“
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# åˆ›å»ºPrometheusé…ç½®
cat > monitoring/prometheus-values.yaml << 'EOF'
prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    additionalScrapeConfigs:
      - job_name: 'demo-app'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - default
                - staging
                - production
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)

grafana:
  adminPassword: admin123
  persistence:
    enabled: true
    size: 5Gi
  
  dashboardsConfigMaps:
    demo-app: demo-app-dashboard
  
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server:80
          access: proxy
          isDefault: true

alertmanager:
  config:
    global:
      slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
    
    receivers:
      - name: 'web.hook'
        slack_configs:
          - channel: '#alerts'
            text: 'Summary: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

nodeExporter:
  enabled: true

pushgateway:
  enabled: true

serverFiles:
  alerting_rules.yml:
    groups:
      - name: demo-app.rules
        rules:
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
              description: "{{ $labels.instance }} é”™è¯¯ç‡ä¸º {{ $value }}"
          
          - alert: HighMemoryUsage
            expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.8
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
              description: "{{ $labels.pod }} å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡80%"
          
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Podé¢‘ç¹é‡å¯"
              description: "{{ $labels.pod }} åœ¨è¿‡å»15åˆ†é’Ÿå†…é‡å¯äº† {{ $value }} æ¬¡"
EOF

# éƒ¨ç½²Prometheus Stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values monitoring/prometheus-values.yaml
```

#### ğŸ“ˆ Grafana Dashboardé…ç½®

**åˆ›å»ºåº”ç”¨ç›‘æ§Dashboard:**
```bash
cat > monitoring/demo-app-dashboard.json << 'EOF'
{
  "dashboard": {
    "id": null,
    "title": "DevOps Demo Appç›‘æ§",
    "tags": ["devops", "demo"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "HTTPè¯·æ±‚æ€»æ•°",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m]))",
            "legendFormat": "RPS"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 50},
                {"color": "red", "value": 100}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "é”™è¯¯ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100",
            "legendFormat": "Error Rate %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 1},
                {"color": "red", "value": 5}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
      },
      {
        "id": 3,
        "title": "å“åº”æ—¶é—´",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 0.5},
                {"color": "red", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
      },
      {
        "id": 4,
        "title": "PodçŠ¶æ€",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0}
      },
      {
        "id": 5,
        "title": "HTTPè¯·æ±‚è¶‹åŠ¿",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (status) (rate(http_requests_total[5m]))",
            "legendFormat": "{{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 6,
        "title": "å†…å­˜ä½¿ç”¨",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (pod) (container_memory_usage_bytes{container=\"demo-app\"})",
            "legendFormat": "{{pod}}"
          }
        ],
        "yAxes": [
          {
            "unit": "bytes"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
EOF

# åˆ›å»ºConfigMap
kubectl create configmap demo-app-dashboard \
  --from-file=monitoring/demo-app-dashboard.json \
  --namespace monitoring
```

#### ğŸ“‹ ELK Stackæ—¥å¿—æ”¶é›†

**éƒ¨ç½²ELK Stack:**
```bash
cat > monitoring/elk-stack.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: monitoring
data:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
    
    output.logstash:
      hosts: ["logstash:5044"]
    
    processors:
      - add_host_metadata: ~
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      serviceAccountName: filebeat
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:8.8.0
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: config
          mountPath: /usr/share/filebeat/filebeat.yml
          subPath: filebeat.yml
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
rules:
- apiGroups: [""]
  resources: ["nodes", "namespaces", "pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
EOF

# ä½¿ç”¨Helméƒ¨ç½²ELK Stack
helm repo add elastic https://helm.elastic.co
helm repo update

# éƒ¨ç½²Elasticsearch
helm install elasticsearch elastic/elasticsearch \
  --namespace monitoring \
  --set replicas=1 \
  --set minimumMasterNodes=1

# éƒ¨ç½²Kibana
helm install kibana elastic/kibana \
  --namespace monitoring \
  --set elasticsearchHosts="http://elasticsearch-master:9200"

# éƒ¨ç½²Logstash
cat > monitoring/logstash-values.yaml << 'EOF'
logstashConfig:
  logstash.yml: |
    http.host: 0.0.0.0
    xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch-master:9200"]

logstashPipeline:
  logstash.conf: |
    input {
      beats {
        port => 5044
      }
    }
    
    filter {
      if [kubernetes] {
        mutate {
          add_field => {
            "container_name" => "%{[kubernetes][container][name]}"
            "namespace" => "%{[kubernetes][namespace]}"
            "pod_name" => "%{[kubernetes][pod][name]}"
          }
        }
      }
      
      # è§£æJSONæ—¥å¿—
      if [message] =~ /^\{.*\}$/ {
        json {
          source => "message"
        }
      }
      
      # æ·»åŠ æ—¶é—´æˆ³
      date {
        match => [ "@timestamp", "ISO8601" ]
      }
    }
    
    output {
      elasticsearch {
        hosts => ["http://elasticsearch-master:9200"]
        index => "demo-app-logs-%{+YYYY.MM.dd}"
      }
    }

service:
  type: ClusterIP
  ports:
    - name: beats
      port: 5044
      protocol: TCP
      targetPort: 5044
EOF

helm install logstash elastic/logstash \
  --namespace monitoring \
  --values monitoring/logstash-values.yaml

# éƒ¨ç½²Filebeat
kubectl apply -f monitoring/elk-stack.yaml
```

### 5.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

#### ğŸ¤– è¿ç»´è‡ªåŠ¨åŒ–è„šæœ¬

**åˆ›å»ºç»¼åˆè¿ç»´è„šæœ¬:**
```bash
#!/bin/bash
# scripts/auto-ops.sh

set -e

# é…ç½®å‚æ•°
NAMESPACE=${NAMESPACE:-production}
APP_NAME=${APP_NAME:-demo-app}
BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-7}
LOG_RETENTION_DAYS=${LOG_RETENTION_DAYS:-30}
ALERT_WEBHOOK=${ALERT_WEBHOOK:-""}

# æ—¥å¿—å‡½æ•°
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# å‘é€å‘Šè­¦
send_alert() {
    local message="$1"
    local level="$2"
    
    log "$level: $message"
    
    if [ -n "$ALERT_WEBHOOK" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"$level: $message\"}" \
            "$ALERT_WEBHOOK" || true
    fi
}

# å¥åº·æ£€æŸ¥
health_check() {
    log "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    # æ£€æŸ¥PodçŠ¶æ€
    local unhealthy_pods=$(kubectl get pods -n $NAMESPACE -l app=$APP_NAME --field-selector=status.phase!=Running --no-headers | wc -l)
    
    if [ $unhealthy_pods -gt 0 ]; then
        send_alert "å‘ç° $unhealthy_pods ä¸ªå¼‚å¸¸Pod" "WARNING"
        
        # å°è¯•é‡å¯å¼‚å¸¸Pod
        kubectl delete pods -n $NAMESPACE -l app=$APP_NAME --field-selector=status.phase!=Running
        sleep 60
        
        # å†æ¬¡æ£€æŸ¥
        unhealthy_pods=$(kubectl get pods -n $NAMESPACE -l app=$APP_NAME --field-selector=status.phase!=Running --no-headers | wc -l)
        if [ $unhealthy_pods -gt 0 ]; then
            send_alert "é‡å¯åä»æœ‰ $unhealthy_pods ä¸ªå¼‚å¸¸Pod" "CRITICAL"
        else
            send_alert "å¼‚å¸¸Podå·²æ¢å¤" "INFO"
        fi
    fi
    
    # æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    local service_endpoint=$(kubectl get service $APP_NAME-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')
    
    if ! kubectl run test-pod --rm -i --restart=Never --image=curlimages/curl -- \
        curl -f http://$service_endpoint/health > /dev/null 2>&1; then
        send_alert "æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥" "CRITICAL"
        return 1
    fi
    
    log "å¥åº·æ£€æŸ¥é€šè¿‡"
    return 0
}

# èµ„æºæ¸…ç†
cleanup_resources() {
    log "æ‰§è¡Œèµ„æºæ¸…ç†..."
    
    # æ¸…ç†Dockerèµ„æº (åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ‰§è¡Œ)
    kubectl get nodes -o name | while read node; do
        node_name=$(echo $node | cut -d'/' -f2)
        log "æ¸…ç†èŠ‚ç‚¹ $node_name çš„Dockerèµ„æº"
        
        kubectl debug node/$node_name -it --image=alpine -- sh -c "
            chroot /host docker system prune -f
            chroot /host docker image prune -f
            chroot /host docker volume prune -f
        " || true
    done
    
    # æ¸…ç†Kubernetesèµ„æº
    log "æ¸…ç†è¿‡æœŸçš„Jobå’ŒPod"
    kubectl delete jobs -n $NAMESPACE --field-selector=status.successful=1 --ignore-not-found=true
    kubectl delete pods -n $NAMESPACE --field-selector=status.phase=Succeeded --ignore-not-found=true
    kubectl delete pods -n $NAMESPACE --field-selector=status.phase=Failed --ignore-not-found=true
    
    log "èµ„æºæ¸…ç†å®Œæˆ"
}

# å¤‡ä»½æ•°æ®
backup_data() {
    log "æ‰§è¡Œæ•°æ®å¤‡ä»½..."
    
    local backup_date=$(date +%Y%m%d_%H%M%S)
    local backup_path="/backup/demo-app-$backup_date"
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    mkdir -p $backup_path
    
    # å¤‡ä»½Redisæ•°æ® (å¦‚æœå­˜åœ¨)
    if kubectl get deployment redis -n $NAMESPACE > /dev/null 2>&1; then
        log "å¤‡ä»½Redisæ•°æ®"
        kubectl exec deployment/redis -n $NAMESPACE -- redis-cli BGSAVE
        kubectl cp $NAMESPACE/$(kubectl get pods -n $NAMESPACE -l app=redis -o jsonpath='{.items[0].metadata.name}'):/data/dump.rdb $backup_path/redis-dump.rdb
    fi
    
    # å¤‡ä»½åº”ç”¨é…ç½®
    log "å¤‡ä»½åº”ç”¨é…ç½®"
    kubectl get configmaps -n $NAMESPACE -o yaml > $backup_path/configmaps.yaml
    kubectl get secrets -n $NAMESPACE -o yaml > $backup_path/secrets.yaml
    
    # å‹ç¼©å¤‡ä»½
    tar -czf $backup_path.tar.gz -C /backup demo-app-$backup_date
    rm -rf $backup_path
    
    # æ¸…ç†æ—§å¤‡ä»½
    find /backup -name "demo-app-*.tar.gz" -mtime +$BACKUP_RETENTION_DAYS -delete
    
    log "æ•°æ®å¤‡ä»½å®Œæˆ: $backup_path.tar.gz"
}

# æ—¥å¿—æ¸…ç†
cleanup_logs() {
    log "æ‰§è¡Œæ—¥å¿—æ¸…ç†..."
    
    # æ¸…ç†æœ¬åœ°æ—¥å¿—
    find /var/log -name "*.log" -mtime +$LOG_RETENTION_DAYS -delete 2>/dev/null || true
    
    # æ¸…ç†Elasticsearchç´¢å¼• (å¦‚æœå­˜åœ¨)
    if kubectl get service elasticsearch-master -n monitoring > /dev/null 2>&1; then
        local es_endpoint=$(kubectl get service elasticsearch-master -n monitoring -o jsonpath='{.spec.clusterIP}')
        local cutoff_date=$(date -d "$LOG_RETENTION_DAYS days ago" +%Y.%m.%d)
        
        # åˆ é™¤è¿‡æœŸç´¢å¼•
        kubectl run es-cleanup --rm -i --restart=Never --image=curlimages/curl -- \
            curl -X DELETE "http://$es_endpoint:9200/demo-app-logs-*" -H "Content-Type: application/json" -d "{
                \"query\": {
                    \"range\": {
                        \"@timestamp\": {
                            \"lt\": \"$cutoff_date||/d\"
                        }
                    }
                }
            }" || true
    fi
    
    log "æ—¥å¿—æ¸…ç†å®Œæˆ"
}

# æ€§èƒ½ä¼˜åŒ–
optimize_performance() {
    log "æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–..."
    
    # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
    local cpu_usage=$(kubectl top pods -n $NAMESPACE --no-headers | awk '{sum+=$2} END {print sum}' | sed 's/m//')
    local memory_usage=$(kubectl top pods -n $NAMESPACE --no-headers | awk '{sum+=$3} END {print sum}' | sed 's/Mi//')
    
    log "å½“å‰CPUä½¿ç”¨: ${cpu_usage}m, å†…å­˜ä½¿ç”¨: ${memory_usage}Mi"
    
    # æ ¹æ®èµ„æºä½¿ç”¨æƒ…å†µè°ƒæ•´HPA
    if [ ${cpu_usage:-0} -gt 500 ]; then
        log "CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè°ƒæ•´HPAå‚æ•°"
        kubectl patch hpa $APP_NAME-hpa -n $NAMESPACE -p '{"spec":{"targetCPUUtilizationPercentage":60}}'
    fi
    
    # æ£€æŸ¥å¹¶é‡å¯é•¿æ—¶é—´è¿è¡Œçš„Pod
    kubectl get pods -n $NAMESPACE -o custom-columns=NAME:.metadata.name,AGE:.status.startTime \
        --no-headers | while read pod_name start_time; do
        if [ -n "$start_time" ]; then
            local pod_age_seconds=$(( $(date +%s) - $(date -d "$start_time" +%s) ))
            local pod_age_days=$(( pod_age_seconds / 86400 ))
            
            if [ $pod_age_days -gt 7 ]; then
                log "é‡å¯é•¿æ—¶é—´è¿è¡Œçš„Pod: $pod_name (è¿è¡Œäº† $pod_age_days å¤©)"
                kubectl delete pod $pod_name -n $NAMESPACE
            fi
        fi
    done
    
    log "æ€§èƒ½ä¼˜åŒ–å®Œæˆ"
}

# ç”Ÿæˆè¿ç»´æŠ¥å‘Š
generate_report() {
    log "ç”Ÿæˆè¿ç»´æŠ¥å‘Š..."
    
    local report_date=$(date +%Y-%m-%d)
    local report_file="/tmp/ops-report-$report_date.md"
    
    cat > $report_file << EOF
# è¿ç»´æŠ¥å‘Š - $report_date

## ç³»ç»Ÿæ¦‚è§ˆ
- æŠ¥å‘Šæ—¶é—´: $(date)
- å‘½åç©ºé—´: $NAMESPACE
- åº”ç”¨åç§°: $APP_NAME

## é›†ç¾¤çŠ¶æ€
### èŠ‚ç‚¹çŠ¶æ€
\`\`\`
$(kubectl get nodes)
\`\`\`

### PodçŠ¶æ€
\`\`\`
$(kubectl get pods -n $NAMESPACE -o wide)
\`\`\`

## èµ„æºä½¿ç”¨æƒ…å†µ
### CPUå’Œå†…å­˜ä½¿ç”¨
\`\`\`
$(kubectl top pods -n $NAMESPACE)
\`\`\`

### å­˜å‚¨ä½¿ç”¨
\`\`\`
$(kubectl get pv)
\`\`\`

## æœåŠ¡çŠ¶æ€
### æœåŠ¡åˆ—è¡¨
\`\`\`
$(kubectl get services -n $NAMESPACE)
\`\`\`

### IngressçŠ¶æ€
\`\`\`
$(kubectl get ingress -n $NAMESPACE)
\`\`\`

## è¿‘æœŸäº‹ä»¶
\`\`\`
$(kubectl get events -n $NAMESPACE --sort-by='.firstTimestamp' | tail -20)
\`\`\`

## å‘Šè­¦ä¿¡æ¯
$(kubectl get pods -n monitoring -l app.kubernetes.io/name=alertmanager -o name | head -1 | xargs kubectl logs -n monitoring --tail=50 | grep -E "(FIRING|RESOLVED)" | tail -10 || echo "æ— å‘Šè­¦ä¿¡æ¯")

---
*æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬ç”Ÿæˆ*
EOF
    
    log "è¿ç»´æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
    
    # å¦‚æœé…ç½®äº†é‚®ä»¶æˆ–Slackï¼Œå¯ä»¥å‘é€æŠ¥å‘Š
    if [ -n "$ALERT_WEBHOOK" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ğŸ“Š è¿ç»´æŠ¥å‘Šå·²ç”Ÿæˆ: $report_date\næŸ¥çœ‹è¯¦æƒ…: \`cat $report_file\`\"}" \
            "$ALERT_WEBHOOK" || true
    fi
}

# ä¸»å‡½æ•°
main() {
    local action=${1:-all}
    
    case $action in
        "health")
            health_check
            ;;
        "cleanup")
            cleanup_resources
            ;;
        "backup")
            backup_data
            ;;
        "logs")
            cleanup_logs
            ;;
        "optimize")
            optimize_performance
            ;;
        "report")
            generate_report
            ;;
        "all")
            log "æ‰§è¡Œå®Œæ•´è¿ç»´æµç¨‹..."
            health_check || send_alert "å¥åº·æ£€æŸ¥å¤±è´¥" "CRITICAL"
            cleanup_resources
            cleanup_logs
            optimize_performance
            backup_data
            generate_report
            log "å®Œæ•´è¿ç»´æµç¨‹æ‰§è¡Œå®Œæˆ"
            ;;
        *)
            echo "ç”¨æ³•: $0 {health|cleanup|backup|logs|optimize|report|all}"
            echo ""
            echo "é€‰é¡¹è¯´æ˜:"
            echo "  health   - æ‰§è¡Œå¥åº·æ£€æŸ¥"
            echo "  cleanup  - æ¸…ç†ç³»ç»Ÿèµ„æº"
            echo "  backup   - å¤‡ä»½é‡è¦æ•°æ®"
            echo "  logs     - æ¸…ç†è¿‡æœŸæ—¥å¿—"
            echo "  optimize - æ€§èƒ½ä¼˜åŒ–"
            echo "  report   - ç”Ÿæˆè¿ç»´æŠ¥å‘Š"
            echo "  all      - æ‰§è¡Œæ‰€æœ‰æ“ä½œ"
            exit 1
            ;;
    esac
}

# é”™è¯¯å¤„ç†
trap 'send_alert "è¿ç»´è„šæœ¬æ‰§è¡Œå¤±è´¥: $0 $*" "ERROR"' ERR

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

#### â° å®šæ—¶ä»»åŠ¡é…ç½®

**åˆ›å»ºCronJobé…ç½®:**
```bash
cat > monitoring/cronjobs.yaml << 'EOF'
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-ops
  namespace: monitoring
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: ops-automation
          containers:
          - name: auto-ops
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache curl bash kubectl
              /scripts/auto-ops.sh all
            env:
            - name: NAMESPACE
              value: "production"
            - name: APP_NAME
              value: "demo-app"
            - name: ALERT_WEBHOOK
              valueFrom:
                secretKeyRef:
                  name: ops-secrets
                  key: slack-webhook
            volumeMounts:
            - name: scripts
              mountPath: /scripts
            - name: backup
              mountPath: /backup
          volumes:
          - name: scripts
            configMap:
              name: ops-scripts
              defaultMode: 0755
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hourly-health-check
  namespace: monitoring
spec:
  schedule: "0 * * * *"  # æ¯å°æ—¶æ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: ops-automation
          containers:
          - name: health-check
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache curl bash kubectl
              /scripts/auto-ops.sh health
            env:
            - name: NAMESPACE
              value: "production"
            - name: APP_NAME
              value: "demo-app"
            - name: ALERT_WEBHOOK
              valueFrom:
                secretKeyRef:
                  name: ops-secrets
                  key: slack-webhook
            volumeMounts:
            - name: scripts
              mountPath: /scripts
          volumes:
          - name: scripts
            configMap:
              name: ops-scripts
              defaultMode: 0755
          restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: weekly-backup
  namespace: monitoring
spec:
  schedule: "0 3 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: ops-automation
          containers:
          - name: backup
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache curl bash kubectl
              /scripts/auto-ops.sh backup
            env:
            - name: NAMESPACE
              value: "production"
            - name: APP_NAME
              value: "demo-app"
            volumeMounts:
            - name: scripts
              mountPath: /scripts
            - name: backup
              mountPath: /backup
          volumes:
          - name: scripts
            configMap:
              name: ops-scripts
              defaultMode: 0755
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
---
# RBACé…ç½®
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ops-automation
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ops-automation
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets", "nodes"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ops-automation
subjects:
- kind: ServiceAccount
  name: ops-automation
  namespace: monitoring
roleRef:
  kind: ClusterRole
  name: ops-automation
  apiGroup: rbac.authorization.k8s.io
---
# å­˜å‚¨é…ç½®
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
EOF

# åˆ›å»ºè¿ç»´è„šæœ¬ConfigMap
kubectl create configmap ops-scripts \
  --from-file=auto-ops.sh=scripts/auto-ops.sh \
  --namespace monitoring

# åˆ›å»ºSecrets (éœ€è¦æ›¿æ¢å®é™…çš„Webhook URL)
kubectl create secret generic ops-secrets \
  --from-literal=slack-webhook="YOUR_SLACK_WEBHOOK_URL" \
  --namespace monitoring

# éƒ¨ç½²CronJobs
kubectl apply -f monitoring/cronjobs.yaml
```

#### âœ… é˜¶æ®µäº”æ£€æŸ¥æ¸…å•

- [ ] Prometheusç›‘æ§ç³»ç»Ÿéƒ¨ç½²æˆåŠŸ
- [ ] Grafana Dashboardé…ç½®å®Œæˆ
- [ ] ELK Stackæ—¥å¿—æ”¶é›†ç³»ç»Ÿæ­£å¸¸è¿è¡Œ
- [ ] å‘Šè­¦è§„åˆ™é…ç½®æ­£ç¡®ï¼Œèƒ½æ­£å¸¸å‘é€å‘Šè­¦
- [ ] è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬æµ‹è¯•é€šè¿‡
- [ ] CronJobå®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆ
- [ ] ç›‘æ§æ•°æ®æ”¶é›†æ­£å¸¸
- [ ] æ—¥å¿—æ”¶é›†å’Œåˆ†æåŠŸèƒ½æ­£å¸¸
- [ ] è¿ç»´æŠ¥å‘Šèƒ½æ­£å¸¸ç”Ÿæˆ

---

## å­¦ä¹ æ—¶é—´è§„åˆ’

### ğŸ“… å»ºè®®å­¦ä¹ è®¡åˆ’

| é˜¶æ®µ | æ—¶é—´å®‰æ’ | æ¯æ—¥å­¦ä¹ æ—¶é—´ | é‡ç‚¹å†…å®¹ |
|------|----------|--------------|----------|
| **é˜¶æ®µä¸€** | ç¬¬1-2å‘¨ | 2-3å°æ—¶ | åŸºç¡€ç¯å¢ƒæ­å»ºï¼Œç³»ç»Ÿå®‰è£…é…ç½® |
| **é˜¶æ®µäºŒ** | ç¬¬3-5å‘¨ | 3-4å°æ—¶ | æ ¸å¿ƒå·¥å…·å®‰è£…é…ç½®ï¼ŒDocker/K8såŸºç¡€ |
| **é˜¶æ®µä¸‰** | ç¬¬6-8å‘¨ | 3-4å°æ—¶ | åº”ç”¨å®¹å™¨åŒ–ï¼ŒK8séƒ¨ç½²å®è·µ |
| **é˜¶æ®µå››** | ç¬¬9-12å‘¨ | 4-5å°æ—¶ | CI/CDæµç¨‹ï¼Œè‡ªåŠ¨åŒ–éƒ¨ç½² |
| **é˜¶æ®µäº”** | ç¬¬13-15å‘¨ | 2-3å°æ—¶ | ç›‘æ§å‘Šè­¦ï¼Œè¿ç»´è‡ªåŠ¨åŒ– |

### ğŸ¯ æ¯å‘¨å­¦ä¹ ç›®æ ‡

**ç¬¬1å‘¨ï¼šç¯å¢ƒå‡†å¤‡**
- [ ] æ­å»ºè™šæ‹ŸåŒ–ç¯å¢ƒ
- [ ] å®‰è£…Ubuntu Server
- [ ] åŸºç¡€ç½‘ç»œé…ç½®
- [ ] SSHå¯†é’¥é…ç½®

**ç¬¬2å‘¨ï¼šç³»ç»Ÿé…ç½®**
- [ ] å®‰å…¨é…ç½®å¼ºåŒ–
- [ ] ç”¨æˆ·æƒé™ç®¡ç†
- [ ] é˜²ç«å¢™é…ç½®
- [ ] ç³»ç»Ÿç›‘æ§åŸºç¡€

**ç¬¬3å‘¨ï¼šDockeråŸºç¡€**
- [ ] Dockerå®‰è£…é…ç½®
- [ ] Dockerå‘½ä»¤å®è·µ
- [ ] Dockerfileç¼–å†™
- [ ] Dockerç½‘ç»œå’Œå­˜å‚¨

**ç¬¬4å‘¨ï¼šDockerè¿›é˜¶**
- [ ] Docker Composeå®è·µ
- [ ] å¤šå®¹å™¨åº”ç”¨éƒ¨ç½²
- [ ] é•œåƒä¼˜åŒ–
- [ ] Dockerå®‰å…¨å®è·µ

**ç¬¬5å‘¨ï¼šKubernetesåŸºç¡€**
- [ ] K8sé›†ç¾¤æ­å»º
- [ ] Podå’ŒServiceæ¦‚å¿µ
- [ ] Deploymentå®è·µ
- [ ] kubectlå‘½ä»¤æŒæ¡

**ç¬¬6å‘¨ï¼šåº”ç”¨å®¹å™¨åŒ–**
- [ ] åˆ›å»ºç¤ºä¾‹åº”ç”¨
- [ ] åº”ç”¨DockeråŒ–
- [ ] å¤šç¯å¢ƒé…ç½®
- [ ] å¥åº·æ£€æŸ¥é…ç½®

**ç¬¬7å‘¨ï¼šK8såº”ç”¨éƒ¨ç½²**
- [ ] åº”ç”¨éƒ¨ç½²åˆ°K8s
- [ ] Serviceå’ŒIngressé…ç½®
- [ ] ConfigMapå’ŒSecretä½¿ç”¨
- [ ] èµ„æºé™åˆ¶å’Œè°ƒåº¦

**ç¬¬8å‘¨ï¼šåº”ç”¨éªŒè¯**
- [ ] åŠŸèƒ½æµ‹è¯•è„šæœ¬
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] ç›‘æ§æŒ‡æ ‡é…ç½®
- [ ] æ•…éšœæ’æŸ¥å®è·µ

**ç¬¬9å‘¨ï¼šHelmå’ŒåŒ…ç®¡ç†**
- [ ] Helm Chartåˆ›å»º
- [ ] æ¨¡æ¿åŒ–é…ç½®
- [ ] ç‰ˆæœ¬ç®¡ç†
- [ ] å¤šç¯å¢ƒéƒ¨ç½²

**ç¬¬10å‘¨ï¼šCI/CDåŸºç¡€**
- [ ] Jenkinså®‰è£…é…ç½®
- [ ] PipelineåŸºç¡€
- [ ] Gité›†æˆ
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•

**ç¬¬11å‘¨ï¼šCI/CDè¿›é˜¶**
- [ ] å¤šåˆ†æ”¯ç­–ç•¥
- [ ] è“ç»¿éƒ¨ç½²
- [ ] æ»šåŠ¨æ›´æ–°
- [ ] å›æ»šç­–ç•¥

**ç¬¬12å‘¨ï¼šCI/CDå®è·µ**
- [ ] GitHub Actions
- [ ] GitLab CI
- [ ] å¤šå¹³å°CI/CDå¯¹æ¯”
- [ ] æœ€ä½³å®è·µæ€»ç»“

**ç¬¬13å‘¨ï¼šç›‘æ§ç³»ç»Ÿ**
- [ ] Prometheuséƒ¨ç½²
- [ ] æŒ‡æ ‡æ”¶é›†é…ç½®
- [ ] Grafana Dashboard
- [ ] å‘Šè­¦è§„åˆ™é…ç½®

**ç¬¬14å‘¨ï¼šæ—¥å¿—ç³»ç»Ÿ**
- [ ] ELK Stackéƒ¨ç½²
- [ ] æ—¥å¿—æ”¶é›†é…ç½®
- [ ] æ—¥å¿—åˆ†æå®è·µ
- [ ] æ—¥å¿—å‘Šè­¦é…ç½®

**ç¬¬15å‘¨ï¼šè¿ç»´è‡ªåŠ¨åŒ–**
- [ ] è‡ªåŠ¨åŒ–è„šæœ¬å¼€å‘
- [ ] å®šæ—¶ä»»åŠ¡é…ç½®
- [ ] è¿ç»´æŠ¥å‘Šç”Ÿæˆ
- [ ] æ•´ä½“é¡¹ç›®æ€»ç»“

---

## å®è·µé¡¹ç›®å»ºè®®

### ğŸš€ æ¨èå®è·µé¡¹ç›®

#### é¡¹ç›®ä¸€ï¼šä¸ªäººåšå®¢ç³»ç»Ÿ (é˜¶æ®µäºŒ-ä¸‰)
```bash
æŠ€æœ¯æ ˆï¼š
- å‰ç«¯ï¼šReact/Vue.js
- åç«¯ï¼šNode.js/Python Flask
- æ•°æ®åº“ï¼šMySQL/PostgreSQL
- ç¼“å­˜ï¼šRedis
- æœç´¢ï¼šElasticsearch

å®è·µç›®æ ‡ï¼š
- å®Œæ•´çš„DockeråŒ–éƒ¨ç½²
- Kubernetesé›†ç¾¤éƒ¨ç½²
- å¤šç¯å¢ƒé…ç½®ç®¡ç†
- åŸºç¡€ç›‘æ§é…ç½®
```

#### é¡¹ç›®äºŒï¼šå¾®æœåŠ¡ç”µå•†å¹³å° (é˜¶æ®µä¸‰-å››)
```bash
æŠ€æœ¯æ ˆï¼š
- ç”¨æˆ·æœåŠ¡ï¼šJava Spring Boot
- å•†å“æœåŠ¡ï¼šPython Django
- è®¢å•æœåŠ¡ï¼šNode.js Express
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šRabbitMQ/Kafka
- APIç½‘å…³ï¼šNginx/Kong
- æ•°æ®åº“ï¼šMySQL + MongoDB

å®è·µç›®æ ‡ï¼š
- å¾®æœåŠ¡æ¶æ„è®¾è®¡
- æœåŠ¡é—´é€šä¿¡é…ç½®
- æ•°æ®ä¸€è‡´æ€§å¤„ç†
- å®Œæ•´CI/CDæµç¨‹
- è“ç»¿éƒ¨ç½²å®è·µ
```

#### é¡¹ç›®ä¸‰ï¼šå®æ—¶æ•°æ®å¤„ç†å¹³å° (é˜¶æ®µå››-äº”)
```bash
æŠ€æœ¯æ ˆï¼š
- æ•°æ®é‡‡é›†ï¼šFluentd/Filebeat
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šApache Kafka
- æµå¤„ç†ï¼šApache Flink
- æ•°æ®å­˜å‚¨ï¼šClickHouse
- å¯è§†åŒ–ï¼šGrafana
- ç›‘æ§ï¼šPrometheus

å®è·µç›®æ ‡ï¼š
- å¤§æ•°æ®å¤„ç†æµç¨‹
- å®æ—¶ç›‘æ§å’Œå‘Šè­¦
- æ€§èƒ½è°ƒä¼˜å®è·µ
- æ•…éšœæ¢å¤æœºåˆ¶
- è¿ç»´è‡ªåŠ¨åŒ–è„šæœ¬
```

### ğŸ“Š é¡¹ç›®è¯„ä¼°æ ‡å‡†

**åŸºç¡€è¦æ±‚ (60åˆ†):**
- [ ] åº”ç”¨èƒ½æ­£å¸¸è¿è¡Œ
- [ ] åŸºæœ¬çš„DockeråŒ–éƒ¨ç½²
- [ ] ç®€å•çš„K8séƒ¨ç½²é…ç½®
- [ ] åŸºç¡€çš„å¥åº·æ£€æŸ¥

**è¿›é˜¶è¦æ±‚ (80åˆ†):**
- [ ] å®Œæ•´çš„CI/CDæµç¨‹
- [ ] å¤šç¯å¢ƒéƒ¨ç½²ç­–ç•¥
- [ ] åŸºç¡€ç›‘æ§å’Œæ—¥å¿—æ”¶é›†
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•é›†æˆ

**é«˜çº§è¦æ±‚ (100åˆ†):**
- [ ] è“ç»¿/é‡‘ä¸é›€éƒ¨ç½²
- [ ] å®Œæ•´çš„ç›‘æ§å‘Šè­¦ä½“ç³»
- [ ] è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜
- [ ] æ•…éšœæ¢å¤æœºåˆ¶

---

## æ€»ç»“

è¿™ä»½ã€Šåˆçº§DevOpså·¥ç¨‹å¸ˆå®Œæ•´å­¦ä¹ æŒ‡å—ã€‹ä»åŸºç¡€ç¯å¢ƒæ­å»ºåˆ°é«˜çº§è¿ç»´è‡ªåŠ¨åŒ–ï¼Œæä¾›äº†ç³»ç»Ÿæ€§çš„å­¦ä¹ è·¯å¾„ã€‚é€šè¿‡5ä¸ªé˜¶æ®µçš„æ¸è¿›å¼å­¦ä¹ ï¼Œåˆçº§å·¥ç¨‹å¸ˆèƒ½å¤Ÿï¼š

### ğŸ¯ æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½

1. **åŸºç¡€è®¾æ–½ç®¡ç†**: Linuxç³»ç»Ÿç®¡ç†ã€ç½‘ç»œé…ç½®ã€å®‰å…¨åŠ å›º
2. **å®¹å™¨åŒ–æŠ€æœ¯**: Dockerå®¹å™¨åŒ–ã€Kubernetesé›†ç¾¤ç®¡ç†
3. **CI/CDæµç¨‹**: Jenkinsã€GitHub Actionsã€GitLab CIç­‰å·¥å…·ä½¿ç”¨
4. **ç›‘æ§è¿ç»´**: Prometheusã€Grafanaã€ELK Stackç­‰ç›‘æ§ä½“ç³»
5. **è‡ªåŠ¨åŒ–è„šæœ¬**: Bashã€Pythonç­‰è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬å¼€å‘

### ğŸ“ˆ èŒä¸šå‘å±•è·¯å¾„

å®Œæˆæœ¬æŒ‡å—å­¦ä¹ åï¼Œå¯ä»¥ç»§ç»­å‘ä»¥ä¸‹æ–¹å‘å‘å±•ï¼š

- **äº‘åŸç”Ÿæ¶æ„å¸ˆ**: æ·±å…¥å­¦ä¹ Service Meshã€Serverlessç­‰æŠ€æœ¯
- **å¹³å°å·¥ç¨‹å¸ˆ**: ä¸“æ³¨äºå¼€å‘è€…å¹³å°å’Œå·¥å…·é“¾å»ºè®¾
- **SREå·¥ç¨‹å¸ˆ**: ä¸“æ³¨äºå¯é æ€§å·¥ç¨‹å’Œå¤§è§„æ¨¡ç³»ç»Ÿè¿ç»´
- **å®‰å…¨å·¥ç¨‹å¸ˆ**: ä¸“æ³¨äºDevSecOpså’Œå®‰å…¨è‡ªåŠ¨åŒ–

### ğŸ“ æŒç»­å­¦ä¹ å»ºè®®

1. **å…³æ³¨æŠ€æœ¯è¶‹åŠ¿**: å®šæœŸäº†è§£CNCFé¡¹ç›®å’Œäº‘åŸç”ŸæŠ€æœ¯å‘å±•
2. **å‚ä¸å¼€æºé¡¹ç›®**: è´¡çŒ®ä»£ç ï¼Œå­¦ä¹ æœ€ä½³å®è·µ
3. **è€ƒå–è®¤è¯**: CKAã€CKADã€AWSç­‰ç›¸å…³è®¤è¯
4. **å®è·µé¡¹ç›®**: æŒç»­è¿›è¡Œå®é™…é¡¹ç›®ç»ƒä¹ 
5. **æŠ€æœ¯åˆ†äº«**: å†™åšå®¢ã€åšåˆ†äº«ï¼Œå·©å›ºæ‰€å­¦çŸ¥è¯†

è®°ä½ï¼ŒDevOpsæ˜¯ä¸€ä¸ªæŒç»­å­¦ä¹ å’Œå®è·µçš„é¢†åŸŸï¼Œä¿æŒå¥½å¥‡å¿ƒå’Œå­¦ä¹ çƒ­æƒ…æ˜¯æˆåŠŸçš„å…³é”®ï¼

---

*æœ¬æŒ‡å—æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿æä¾›åé¦ˆå’Œå»ºè®®*