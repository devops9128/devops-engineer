# 高级系统架构设计

> 从单体思维到分布式系统架构师的进阶指南

## 📚 目录

- [分布式系统基础理论](#分布式系统基础理论)
- [微服务架构设计原则](#微服务架构设计原则)
- [高可用性架构设计](#高可用性架构设计)
- [数据架构设计](#数据架构设计)
- [性能架构优化](#性能架构优化)
- [实践环境搭建](#实践环境搭建)
- [架构设计实战](#架构设计实战)

---

## 分布式系统基础理论

### 🎯 **CAP定理与BASE理论**

#### CAP定理核心概念
```
一致性 (Consistency)    - 所有节点同一时间看到相同数据
可用性 (Availability)   - 系统持续可用，即使部分节点故障
分区容错 (Partition)     - 网络分区时系统继续运行
```

**CAP不可能三角**:
- **CA系统**: 传统单体数据库 (MySQL, PostgreSQL)
- **CP系统**: 分布式数据库 (MongoDB, HBase)
- **AP系统**: 缓存系统 (Redis Cluster, Cassandra)

#### BASE理论实践
```bash
# BASE = Basically Available + Soft State + Eventually Consistent

# 基本可用 (Basically Available)
# 系统允许损失部分可用性来保证核心功能

# 软状态 (Soft State)  
# 系统状态可以有一定时间的延迟

# 最终一致性 (Eventually Consistent)
# 系统保证最终所有副本达到一致状态
```

#### 分布式系统挑战

**网络分区**:
```bash
# 网络分区模拟
# 使用iptables模拟网络分区
sudo iptables -A INPUT -s 192.168.1.10 -j DROP
sudo iptables -A OUTPUT -d 192.168.1.10 -j DROP

# 观察系统行为
curl -v http://api.service.com/health
```

**时钟同步**:
```bash
# NTP时间同步配置
sudo apt install ntp
sudo systemctl enable ntp

# 验证时钟同步
ntpq -p
chrony sources -v
```

### 🔄 **分布式一致性算法**

#### Raft算法实现
```go
// Raft节点状态
type NodeState int
const (
    Follower NodeState = iota
    Candidate
    Leader
)

// Raft节点结构
type RaftNode struct {
    ID          int
    State       NodeState
    CurrentTerm int
    VotedFor    int
    Log         []LogEntry
    CommitIndex int
    LastApplied int
}

// 选举超时
func (n *RaftNode) StartElection() {
    n.CurrentTerm++
    n.State = Candidate
    n.VotedFor = n.ID
    
    // 发送投票请求
    votes := 1
    for _, peer := range n.peers {
        if peer.RequestVote(n.CurrentTerm, n.ID) {
            votes++
        }
    }
    
    // 获得多数票成为Leader
    if votes > len(n.peers)/2 {
        n.State = Leader
        n.sendHeartbeats()
    }
}
```

#### 使用etcd实现分布式锁
```bash
# 安装etcd
curl -L https://github.com/etcd-io/etcd/releases/download/v3.5.0/etcd-v3.5.0-linux-amd64.tar.gz -o etcd.tar.gz
tar xzf etcd.tar.gz
sudo mv etcd-v3.5.0-linux-amd64/etcd* /usr/local/bin/

# 启动etcd集群
etcd --name node1 --data-dir /var/lib/etcd/node1 \
  --listen-client-urls http://0.0.0.0:2379 \
  --advertise-client-urls http://192.168.1.10:2379 \
  --listen-peer-urls http://0.0.0.0:2380 \
  --initial-advertise-peer-urls http://192.168.1.10:2380 \
  --initial-cluster node1=http://192.168.1.10:2380,node2=http://192.168.1.11:2380,node3=http://192.168.1.12:2380
```

```go
// Go客户端分布式锁实现
package main

import (
    "context"
    "time"
    "go.etcd.io/etcd/clientv3"
    "go.etcd.io/etcd/clientv3/concurrency"
)

func acquireDistributedLock() {
    client, err := clientv3.New(clientv3.Config{
        Endpoints:   []string{"192.168.1.10:2379"},
        DialTimeout: 5 * time.Second,
    })
    
    // 创建会话
    session, err := concurrency.NewSession(client)
    defer session.Close()
    
    // 创建互斥锁
    mutex := concurrency.NewMutex(session, "/distributed-lock/")
    
    // 获取锁
    ctx := context.Background()
    if err := mutex.Lock(ctx); err != nil {
        panic(err)
    }
    
    // 执行临界区代码
    performCriticalSection()
    
    // 释放锁
    mutex.Unlock(ctx)
}
```

---

## 微服务架构设计原则

### 🏗️ **微服务拆分策略**

#### 域驱动设计 (DDD) 拆分
```
# 电商系统服务拆分示例

用户域 (User Domain)
├── 用户服务 (User Service)
├── 认证服务 (Auth Service)
└── 用户画像服务 (Profile Service)

商品域 (Product Domain)
├── 商品服务 (Product Service) 
├── 库存服务 (Inventory Service)
└── 类目服务 (Category Service)

订单域 (Order Domain)
├── 订单服务 (Order Service)
├── 支付服务 (Payment Service)
└── 物流服务 (Shipping Service)
```

#### 服务边界定义
```yaml
# 用户服务边界定义
apiVersion: v1
kind: Service
metadata:
  name: user-service
  annotations:
    service.boundary/domain: "user"
    service.boundary/data-ownership: "user_profile,user_preferences"
    service.boundary/external-apis: "GET /users, POST /users, PUT /users/{id}"
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
---
# 服务接口契约
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-contract
data:
  openapi.yaml: |
    openapi: 3.0.0
    info:
      title: User Service API
      version: 1.0.0
    paths:
      /users:
        get:
          summary: Get users
          responses:
            '200':
              description: Successful response
        post:
          summary: Create user
          requestBody:
            required: true
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/User'
```

### 🌐 **服务间通信设计**

#### 同步通信 (HTTP/gRPC)
```proto
// user.proto - gRPC服务定义
syntax = "proto3";

package user;

service UserService {
  rpc GetUser(GetUserRequest) returns (GetUserResponse);
  rpc CreateUser(CreateUserRequest) returns (CreateUserResponse);
  rpc UpdateUser(UpdateUserRequest) returns (UpdateUserResponse);
}

message User {
  int64 id = 1;
  string email = 2;
  string name = 3;
  int64 created_at = 4;
}

message GetUserRequest {
  int64 user_id = 1;
}

message GetUserResponse {
  User user = 1;
}
```

```go
// gRPC服务实现
package main

import (
    "context"
    "net"
    "google.golang.org/grpc"
    pb "github.com/company/user-service/proto"
)

type userServiceServer struct {
    pb.UnimplementedUserServiceServer
    userRepo UserRepository
}

func (s *userServiceServer) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.GetUserResponse, error) {
    user, err := s.userRepo.GetUser(req.UserId)
    if err != nil {
        return nil, err
    }
    
    return &pb.GetUserResponse{
        User: &pb.User{
            Id:        user.ID,
            Email:     user.Email,
            Name:      user.Name,
            CreatedAt: user.CreatedAt.Unix(),
        },
    }, nil
}

func main() {
    lis, err := net.Listen("tcp", ":8080")
    if err != nil {
        panic(err)
    }
    
    server := grpc.NewServer()
    pb.RegisterUserServiceServer(server, &userServiceServer{
        userRepo: NewUserRepository(),
    })
    
    server.Serve(lis)
}
```

#### 异步通信 (消息队列)
```yaml
# Kafka配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
data:
  server.properties: |
    broker.id=1
    listeners=PLAINTEXT://0.0.0.0:9092
    advertised.listeners=PLAINTEXT://kafka:9092
    num.network.threads=3
    num.io.threads=8
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    log.dirs=/var/lib/kafka/logs
    num.partitions=3
    num.recovery.threads.per.data.dir=1
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1
    log.retention.hours=168
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000
    zookeeper.connect=zookeeper:2181
    zookeeper.connection.timeout.ms=18000
    group.initial.rebalance.delay.ms=0
```

```go
// 事件发布订阅模式
package main

import (
    "encoding/json"
    "github.com/Shopify/sarama"
)

// 用户创建事件
type UserCreatedEvent struct {
    UserID    int64  `json:"user_id"`
    Email     string `json:"email"`
    Name      string `json:"name"`
    Timestamp int64  `json:"timestamp"`
}

// 事件发布者
type EventPublisher struct {
    producer sarama.SyncProducer
}

func (p *EventPublisher) PublishUserCreated(event UserCreatedEvent) error {
    data, err := json.Marshal(event)
    if err != nil {
        return err
    }
    
    msg := &sarama.ProducerMessage{
        Topic: "user.created",
        Value: sarama.StringEncoder(data),
    }
    
    _, _, err = p.producer.SendMessage(msg)
    return err
}

// 事件消费者
type EventConsumer struct {
    consumer sarama.Consumer
}

func (c *EventConsumer) ConsumeUserCreated() {
    partitionConsumer, err := c.consumer.ConsumePartition("user.created", 0, sarama.OffsetNewest)
    if err != nil {
        panic(err)
    }
    
    for message := range partitionConsumer.Messages() {
        var event UserCreatedEvent
        if err := json.Unmarshal(message.Value, &event); err != nil {
            continue
        }
        
        // 处理用户创建事件
        handleUserCreated(event)
    }
}
```

### 🛡️ **微服务治理**

#### API网关配置
```yaml
# Kong API网关配置
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limiting
config:
  minute: 100
  hour: 1000
plugin: rate-limiting
---
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
plugin: jwt
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-gateway
  annotations:
    kubernetes.io/ingress.class: kong
    konghq.com/plugins: rate-limiting,jwt-auth
spec:
  rules:
  - host: api.company.com
    http:
      paths:
      - path: /users
        pathType: Prefix
        backend:
          service:
            name: user-service
            port:
              number: 8080
      - path: /products
        pathType: Prefix
        backend:
          service:
            name: product-service
            port:
              number: 8080
```

#### 服务发现配置
```yaml
# Consul服务发现
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consul
spec:
  replicas: 3
  selector:
    matchLabels:
      app: consul
  template:
    metadata:
      labels:
        app: consul
    spec:
      containers:
      - name: consul
        image: consul:1.15.2
        ports:
        - containerPort: 8500
        - containerPort: 8600
        env:
        - name: CONSUL_LOCAL_CONFIG
          value: |
            {
              "datacenter": "dc1",
              "data_dir": "/consul/data",
              "log_level": "INFO",
              "server": true,
              "bootstrap_expect": 3,
              "bind_addr": "0.0.0.0",
              "client_addr": "0.0.0.0",
              "retry_join": ["consul-0.consul", "consul-1.consul", "consul-2.consul"],
              "ui_config": {
                "enabled": true
              },
              "connect": {
                "enabled": true
              }
            }
        volumeMounts:
        - name: consul-data
          mountPath: /consul/data
      volumes:
      - name: consul-data
        emptyDir: {}
```

---

## 高可用性架构设计

### ⚡ **容错设计模式**

#### 熔断器模式
```go
// 熔断器实现
package main

import (
    "errors"
    "sync"
    "time"
)

type CircuitBreakerState int

const (
    Closed CircuitBreakerState = iota
    Open
    HalfOpen
)

type CircuitBreaker struct {
    mutex           sync.Mutex
    state           CircuitBreakerState
    failureCount    int
    successCount    int
    failureThreshold int
    timeout         time.Duration
    lastFailureTime time.Time
}

func NewCircuitBreaker(failureThreshold int, timeout time.Duration) *CircuitBreaker {
    return &CircuitBreaker{
        state:           Closed,
        failureThreshold: failureThreshold,
        timeout:         timeout,
    }
}

func (cb *CircuitBreaker) Call(fn func() error) error {
    cb.mutex.Lock()
    defer cb.mutex.Unlock()
    
    if cb.state == Open {
        if time.Since(cb.lastFailureTime) > cb.timeout {
            cb.state = HalfOpen
            cb.failureCount = 0
        } else {
            return errors.New("circuit breaker is open")
        }
    }
    
    err := fn()
    
    if err != nil {
        cb.onFailure()
        return err
    }
    
    cb.onSuccess()
    return nil
}

func (cb *CircuitBreaker) onFailure() {
    cb.failureCount++
    cb.lastFailureTime = time.Now()
    
    if cb.failureCount >= cb.failureThreshold {
        cb.state = Open
    }
}

func (cb *CircuitBreaker) onSuccess() {
    cb.failureCount = 0
    cb.successCount++
    
    if cb.state == HalfOpen {
        cb.state = Closed
    }
}
```

#### 重试机制
```go
// 指数退避重试
package main

import (
    "context"
    "math"
    "time"
)

type RetryConfig struct {
    MaxRetries    int
    InitialDelay  time.Duration
    MaxDelay      time.Duration
    Multiplier    float64
    Jitter        bool
}

func ExponentialBackoffRetry(ctx context.Context, config RetryConfig, fn func() error) error {
    var lastErr error
    delay := config.InitialDelay
    
    for attempt := 0; attempt <= config.MaxRetries; attempt++ {
        if attempt > 0 {
            select {
            case <-ctx.Done():
                return ctx.Err()
            case <-time.After(delay):
            }
        }
        
        if err := fn(); err == nil {
            return nil
        } else {
            lastErr = err
        }
        
        // 计算下次重试延迟
        delay = time.Duration(float64(delay) * config.Multiplier)
        if delay > config.MaxDelay {
            delay = config.MaxDelay
        }
        
        // 添加抖动
        if config.Jitter {
            jitter := time.Duration(math.Rand() * float64(delay) * 0.1)
            delay += jitter
        }
    }
    
    return lastErr
}

// 使用示例
func callExternalService() error {
    config := RetryConfig{
        MaxRetries:   3,
        InitialDelay: 100 * time.Millisecond,
        MaxDelay:     5 * time.Second,
        Multiplier:   2.0,
        Jitter:       true,
    }
    
    return ExponentialBackoffRetry(context.Background(), config, func() error {
        // 调用外部服务
        return httpClient.Get("https://api.external.com/data")
    })
}
```

### 🔄 **负载均衡策略**

#### 多级负载均衡
```yaml
# Nginx负载均衡配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    upstream backend {
        # 权重轮询
        server backend1.example.com:8080 weight=3;
        server backend2.example.com:8080 weight=2;
        server backend3.example.com:8080 weight=1;
        
        # 健康检查
        keepalive 32;
        keepalive_requests 100;
        keepalive_timeout 60s;
    }
    
    upstream backend_backup {
        server backup1.example.com:8080;
        server backup2.example.com:8080;
    }
    
    server {
        listen 80;
        server_name api.example.com;
        
        location / {
            proxy_pass http://backend;
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
            proxy_next_upstream_tries 3;
            proxy_next_upstream_timeout 10s;
            
            # 故障转移到备用服务器
            error_page 500 502 503 504 @backup;
        }
        
        location @backup {
            internal;
            proxy_pass http://backend_backup;
        }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-lb
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-lb
  template:
    metadata:
      labels:
        app: nginx-lb
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
```

#### Kubernetes HPA配置
```yaml
# 水平Pod自动扩缩容
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: custom_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### 🏥 **健康检查与监控**

#### 多维度健康检查
```go
// 健康检查服务
package main

import (
    "context"
    "encoding/json"
    "net/http"
    "time"
)

type HealthStatus string

const (
    Healthy   HealthStatus = "healthy"
    Unhealthy HealthStatus = "unhealthy"
    Degraded  HealthStatus = "degraded"
)

type HealthCheck struct {
    Name   string       `json:"name"`
    Status HealthStatus `json:"status"`
    Error  string       `json:"error,omitempty"`
}

type HealthResponse struct {
    Status    HealthStatus  `json:"status"`
    Checks    []HealthCheck `json:"checks"`
    Timestamp time.Time     `json:"timestamp"`
}

type HealthChecker interface {
    Check(ctx context.Context) HealthCheck
}

// 数据库健康检查
type DatabaseHealthChecker struct {
    db Database
}

func (d *DatabaseHealthChecker) Check(ctx context.Context) HealthCheck {
    ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
    defer cancel()
    
    if err := d.db.Ping(ctx); err != nil {
        return HealthCheck{
            Name:   "database",
            Status: Unhealthy,
            Error:  err.Error(),
        }
    }
    
    return HealthCheck{
        Name:   "database",
        Status: Healthy,
    }
}

// 外部服务健康检查
type ExternalServiceHealthChecker struct {
    client  *http.Client
    url     string
    timeout time.Duration
}

func (e *ExternalServiceHealthChecker) Check(ctx context.Context) HealthCheck {
    ctx, cancel := context.WithTimeout(ctx, e.timeout)
    defer cancel()
    
    req, err := http.NewRequestWithContext(ctx, "GET", e.url+"/health", nil)
    if err != nil {
        return HealthCheck{
            Name:   "external_service",
            Status: Unhealthy,
            Error:  err.Error(),
        }
    }
    
    resp, err := e.client.Do(req)
    if err != nil {
        return HealthCheck{
            Name:   "external_service",
            Status: Unhealthy,
            Error:  err.Error(),
        }
    }
    defer resp.Body.Close()
    
    if resp.StatusCode != http.StatusOK {
        return HealthCheck{
            Name:   "external_service",
            Status: Degraded,
            Error:  "service returning non-200 status",
        }
    }
    
    return HealthCheck{
        Name:   "external_service",
        Status: Healthy,
    }
}

// 健康检查聚合器
type HealthService struct {
    checkers []HealthChecker
}

func (h *HealthService) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
    defer cancel()
    
    checks := make([]HealthCheck, len(h.checkers))
    overallStatus := Healthy
    
    // 并行执行所有健康检查
    checkChan := make(chan struct {
        index int
        check HealthCheck
    }, len(h.checkers))
    
    for i, checker := range h.checkers {
        go func(index int, checker HealthChecker) {
            check := checker.Check(ctx)
            checkChan <- struct {
                index int
                check HealthCheck
            }{index, check}
        }(i, checker)
    }
    
    // 收集检查结果
    for i := 0; i < len(h.checkers); i++ {
        result := <-checkChan
        checks[result.index] = result.check
        
        if result.check.Status == Unhealthy {
            overallStatus = Unhealthy
        } else if result.check.Status == Degraded && overallStatus == Healthy {
            overallStatus = Degraded
        }
    }
    
    response := HealthResponse{
        Status:    overallStatus,
        Checks:    checks,
        Timestamp: time.Now(),
    }
    
    w.Header().Set("Content-Type", "application/json")
    if overallStatus == Unhealthy {
        w.WriteHeader(http.StatusServiceUnavailable)
    } else if overallStatus == Degraded {
        w.WriteHeader(http.StatusPartialContent)
    }
    
    json.NewEncoder(w).Encode(response)
}
```

---

## 数据架构设计

### 📊 **数据存储策略**

#### 读写分离架构
```yaml
# MySQL主从复制配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-master-config
data:
  my.cnf: |
    [mysqld]
    server-id = 1
    log-bin = mysql-bin
    binlog-format = ROW
    gtid-mode = ON
    enforce-gtid-consistency = ON
    log-slave-updates = ON
    read-only = OFF
    
    # 性能优化
    innodb_buffer_pool_size = 2G
    innodb_log_file_size = 256M
    innodb_flush_log_at_trx_commit = 2
    sync_binlog = 0
    
    # 连接优化
    max_connections = 1000
    max_connect_errors = 100000
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-slave-config
data:
  my.cnf: |
    [mysqld]
    server-id = 2
    log-bin = mysql-bin
    binlog-format = ROW
    gtid-mode = ON
    enforce-gtid-consistency = ON
    log-slave-updates = ON
    read-only = ON
    slave-skip-errors = 1062,1053,1146
    
    # 只读优化
    innodb_buffer_pool_size = 4G
    query_cache_size = 512M
    query_cache_type = 1
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-master
spec:
  serviceName: mysql-master
  replicas: 1
  selector:
    matchLabels:
      app: mysql-master
  template:
    metadata:
      labels:
        app: mysql-master
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: root-password
        volumeMounts:
        - name: mysql-config
          mountPath: /etc/mysql/conf.d
        - name: mysql-data
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-config
        configMap:
          name: mysql-master-config
  volumeClaimTemplates:
  - metadata:
      name: mysql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
```

#### 数据分片策略
```go
// 数据分片路由器
package main

import (
    "crypto/md5"
    "fmt"
    "hash/crc32"
)

type ShardingStrategy interface {
    GetShard(key string) int
}

// 一致性哈希分片
type ConsistentHashSharding struct {
    shardCount int
    ring       map[uint32]int
}

func NewConsistentHashSharding(shardCount int) *ConsistentHashSharding {
    ring := make(map[uint32]int)
    
    // 为每个分片创建虚拟节点
    for i := 0; i < shardCount; i++ {
        for j := 0; j < 100; j++ {
            virtualKey := fmt.Sprintf("shard_%d_virtual_%d", i, j)
            hash := crc32.ChecksumIEEE([]byte(virtualKey))
            ring[hash] = i
        }
    }
    
    return &ConsistentHashSharding{
        shardCount: shardCount,
        ring:       ring,
    }
}

func (c *ConsistentHashSharding) GetShard(key string) int {
    hash := crc32.ChecksumIEEE([]byte(key))
    
    // 找到第一个大于等于hash值的虚拟节点
    for virtualHash, shard := range c.ring {
        if virtualHash >= hash {
            return shard
        }
    }
    
    // 如果没找到，返回第一个分片
    return 0
}

// 范围分片
type RangeSharding struct {
    ranges []ShardRange
}

type ShardRange struct {
    Min   interface{}
    Max   interface{}
    Shard int
}

func (r *RangeSharding) GetShard(key string) int {
    // 根据key的范围确定分片
    for _, shardRange := range r.ranges {
        if key >= shardRange.Min.(string) && key <= shardRange.Max.(string) {
            return shardRange.Shard
        }
    }
    return 0
}

// 数据访问层
type ShardedDatabase struct {
    shards   []Database
    strategy ShardingStrategy
}

func (s *ShardedDatabase) Get(key string) (interface{}, error) {
    shardIndex := s.strategy.GetShard(key)
    return s.shards[shardIndex].Get(key)
}

func (s *ShardedDatabase) Set(key string, value interface{}) error {
    shardIndex := s.strategy.GetShard(key)
    return s.shards[shardIndex].Set(key, value)
}

// 跨分片查询
func (s *ShardedDatabase) MultiShardQuery(query string) ([]interface{}, error) {
    results := make(chan interface{}, len(s.shards))
    errors := make(chan error, len(s.shards))
    
    // 并行查询所有分片
    for _, shard := range s.shards {
        go func(db Database) {
            result, err := db.Query(query)
            if err != nil {
                errors <- err
                return
            }
            results <- result
        }(shard)
    }
    
    // 收集结果
    var allResults []interface{}
    for i := 0; i < len(s.shards); i++ {
        select {
        case result := <-results:
            allResults = append(allResults, result)
        case err := <-errors:
            return nil, err
        }
    }
    
    return allResults, nil
}
```

### 📈 **缓存架构设计**

#### 多级缓存策略
```yaml
# Redis集群配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
data:
  redis.conf: |
    # 集群配置
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 15000
    appendonly yes
    
    # 内存优化
    maxmemory 2gb
    maxmemory-policy allkeys-lru
    
    # 持久化
    save 900 1
    save 300 10
    save 60 10000
    
    # 网络优化
    tcp-keepalive 300
    timeout 0
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 6
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      containers:
      - name: redis
        image: redis:7.0
        ports:
        - containerPort: 6379
        - containerPort: 16379
        command:
        - redis-server
        - /etc/redis/redis.conf
        volumeMounts:
        - name: redis-config
          mountPath: /etc/redis
        - name: redis-data
          mountPath: /data
      volumes:
      - name: redis-config
        configMap:
          name: redis-config
  volumeClaimTemplates:
  - metadata:
      name: redis-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 50Gi
```

```go
// 多级缓存实现
package main

import (
    "context"
    "encoding/json"
    "sync"
    "time"
    
    "github.com/go-redis/redis/v8"
    "github.com/patrickmn/go-cache"
)

type CacheLevel int

const (
    L1Cache CacheLevel = iota // 内存缓存
    L2Cache                   // Redis缓存
    L3Cache                   // 数据库
)

type MultiLevelCache struct {
    l1Cache *cache.Cache
    l2Cache *redis.Client
    l3Cache Database
    mutex   sync.RWMutex
}

func NewMultiLevelCache(redisClient *redis.Client, database Database) *MultiLevelCache {
    return &MultiLevelCache{
        l1Cache: cache.New(5*time.Minute, 10*time.Minute),
        l2Cache: redisClient,
        l3Cache: database,
    }
}

func (m *MultiLevelCache) Get(ctx context.Context, key string) (interface{}, error) {
    // L1缓存查询
    if value, found := m.l1Cache.Get(key); found {
        return value, nil
    }
    
    // L2缓存查询
    result, err := m.l2Cache.Get(ctx, key).Result()
    if err == nil {
        var value interface{}
        if err := json.Unmarshal([]byte(result), &value); err == nil {
            // 更新L1缓存
            m.l1Cache.Set(key, value, cache.DefaultExpiration)
            return value, nil
        }
    }
    
    // L3数据库查询
    value, err := m.l3Cache.Get(key)
    if err != nil {
        return nil, err
    }
    
    // 更新L2缓存
    data, _ := json.Marshal(value)
    m.l2Cache.Set(ctx, key, data, time.Hour)
    
    // 更新L1缓存
    m.l1Cache.Set(key, value, cache.DefaultExpiration)
    
    return value, nil
}

func (m *MultiLevelCache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error {
    // 更新数据库
    if err := m.l3Cache.Set(key, value); err != nil {
        return err
    }
    
    // 更新L2缓存
    data, _ := json.Marshal(value)
    m.l2Cache.Set(ctx, key, data, ttl)
    
    // 更新L1缓存
    m.l1Cache.Set(key, value, ttl)
    
    return nil
}

func (m *MultiLevelCache) Delete(ctx context.Context, key string) error {
    // 删除L1缓存
    m.l1Cache.Delete(key)
    
    // 删除L2缓存
    m.l2Cache.Del(ctx, key)
    
    // 删除数据库记录
    return m.l3Cache.Delete(key)
}

// 缓存预热
func (m *MultiLevelCache) Warmup(ctx context.Context, keys []string) error {
    for _, key := range keys {
        go func(k string) {
            m.Get(ctx, k)
        }(key)
    }
    return nil
}
```

---

## 性能架构优化

### ⚡ **性能监控体系**

#### APM集成配置
```yaml
# Jaeger分布式追踪
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-all-in-one
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.45
        ports:
        - containerPort: 16686
        - containerPort: 14268
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: SPAN_STORAGE_TYPE
          value: "elasticsearch"
        - name: ES_SERVER_URLS
          value: "http://elasticsearch:9200"
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-service
spec:
  selector:
    app: jaeger
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: collector
    port: 14268
    targetPort: 14268
  - name: agent-udp-1
    port: 6831
    targetPort: 6831
    protocol: UDP
  - name: agent-udp-2
    port: 6832
    targetPort: 6832
    protocol: UDP
```

```go
// OpenTelemetry集成
package main

import (
    "context"
    "log"
    
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/jaeger"
    "go.opentelemetry.io/otel/sdk/resource"
    "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
)

func initTracer() func() {
    // 创建Jaeger导出器
    exp, err := jaeger.New(jaeger.WithCollectorEndpoint(jaeger.WithEndpoint("http://jaeger:14268/api/traces")))
    if err != nil {
        log.Fatal(err)
    }
    
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exp),
        trace.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceName("user-service"),
            semconv.ServiceVersion("v1.0.0"),
        )),
    )
    
    otel.SetTracerProvider(tp)
    
    return func() {
        if err := tp.Shutdown(context.Background()); err != nil {
            log.Fatal(err)
        }
    }
}

// 业务方法追踪
func GetUser(ctx context.Context, userID string) (*User, error) {
    tracer := otel.Tracer("user-service")
    ctx, span := tracer.Start(ctx, "GetUser")
    defer span.End()
    
    span.SetAttributes(
        attribute.String("user.id", userID),
        attribute.String("operation", "get_user"),
    )
    
    // 数据库查询追踪
    user, err := getUserFromDB(ctx, userID)
    if err != nil {
        span.RecordError(err)
        return nil, err
    }
    
    span.SetAttributes(
        attribute.String("user.email", user.Email),
        attribute.Bool("user.active", user.Active),
    )
    
    return user, nil
}

func getUserFromDB(ctx context.Context, userID string) (*User, error) {
    tracer := otel.Tracer("user-service")
    ctx, span := tracer.Start(ctx, "getUserFromDB")
    defer span.End()
    
    span.SetAttributes(
        attribute.String("db.system", "postgresql"),
        attribute.String("db.operation", "SELECT"),
        attribute.String("db.statement", "SELECT * FROM users WHERE id = ?"),
    )
    
    // 执行数据库查询
    return db.QueryUser(ctx, userID)
}
```

### 📊 **性能基准测试**

#### 压力测试配置
```yaml
# K6性能测试
apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-test-script
data:
  load-test.js: |
    import http from 'k6/http';
    import { check, sleep } from 'k6';
    
    export let options = {
      stages: [
        { duration: '2m', target: 100 },  // 2分钟内逐渐增加到100个用户
        { duration: '5m', target: 100 },  // 5分钟保持100个用户
        { duration: '2m', target: 200 },  // 2分钟内增加到200个用户
        { duration: '5m', target: 200 },  // 5分钟保持200个用户
        { duration: '2m', target: 0 },    // 2分钟内减少到0个用户
      ],
      thresholds: {
        http_req_duration: ['p(95)<500'],  // 95%的请求响应时间小于500ms
        http_req_failed: ['rate<0.1'],     // 错误率小于10%
      },
    };
    
    export default function() {
      // 测试用户API
      let response = http.get('http://user-service:8080/users/123');
      check(response, {
        'status is 200': (r) => r.status === 200,
        'response time < 500ms': (r) => r.timings.duration < 500,
      });
      
      // 测试创建用户API
      let payload = JSON.stringify({
        name: 'Test User',
        email: 'test@example.com',
      });
      
      response = http.post('http://user-service:8080/users', payload, {
        headers: { 'Content-Type': 'application/json' },
      });
      
      check(response, {
        'create user status is 201': (r) => r.status === 201,
      });
      
      sleep(1);
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: k6-load-test
spec:
  template:
    spec:
      containers:
      - name: k6
        image: grafana/k6:latest
        command: ["k6", "run", "/scripts/load-test.js"]
        volumeMounts:
        - name: test-script
          mountPath: /scripts
      volumes:
      - name: test-script
        configMap:
          name: k6-test-script
      restartPolicy: Never
```

#### 性能分析和优化
```go
// 性能分析工具集成
package main

import (
    "context"
    "net/http"
    _ "net/http/pprof"
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

// Prometheus指标定义
var (
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status"},
    )
    
    httpRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "Duration of HTTP requests in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
    
    databaseConnectionPool = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "database_connections",
            Help: "Number of database connections",
        },
        []string{"state"}, // active, idle, waiting
    )
    
    cacheHitRatio = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cache_hit_ratio",
            Help: "Cache hit ratio",
        },
        []string{"cache_level"},
    )
)

// 性能监控中间件
func performanceMiddleware(next http.HandlerFunc) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        
        // 包装ResponseWriter以捕获状态码
        wrappedWriter := &responseWriter{ResponseWriter: w, statusCode: 200}
        
        // 执行请求
        next(wrappedWriter, r)
        
        // 记录指标
        duration := time.Since(start).Seconds()
        httpRequestDuration.WithLabelValues(r.Method, r.URL.Path).Observe(duration)
        httpRequestsTotal.WithLabelValues(r.Method, r.URL.Path, string(wrappedWriter.statusCode)).Inc()
    }
}

type responseWriter struct {
    http.ResponseWriter
    statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
    rw.statusCode = code
    rw.ResponseWriter.WriteHeader(code)
}

// 数据库连接池监控
func monitorDatabasePool(db *sql.DB) {
    go func() {
        ticker := time.NewTicker(30 * time.Second)
        defer ticker.Stop()
        
        for range ticker.C {
            stats := db.Stats()
            databaseConnectionPool.WithLabelValues("open").Set(float64(stats.OpenConnections))
            databaseConnectionPool.WithLabelValues("in_use").Set(float64(stats.InUse))
            databaseConnectionPool.WithLabelValues("idle").Set(float64(stats.Idle))
            databaseConnectionPool.WithLabelValues("waiting").Set(float64(stats.WaitCount))
        }
    }()
}

// 缓存性能监控
type CacheMetrics struct {
    hits   int64
    misses int64
}

func (c *CacheMetrics) RecordHit() {
    atomic.AddInt64(&c.hits, 1)
    c.updateRatio()
}

func (c *CacheMetrics) RecordMiss() {
    atomic.AddInt64(&c.misses, 1)
    c.updateRatio()
}

func (c *CacheMetrics) updateRatio() {
    total := c.hits + c.misses
    if total > 0 {
        ratio := float64(c.hits) / float64(total)
        cacheHitRatio.WithLabelValues("l1").Set(ratio)
    }
}

func main() {
    // 启动pprof服务
    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()
    
    // 启动Prometheus指标服务
    http.Handle("/metrics", promhttp.Handler())
    
    // 业务路由
    http.HandleFunc("/users", performanceMiddleware(getUsersHandler))
    http.HandleFunc("/health", healthCheckHandler)
    
    http.ListenAndServe(":8080", nil)
}
```

---

## 实践环境搭建

### 🖥️ **完整实验环境**

#### 基础设施搭建
```bash
#!/bin/bash
# 高级系统架构实验环境搭建脚本

# 1. 创建Kubernetes集群
echo "创建Kubernetes集群..."
kind create cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
- role: worker
- role: worker
- role: worker
EOF

# 2. 安装Istio Service Mesh
echo "安装Istio..."
curl -L https://istio.io/downloadIstio | sh -
sudo mv istio-*/bin/istioctl /usr/local/bin/
istioctl install --set values.defaultRevision=default -y
kubectl label namespace default istio-injection=enabled

# 3. 安装监控栈
echo "安装Prometheus + Grafana..."
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin123

# 4. 安装Jaeger
echo "安装Jaeger..."
kubectl create namespace observability
kubectl apply -n observability -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.45.0/jaeger-operator.yaml

# 5. 安装数据库
echo "安装PostgreSQL..."
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install postgresql bitnami/postgresql \
  --set auth.postgresPassword=postgres123 \
  --set primary.persistence.size=20Gi

# 6. 安装Redis集群
echo "安装Redis..."
helm install redis bitnami/redis-cluster \
  --set auth.enabled=false \
  --set persistence.size=10Gi

# 7. 安装消息队列
echo "安装Kafka..."
helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/
helm install kafka confluentinc/cp-helm-charts \
  --set cp-schema-registry.enabled=false \
  --set cp-kafka-rest.enabled=false \
  --set cp-kafka-connect.enabled=false

echo "环境搭建完成！"
echo "访问地址："
echo "- Grafana: http://localhost:3000 (admin/admin123)"
echo "- Prometheus: http://localhost:9090"
echo "- Jaeger: http://localhost:16686"
```

#### 开发工具配置
```yaml
# VS Code开发环境配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: vscode-config
data:
  settings.json: |
    {
      "go.toolsManagement.autoUpdate": true,
      "go.useLanguageServer": true,
      "go.lintTool": "golangci-lint",
      "go.formatTool": "goimports",
      "kubernetes.defaultNamespace": "default",
      "kubernetes.outputFormat": "yaml",
      "docker.showStartPage": false,
      "yaml.schemas": {
        "kubernetes": "*.k8s.yaml"
      },
      "terraform.experimentalFeatures": {
        "validateOnSave": true
      }
    }
  
  extensions.txt: |
    ms-vscode.Go
    ms-kubernetes-tools.vscode-kubernetes-tools
    ms-azuretools.vscode-docker
    HashiCorp.terraform
    redhat.vscode-yaml
    ms-vscode.vscode-json
    Gruntfuggly.todo-tree
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vscode-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vscode-server
  template:
    metadata:
      labels:
        app: vscode-server
    spec:
      containers:
      - name: vscode
        image: codercom/code-server:latest
        ports:
        - containerPort: 8080
        env:
        - name: PASSWORD
          value: "codeserver123"
        volumeMounts:
        - name: workspace
          mountPath: /home/coder/workspace
        - name: config
          mountPath: /home/coder/.local/share/code-server/User
      volumes:
      - name: workspace
        persistentVolumeClaim:
          claimName: workspace-pvc
      - name: config
        configMap:
          name: vscode-config
```

---

## 架构设计实战

### 🎯 **实战项目：分布式电商平台**

#### 系统架构设计
```
                    ┌─────────────────┐
                    │   Load Balancer │
                    │   (Nginx/Kong)  │
                    └─────────┬───────┘
                              │
              ┌───────────────┴───────────────┐
              │                               │
    ┌─────────▼─────────┐           ┌─────────▼─────────┐
    │   API Gateway     │           │   Web Gateway     │
    │   (Kong/Zuul)     │           │   (Nginx/Traefik) │
    └─────────┬─────────┘           └─────────┬─────────┘
              │                               │
    ┌─────────▼─────────────────────────────────────────┐
    │              Service Mesh (Istio)                │
    └─────────┬───────────────────────────────┬─────────┘
              │                               │
    ┌─────────▼─────────┐           ┌─────────▼─────────┐
    │   User Services   │           │ Product Services  │
    │                   │           │                   │
    │ ┌───────────────┐ │           │ ┌───────────────┐ │
    │ │ User Service  │ │           │ │Product Service│ │
    │ │ Auth Service  │ │           │ │Inventory Svc  │ │
    │ │Profile Service│ │           │ │Category Svc   │ │
    │ └───────────────┘ │           │ └───────────────┘ │
    └─────────┬─────────┘           └─────────┬─────────┘
              │                               │
    ┌─────────▼─────────┐           ┌─────────▼─────────┐
    │   Order Services  │           │ Payment Services  │
    │                   │           │                   │
    │ ┌───────────────┐ │           │ ┌───────────────┐ │
    │ │ Order Service │ │           │ │Payment Service│ │
    │ │Cart Service   │ │           │ │Invoice Service│ │
    │ │Shipping Svc   │ │           │ │Refund Service │ │
    │ └───────────────┘ │           │ └───────────────┘ │
    └─────────┬─────────┘           └─────────┬─────────┘
              │                               │
    ┌─────────▼───────────────────────────────▼─────────┐
    │                Data Layer                        │
    │                                                  │
    │ ┌──────────┐ ┌──────────┐ ┌──────────┐          │
    │ │PostgreSQL│ │  Redis   │ │  Kafka   │          │
    │ │ Cluster  │ │ Cluster  │ │ Cluster  │          │
    │ └──────────┘ └──────────┘ └──────────┘          │
    └──────────────────────────────────────────────────┘
```

#### 微服务实现

**用户服务实现**:
```go
// cmd/user-service/main.go
package main

import (
    "context"
    "log"
    "net"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/reflection"
    
    "github.com/company/ecommerce/internal/user"
    pb "github.com/company/ecommerce/proto/user"
)

func main() {
    lis, err := net.Listen("tcp", ":8080")
    if err != nil {
        log.Fatalf("Failed to listen: %v", err)
    }
    
    // 创建用户服务
    userRepo := user.NewPostgreSQLRepository()
    userService := user.NewService(userRepo)
    
    // 创建gRPC服务器
    server := grpc.NewServer()
    pb.RegisterUserServiceServer(server, &userServiceServer{
        service: userService,
    })
    
    // 启用反射（用于调试）
    reflection.Register(server)
    
    log.Println("User service starting on :8080")
    if err := server.Serve(lis); err != nil {
        log.Fatalf("Failed to serve: %v", err)
    }
}

type userServiceServer struct {
    pb.UnimplementedUserServiceServer
    service user.Service
}

func (s *userServiceServer) CreateUser(ctx context.Context, req *pb.CreateUserRequest) (*pb.CreateUserResponse, error) {
    user := &user.User{
        Email:    req.Email,
        Name:     req.Name,
        Password: req.Password,
    }
    
    createdUser, err := s.service.CreateUser(ctx, user)
    if err != nil {
        return nil, err
    }
    
    return &pb.CreateUserResponse{
        User: &pb.User{
            Id:    createdUser.ID,
            Email: createdUser.Email,
            Name:  createdUser.Name,
        },
    }, nil
}
```

**订单服务实现**:
```go
// internal/order/service.go
package order

import (
    "context"
    "fmt"
    
    "github.com/company/ecommerce/internal/events"
)

type Service interface {
    CreateOrder(ctx context.Context, order *Order) (*Order, error)
    GetOrder(ctx context.Context, orderID string) (*Order, error)
    UpdateOrderStatus(ctx context.Context, orderID string, status OrderStatus) error
}

type service struct {
    repo      Repository
    eventBus  events.EventBus
    inventory InventoryClient
    payment   PaymentClient
}

func NewService(repo Repository, eventBus events.EventBus, inventory InventoryClient, payment PaymentClient) Service {
    return &service{
        repo:      repo,
        eventBus:  eventBus,
        inventory: inventory,
        payment:   payment,
    }
}

func (s *service) CreateOrder(ctx context.Context, order *Order) (*Order, error) {
    // 1. 验证库存
    for _, item := range order.Items {
        available, err := s.inventory.CheckAvailability(ctx, item.ProductID, item.Quantity)
        if err != nil {
            return nil, fmt.Errorf("failed to check inventory: %w", err)
        }
        if !available {
            return nil, fmt.Errorf("insufficient inventory for product %s", item.ProductID)
        }
    }
    
    // 2. 预扣库存
    for _, item := range order.Items {
        if err := s.inventory.ReserveInventory(ctx, item.ProductID, item.Quantity); err != nil {
            // 回滚已预扣的库存
            s.rollbackInventoryReservation(ctx, order)
            return nil, fmt.Errorf("failed to reserve inventory: %w", err)
        }
    }
    
    // 3. 创建订单
    order.Status = OrderStatusPending
    createdOrder, err := s.repo.Create(ctx, order)
    if err != nil {
        s.rollbackInventoryReservation(ctx, order)
        return nil, fmt.Errorf("failed to create order: %w", err)
    }
    
    // 4. 发布订单创建事件
    event := &events.OrderCreatedEvent{
        OrderID:   createdOrder.ID,
        UserID:    createdOrder.UserID,
        Amount:    createdOrder.TotalAmount,
        Timestamp: createdOrder.CreatedAt,
    }
    
    if err := s.eventBus.Publish(ctx, "order.created", event); err != nil {
        log.Printf("Failed to publish order created event: %v", err)
    }
    
    return createdOrder, nil
}

func (s *service) rollbackInventoryReservation(ctx context.Context, order *Order) {
    for _, item := range order.Items {
        if err := s.inventory.ReleaseReservation(ctx, item.ProductID, item.Quantity); err != nil {
            log.Printf("Failed to rollback inventory reservation: %v", err)
        }
    }
}
```

#### 部署配置

**Kubernetes部署清单**:
```yaml
# user-service部署
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  labels:
    app: user-service
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: user-service
        image: ecommerce/user-service:v1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secret
              key: url
        - name: REDIS_URL
          value: "redis://redis-cluster:6379"
        - name: JAEGER_ENDPOINT
          value: "http://jaeger-collector:14268/api/traces"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          grpc:
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  labels:
    app: user-service
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
    name: grpc
---
# Istio VirtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - match:
    - uri:
        prefix: /users
    route:
    - destination:
        host: user-service
        port:
          number: 8080
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
    retries:
      attempts: 3
      perTryTimeout: 10s
```

### ✅ **验收标准**

完成本章学习后，你应该能够：

**架构设计能力**:
- [ ] 设计支持百万用户的分布式系统架构
- [ ] 合理选择和应用分布式系统设计模式
- [ ] 设计高可用、高性能的数据架构
- [ ] 实施有效的容错和恢复机制

**技术实现能力**:
- [ ] 实现完整的微服务系统
- [ ] 配置和管理Service Mesh
- [ ] 建立分布式追踪和监控体系
- [ ] 进行系统性能调优和容量规划

**问题解决能力**:
- [ ] 快速定位和解决分布式系统问题
- [ ] 进行系统性能瓶颈分析
- [ ] 设计和实施系统扩容方案
- [ ] 处理复杂的数据一致性问题

---

*下一章: [微服务架构实践](./微服务架构实践.md)*