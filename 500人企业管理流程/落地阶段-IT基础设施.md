# è½åœ°é˜¶æ®µ - ITåŸºç¡€è®¾æ–½ç®¡ç†

## é˜¶æ®µæ¦‚è¿°
è½åœ°é˜¶æ®µæ˜¯ç³»ç»Ÿæ­£å¼æŠ•å…¥ç”Ÿäº§ä½¿ç”¨çš„å…³é”®æ—¶æœŸï¼ŒåŒ…æ‹¬æ­£å¼ä¸Šçº¿ã€è¿è¡Œç›‘æ§ã€é—®é¢˜å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–ç­‰æ ¸å¿ƒæ´»åŠ¨ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œå¹¶æ»¡è¶³ä¸šåŠ¡éœ€æ±‚ã€‚

## 1. æ­£å¼ä¸Šçº¿ç®¡ç†

### 1.1 ä¸Šçº¿å‡†å¤‡æ£€æŸ¥æ¸…å•

#### æŠ€æœ¯å‡†å¤‡éªŒè¯
```yaml
ä¸Šçº¿å‰æŠ€æœ¯æ£€æŸ¥æ¸…å•:
  åŸºç¡€è®¾æ–½æ£€æŸ¥:
    - [ ] æ‰€æœ‰æœåŠ¡å™¨è¿è¡Œæ­£å¸¸
    - [ ] ç½‘ç»œè¿é€šæ€§æµ‹è¯•é€šè¿‡
    - [ ] å­˜å‚¨ç³»ç»Ÿå·¥ä½œæ­£å¸¸
    - [ ] è´Ÿè½½å‡è¡¡é…ç½®æ­£ç¡®
    - [ ] SSLè¯ä¹¦æœ‰æ•ˆä¸”é…ç½®æ­£ç¡®
    - [ ] é˜²ç«å¢™è§„åˆ™é…ç½®å®Œæˆ
    - [ ] DNSè§£æé…ç½®æ­£ç¡®
    
  åº”ç”¨ç³»ç»Ÿæ£€æŸ¥:
    - [ ] æ‰€æœ‰åº”ç”¨æœåŠ¡å¯åŠ¨æˆåŠŸ
    - [ ] æ•°æ®åº“è¿æ¥æµ‹è¯•é€šè¿‡
    - [ ] ç¼“å­˜ç³»ç»Ÿå·¥ä½œæ­£å¸¸
    - [ ] APIæ¥å£æµ‹è¯•é€šè¿‡
    - [ ] æ–‡ä»¶ä¸Šä¼ ä¸‹è½½åŠŸèƒ½æ­£å¸¸
    - [ ] ç”¨æˆ·è®¤è¯ç³»ç»Ÿæ­£å¸¸
    
  ç›‘æ§ç³»ç»Ÿæ£€æŸ¥:
    - [ ] ç›‘æ§ç³»ç»Ÿé‡‡é›†æ•°æ®æ­£å¸¸
    - [ ] å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
    - [ ] å‘Šè­¦é€šçŸ¥æ¸ é“æµ‹è¯•é€šè¿‡
    - [ ] æ—¥å¿—æ”¶é›†ç³»ç»Ÿæ­£å¸¸
    - [ ] æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿å°±ç»ª
    
  å¤‡ä»½ç³»ç»Ÿæ£€æŸ¥:
    - [ ] è‡ªåŠ¨å¤‡ä»½ä»»åŠ¡é…ç½®å®Œæˆ
    - [ ] å¤‡ä»½æ•°æ®å®Œæ•´æ€§éªŒè¯
    - [ ] æ¢å¤æµç¨‹æµ‹è¯•é€šè¿‡
    - [ ] å¼‚åœ°å¤‡ä»½é…ç½®æ­£ç¡®
```

#### ä¸šåŠ¡å‡†å¤‡éªŒè¯
```yaml
ä¸Šçº¿å‰ä¸šåŠ¡æ£€æŸ¥æ¸…å•:
  ç”¨æˆ·åŸ¹è®­:
    - [ ] ç®¡ç†å‘˜åŸ¹è®­å®Œæˆ
    - [ ] ç»ˆç«¯ç”¨æˆ·åŸ¹è®­å®Œæˆ
    - [ ] æ“ä½œæ‰‹å†Œå‘å¸ƒ
    - [ ] å¸¸è§é—®é¢˜FAQå‡†å¤‡
    
  æµç¨‹å‡†å¤‡:
    - [ ] ä¸šåŠ¡æµç¨‹æ–‡æ¡£æ›´æ–°
    - [ ] åº”æ€¥å“åº”æµç¨‹ç¡®è®¤
    - [ ] å˜æ›´ç®¡ç†æµç¨‹å°±ç»ª
    - [ ] æ•…éšœå¤„ç†æµç¨‹æ˜ç¡®
    
  æ”¯æŒå‡†å¤‡:
    - [ ] æŠ€æœ¯æ”¯æŒå›¢é˜Ÿå°±ä½
    - [ ] 7x24å°æ—¶å€¼ç­å®‰æ’
    - [ ] ä¾›åº”å•†æ”¯æŒçƒ­çº¿ç¡®è®¤
    - [ ] å‡çº§è”ç³»äººæ¸…å•å‡†å¤‡
```

### 1.2 ä¸Šçº¿å®æ–½æµç¨‹

#### åˆ†é˜¶æ®µä¸Šçº¿ç­–ç•¥
```bash
#!/bin/bash
# åˆ†é˜¶æ®µä¸Šçº¿è‡ªåŠ¨åŒ–è„šæœ¬

# é˜¶æ®µ1: å†…éƒ¨ç”¨æˆ·æµ‹è¯• (10%æµé‡)
phase1_rollout() {
    echo "=== é˜¶æ®µ1: å†…éƒ¨ç”¨æˆ·æµ‹è¯•å¼€å§‹ ==="
    
    # é…ç½®è´Ÿè½½å‡è¡¡å™¨ï¼Œå°†10%æµé‡å¯¼å‘æ–°ç³»ç»Ÿ
    curl -X POST "https://lb.company.local/api/config" \
         -H "Authorization: Bearer $LB_API_TOKEN" \
         -H "Content-Type: application/json" \
         -d '{
           "traffic_split": {
             "new_system": 10,
             "old_system": 90
           },
           "target_users": ["internal"]
         }'
    
    # ç›‘æ§å…³é”®æŒ‡æ ‡
    monitor_metrics 30  # ç›‘æ§30åˆ†é’Ÿ
    
    if check_health_status; then
        echo "âœ“ é˜¶æ®µ1æˆåŠŸï¼Œå‡†å¤‡è¿›å…¥é˜¶æ®µ2"
        return 0
    else
        echo "âœ— é˜¶æ®µ1å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š"
        rollback_phase1
        return 1
    fi
}

# é˜¶æ®µ2: éƒ¨åˆ†ç”¨æˆ·æµ‹è¯• (30%æµé‡)  
phase2_rollout() {
    echo "=== é˜¶æ®µ2: éƒ¨åˆ†ç”¨æˆ·æµ‹è¯•å¼€å§‹ ==="
    
    curl -X POST "https://lb.company.local/api/config" \
         -H "Authorization: Bearer $LB_API_TOKEN" \
         -H "Content-Type: application/json" \
         -d '{
           "traffic_split": {
             "new_system": 30,
             "old_system": 70
           }
         }'
    
    monitor_metrics 60  # ç›‘æ§60åˆ†é’Ÿ
    
    if check_health_status; then
        echo "âœ“ é˜¶æ®µ2æˆåŠŸï¼Œå‡†å¤‡è¿›å…¥é˜¶æ®µ3"
        return 0
    else
        echo "âœ— é˜¶æ®µ2å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š"
        rollback_phase2
        return 1
    fi
}

# é˜¶æ®µ3: å…¨é‡ä¸Šçº¿ (100%æµé‡)
phase3_rollout() {
    echo "=== é˜¶æ®µ3: å…¨é‡ä¸Šçº¿å¼€å§‹ ==="
    
    curl -X POST "https://lb.company.local/api/config" \
         -H "Authorization: Bearer $LB_API_TOKEN" \
         -H "Content-Type: application/json" \
         -d '{
           "traffic_split": {
             "new_system": 100,
             "old_system": 0
           }
         }'
    
    monitor_metrics 120  # ç›‘æ§120åˆ†é’Ÿ
    
    if check_health_status; then
        echo "âœ“ å…¨é‡ä¸Šçº¿æˆåŠŸ"
        notify_stakeholders "ä¸Šçº¿æˆåŠŸ"
        return 0
    else
        echo "âœ— å…¨é‡ä¸Šçº¿å¤±è´¥ï¼Œæ‰§è¡Œç´§æ€¥å›æ»š"
        emergency_rollback
        return 1
    fi
}

# å¥åº·çŠ¶æ€æ£€æŸ¥
check_health_status() {
    local error_rate=$(get_error_rate)
    local response_time=$(get_avg_response_time)
    local cpu_usage=$(get_cpu_usage)
    local memory_usage=$(get_memory_usage)
    
    echo "å½“å‰ç³»ç»ŸæŒ‡æ ‡:"
    echo "  é”™è¯¯ç‡: ${error_rate}%"
    echo "  å¹³å‡å“åº”æ—¶é—´: ${response_time}ms"
    echo "  CPUä½¿ç”¨ç‡: ${cpu_usage}%"
    echo "  å†…å­˜ä½¿ç”¨ç‡: ${memory_usage}%"
    
    # å¥åº·çŠ¶æ€é˜ˆå€¼æ£€æŸ¥
    if (( $(echo "$error_rate > 5" | bc -l) )); then
        echo "âœ— é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼(5%)"
        return 1
    fi
    
    if (( $(echo "$response_time > 3000" | bc -l) )); then
        echo "âœ— å“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼(3000ms)"
        return 1
    fi
    
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        echo "âœ— CPUä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼(80%)"
        return 1
    fi
    
    if (( $(echo "$memory_usage > 85" | bc -l) )); then
        echo "âœ— å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼(85%)"
        return 1
    fi
    
    echo "âœ“ æ‰€æœ‰å¥åº·æŒ‡æ ‡æ­£å¸¸"
    return 0
}

# æ‰§è¡Œåˆ†é˜¶æ®µä¸Šçº¿
main() {
    echo "å¼€å§‹åˆ†é˜¶æ®µä¸Šçº¿æµç¨‹..."
    
    if phase1_rollout; then
        if phase2_rollout; then
            if phase3_rollout; then
                echo "ğŸ‰ ä¸Šçº¿æµç¨‹å…¨éƒ¨å®Œæˆï¼"
                cleanup_old_system
            fi
        fi
    fi
}

# æ‰§è¡Œä¸»æµç¨‹
main
```

#### æ•°æ®è¿ç§»ç®¡ç†
```sql
-- æ•°æ®è¿ç§»è„šæœ¬ç¤ºä¾‹

-- 1. åˆ›å»ºè¿ç§»æ—¥å¿—è¡¨
CREATE TABLE migration_log (
    id SERIAL PRIMARY KEY,
    migration_name VARCHAR(255) NOT NULL,
    start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    end_time TIMESTAMP,
    status VARCHAR(50),
    error_message TEXT,
    rows_migrated INTEGER DEFAULT 0
);

-- 2. ç”¨æˆ·æ•°æ®è¿ç§»
DO $$
DECLARE
    migration_name VARCHAR(255) := 'ç”¨æˆ·æ•°æ®è¿ç§»';
    start_time TIMESTAMP := CURRENT_TIMESTAMP;
    migrated_count INTEGER := 0;
    error_msg TEXT := '';
BEGIN
    -- è®°å½•è¿ç§»å¼€å§‹
    INSERT INTO migration_log (migration_name, start_time, status) 
    VALUES (migration_name, start_time, 'RUNNING');
    
    -- æ‰§è¡Œè¿ç§»
    BEGIN
        -- è¿ç§»ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        INSERT INTO new_users (
            user_id, username, email, phone, 
            created_at, updated_at, status
        )
        SELECT 
            id, login_name, email_address, phone_number,
            create_time, modify_time, user_status
        FROM old_users 
        WHERE migrate_flag = 0;
        
        GET DIAGNOSTICS migrated_count = ROW_COUNT;
        
        -- æ›´æ–°è¿ç§»æ ‡å¿—
        UPDATE old_users SET migrate_flag = 1 WHERE migrate_flag = 0;
        
        -- æ›´æ–°è¿ç§»æ—¥å¿—
        UPDATE migration_log 
        SET end_time = CURRENT_TIMESTAMP, 
            status = 'COMPLETED',
            rows_migrated = migrated_count
        WHERE migration_name = migration_name 
        AND start_time = start_time;
        
        RAISE NOTICE 'ç”¨æˆ·æ•°æ®è¿ç§»å®Œæˆï¼Œå…±è¿ç§» % æ¡è®°å½•', migrated_count;
        
    EXCEPTION WHEN OTHERS THEN
        error_msg := SQLERRM;
        
        -- è®°å½•é”™è¯¯
        UPDATE migration_log 
        SET end_time = CURRENT_TIMESTAMP, 
            status = 'FAILED',
            error_message = error_msg
        WHERE migration_name = migration_name 
        AND start_time = start_time;
        
        RAISE EXCEPTION 'ç”¨æˆ·æ•°æ®è¿ç§»å¤±è´¥: %', error_msg;
    END;
END $$;

-- 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
DO $$
DECLARE
    old_count INTEGER;
    new_count INTEGER;
    consistency_check BOOLEAN := TRUE;
BEGIN
    -- æ£€æŸ¥ç”¨æˆ·æ•°é‡ä¸€è‡´æ€§
    SELECT COUNT(*) INTO old_count FROM old_users WHERE migrate_flag = 1;
    SELECT COUNT(*) INTO new_count FROM new_users;
    
    IF old_count != new_count THEN
        consistency_check := FALSE;
        RAISE WARNING 'ç”¨æˆ·æ•°é‡ä¸ä¸€è‡´: æ—§ç³»ç»Ÿ=%, æ–°ç³»ç»Ÿ=%', old_count, new_count;
    END IF;
    
    -- æ£€æŸ¥å…³é”®å­—æ®µä¸€è‡´æ€§
    PERFORM 1 FROM old_users o 
    LEFT JOIN new_users n ON o.id = n.user_id 
    WHERE o.migrate_flag = 1 AND n.user_id IS NULL;
    
    IF FOUND THEN
        consistency_check := FALSE;
        RAISE WARNING 'å‘ç°æ•°æ®ä¸ä¸€è‡´çš„ç”¨æˆ·è®°å½•';
    END IF;
    
    IF consistency_check THEN
        RAISE NOTICE 'âœ“ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡';
    ELSE
        RAISE EXCEPTION 'âœ— æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥';
    END IF;
END $$;
```

### 1.3 ç”¨æˆ·åˆ‡æ¢ç®¡ç†

#### ç”¨æˆ·åˆ†æ‰¹åˆ‡æ¢ç­–ç•¥
```python
#!/usr/bin/env python3
# ç”¨æˆ·åˆ†æ‰¹åˆ‡æ¢ç®¡ç†è„šæœ¬

import time
import json
import requests
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UserMigrationManager:
    def __init__(self):
        self.api_base_url = "https://api.company.local"
        self.api_token = "your_api_token"
        self.migration_batches = []
        
    def create_migration_batches(self):
        """åˆ›å»ºç”¨æˆ·è¿ç§»æ‰¹æ¬¡"""
        
        # æ‰¹æ¬¡1: ITéƒ¨é—¨ç”¨æˆ· (ä¼˜å…ˆè¿ç§»ï¼Œç†Ÿæ‚‰ç³»ç»Ÿ)
        batch1 = {
            'batch_id': 1,
            'name': 'ITéƒ¨é—¨ç”¨æˆ·',
            'user_filter': {'department': 'IT'},
            'migration_time': datetime.now() + timedelta(hours=1),
            'rollback_threshold': {'error_rate': 10, 'timeout': 30}
        }
        
        # æ‰¹æ¬¡2: ç®¡ç†å±‚ç”¨æˆ·
        batch2 = {
            'batch_id': 2,
            'name': 'ç®¡ç†å±‚ç”¨æˆ·',
            'user_filter': {'role': 'manager'},
            'migration_time': datetime.now() + timedelta(hours=4),
            'rollback_threshold': {'error_rate': 5, 'timeout': 60}
        }
        
        # æ‰¹æ¬¡3: é”€å”®éƒ¨é—¨ç”¨æˆ·
        batch3 = {
            'batch_id': 3,
            'name': 'é”€å”®éƒ¨é—¨ç”¨æˆ·',
            'user_filter': {'department': 'Sales'},
            'migration_time': datetime.now() + timedelta(hours=8),
            'rollback_threshold': {'error_rate': 3, 'timeout': 120}
        }
        
        # æ‰¹æ¬¡4: å…¶ä»–éƒ¨é—¨ç”¨æˆ·
        batch4 = {
            'batch_id': 4,
            'name': 'å…¶ä»–éƒ¨é—¨ç”¨æˆ·',
            'user_filter': {'department': ['HR', 'Finance', 'Operations']},
            'migration_time': datetime.now() + timedelta(days=1),
            'rollback_threshold': {'error_rate': 2, 'timeout': 180}
        }
        
        self.migration_batches = [batch1, batch2, batch3, batch4]
        logger.info(f"åˆ›å»ºäº† {len(self.migration_batches)} ä¸ªè¿ç§»æ‰¹æ¬¡")
        
    def migrate_user_batch(self, batch):
        """è¿ç§»æŒ‡å®šæ‰¹æ¬¡çš„ç”¨æˆ·"""
        
        logger.info(f"å¼€å§‹è¿ç§»æ‰¹æ¬¡: {batch['name']}")
        
        try:
            # 1. è·å–æ‰¹æ¬¡ç”¨æˆ·åˆ—è¡¨
            users = self.get_batch_users(batch['user_filter'])
            logger.info(f"æ‰¹æ¬¡ {batch['name']} åŒ…å« {len(users)} ä¸ªç”¨æˆ·")
            
            # 2. æ›´æ–°ç”¨æˆ·è·¯ç”±é…ç½®
            for user in users:
                self.update_user_routing(user['user_id'], 'new_system')
                logger.debug(f"ç”¨æˆ· {user['username']} å·²åˆ‡æ¢åˆ°æ–°ç³»ç»Ÿ")
            
            # 3. ç›‘æ§è¿ç§»æ•ˆæœ
            monitoring_result = self.monitor_batch_migration(batch, users)
            
            # 4. åˆ¤æ–­æ˜¯å¦éœ€è¦å›æ»š
            if self.should_rollback(monitoring_result, batch['rollback_threshold']):
                logger.warning(f"æ‰¹æ¬¡ {batch['name']} éœ€è¦å›æ»š")
                self.rollback_user_batch(batch, users)
                return False
            else:
                logger.info(f"æ‰¹æ¬¡ {batch['name']} è¿ç§»æˆåŠŸ")
                self.mark_batch_completed(batch['batch_id'])
                return True
                
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch['name']} è¿ç§»å¤±è´¥: {str(e)}")
            self.rollback_user_batch(batch, users)
            return False
    
    def get_batch_users(self, user_filter):
        """æ ¹æ®è¿‡æ»¤æ¡ä»¶è·å–ç”¨æˆ·åˆ—è¡¨"""
        
        response = requests.post(
            f"{self.api_base_url}/users/filter",
            headers={'Authorization': f'Bearer {self.api_token}'},
            json=user_filter
        )
        
        if response.status_code == 200:
            return response.json()['users']
        else:
            raise Exception(f"è·å–ç”¨æˆ·åˆ—è¡¨å¤±è´¥: {response.text}")
    
    def update_user_routing(self, user_id, target_system):
        """æ›´æ–°ç”¨æˆ·è·¯ç”±é…ç½®"""
        
        response = requests.put(
            f"{self.api_base_url}/routing/user/{user_id}",
            headers={'Authorization': f'Bearer {self.api_token}'},
            json={'target_system': target_system}
        )
        
        if response.status_code != 200:
            raise Exception(f"æ›´æ–°ç”¨æˆ·è·¯ç”±å¤±è´¥: {response.text}")
    
    def monitor_batch_migration(self, batch, users):
        """ç›‘æ§æ‰¹æ¬¡è¿ç§»æ•ˆæœ"""
        
        logger.info(f"å¼€å§‹ç›‘æ§æ‰¹æ¬¡ {batch['name']} è¿ç§»æ•ˆæœ...")
        
        monitoring_duration = batch['rollback_threshold']['timeout']
        start_time = time.time()
        
        metrics = {
            'error_count': 0,
            'total_requests': 0,
            'avg_response_time': 0,
            'user_complaints': 0
        }
        
        while time.time() - start_time < monitoring_duration:
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            current_metrics = self.collect_metrics(users)
            
            metrics['error_count'] += current_metrics['error_count']
            metrics['total_requests'] += current_metrics['total_requests']
            metrics['avg_response_time'] = current_metrics['avg_response_time']
            metrics['user_complaints'] += current_metrics['user_complaints']
            
            # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡æŒ‡æ ‡
            logger.info(f"å½“å‰æŒ‡æ ‡ - é”™è¯¯ç‡: {(metrics['error_count']/max(metrics['total_requests'], 1))*100:.2f}%, "
                       f"å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']}ms, "
                       f"ç”¨æˆ·æŠ•è¯‰: {metrics['user_complaints']}")
            
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        
        return metrics
    
    def should_rollback(self, metrics, threshold):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å›æ»š"""
        
        error_rate = (metrics['error_count'] / max(metrics['total_requests'], 1)) * 100
        
        if error_rate > threshold['error_rate']:
            logger.warning(f"é”™è¯¯ç‡ {error_rate:.2f}% è¶…è¿‡é˜ˆå€¼ {threshold['error_rate']}%")
            return True
        
        if metrics['avg_response_time'] > 5000:  # 5ç§’
            logger.warning(f"å¹³å‡å“åº”æ—¶é—´ {metrics['avg_response_time']}ms è¿‡é•¿")
            return True
        
        if metrics['user_complaints'] > 10:
            logger.warning(f"ç”¨æˆ·æŠ•è¯‰æ•°é‡ {metrics['user_complaints']} è¿‡å¤š")
            return True
        
        return False
    
    def rollback_user_batch(self, batch, users):
        """å›æ»šç”¨æˆ·æ‰¹æ¬¡"""
        
        logger.info(f"å¼€å§‹å›æ»šæ‰¹æ¬¡: {batch['name']}")
        
        for user in users:
            self.update_user_routing(user['user_id'], 'old_system')
            logger.debug(f"ç”¨æˆ· {user['username']} å·²å›æ»šåˆ°æ—§ç³»ç»Ÿ")
        
        # å‘é€å›æ»šé€šçŸ¥
        self.send_rollback_notification(batch, users)
        
    def execute_migration_plan(self):
        """æ‰§è¡Œå®Œæ•´çš„è¿ç§»è®¡åˆ’"""
        
        logger.info("å¼€å§‹æ‰§è¡Œç”¨æˆ·è¿ç§»è®¡åˆ’...")
        
        for batch in self.migration_batches:
            # ç­‰å¾…åˆ°é¢„å®šçš„è¿ç§»æ—¶é—´
            while datetime.now() < batch['migration_time']:
                logger.info(f"ç­‰å¾…æ‰¹æ¬¡ {batch['name']} è¿ç§»æ—¶é—´...")
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
            # æ‰§è¡Œæ‰¹æ¬¡è¿ç§»
            success = self.migrate_user_batch(batch)
            
            if not success:
                logger.error(f"æ‰¹æ¬¡ {batch['name']} è¿ç§»å¤±è´¥ï¼Œæš‚åœåç»­è¿ç§»")
                break
            
            # æ‰¹æ¬¡é—´é—´éš”
            logger.info("ç­‰å¾…ä¸‹ä¸€ä¸ªæ‰¹æ¬¡...")
            time.sleep(3600)  # 1å°æ—¶é—´éš”
        
        logger.info("ç”¨æˆ·è¿ç§»è®¡åˆ’æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    manager = UserMigrationManager()
    manager.create_migration_batches()
    manager.execute_migration_plan()
```

## 2. è¿è¡Œç›‘æ§ç®¡ç†

### 2.1 å®æ—¶ç›‘æ§ç³»ç»Ÿ

#### ç›‘æ§ä»ªè¡¨æ¿é…ç½®
```yaml
# Grafanaç›‘æ§ä»ªè¡¨æ¿é…ç½®
Grafanaä»ªè¡¨æ¿:
  ç³»ç»Ÿæ¦‚è§ˆä»ªè¡¨æ¿:
    åç§°: "ITåŸºç¡€è®¾æ–½æ€»è§ˆ"
    åˆ·æ–°é—´éš”: 30ç§’
    
    é¢æ¿é…ç½®:
      æœåŠ¡çŠ¶æ€é¢æ¿:
        ç±»å‹: "çŠ¶æ€é¢æ¿"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          up{job=~"web-server|database|cache"}
        æ˜¾ç¤º: æœåŠ¡åœ¨çº¿çŠ¶æ€
        
      ç³»ç»Ÿè´Ÿè½½é¢æ¿:
        ç±»å‹: "æ—¶é—´åºåˆ—å›¾"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          avg(node_load1{instance=~".*"})
          avg(node_load5{instance=~".*"})
          avg(node_load15{instance=~".*"})
        æ ‡é¢˜: "ç³»ç»Ÿå¹³å‡è´Ÿè½½"
        
      CPUä½¿ç”¨ç‡é¢æ¿:
        ç±»å‹: "ç»Ÿè®¡é¢æ¿"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
        é˜ˆå€¼: 
          - ç»¿è‰²: 0-70%
          - é»„è‰²: 70-85%
          - çº¢è‰²: 85-100%
          
      å†…å­˜ä½¿ç”¨ç‡é¢æ¿:
        ç±»å‹: "é‡è¡¨"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
        é˜ˆå€¼: 
          - ç»¿è‰²: 0-75%
          - é»„è‰²: 75-90%
          - çº¢è‰²: 90-100%
          
      ç£ç›˜ä½¿ç”¨ç‡é¢æ¿:
        ç±»å‹: "æŸ±çŠ¶å›¾"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"}) * 100)
          
      ç½‘ç»œæµé‡é¢æ¿:
        ç±»å‹: "æ—¶é—´åºåˆ—å›¾"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          irate(node_network_receive_bytes_total{device!="lo"}[5m]) * 8
          irate(node_network_transmit_bytes_total{device!="lo"}[5m]) * 8
        å•ä½: "bps"
        
      åº”ç”¨å“åº”æ—¶é—´é¢æ¿:
        ç±»å‹: "æ—¶é—´åºåˆ—å›¾"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
        æ ‡é¢˜: "95%å“åº”æ—¶é—´"
        
      æ•°æ®åº“è¿æ¥æ•°é¢æ¿:
        ç±»å‹: "å•ä¸€ç»Ÿè®¡"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          postgresql_total_connections
          mysql_global_status_threads_connected
          
      é”™è¯¯ç‡é¢æ¿:
        ç±»å‹: "æ—¶é—´åºåˆ—å›¾"
        æ•°æ®æº: "Prometheus"
        æŸ¥è¯¢: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100
        é˜ˆå€¼çº¿: 5%

  å‘Šè­¦è§„åˆ™é…ç½®:
    CPUä½¿ç”¨ç‡å‘Šè­¦:
      è¡¨è¾¾å¼: |
        100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
      æŒç»­æ—¶é—´: 5åˆ†é’Ÿ
      ä¸¥é‡çº§åˆ«: è­¦å‘Š
      
    å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦:
      è¡¨è¾¾å¼: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      æŒç»­æ—¶é—´: 3åˆ†é’Ÿ
      ä¸¥é‡çº§åˆ«: ä¸¥é‡
      
    ç£ç›˜ç©ºé—´å‘Šè­¦:
      è¡¨è¾¾å¼: |
        100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"}) * 100) > 85
      æŒç»­æ—¶é—´: 1åˆ†é’Ÿ
      ä¸¥é‡çº§åˆ«: è­¦å‘Š
      
    æœåŠ¡å®•æœºå‘Šè­¦:
      è¡¨è¾¾å¼: |
        up{job=~"web-server|database|cache"} == 0
      æŒç»­æ—¶é—´: 30ç§’
      ä¸¥é‡çº§åˆ«: ç´§æ€¥
      
    å“åº”æ—¶é—´å‘Šè­¦:
      è¡¨è¾¾å¼: |
        histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 3
      æŒç»­æ—¶é—´: 2åˆ†é’Ÿ
      ä¸¥é‡çº§åˆ«: è­¦å‘Š
```

#### è‡ªåŠ¨åŒ–ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# è‡ªåŠ¨åŒ–ç›‘æ§æ£€æŸ¥è„šæœ¬

# é…ç½®å‚æ•°
MONITORING_LOG="/var/log/infrastructure-monitoring.log"
ALERT_WEBHOOK="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
EMAIL_RECIPIENTS="admin@company.local,ops@company.local"

# æ—¥å¿—å‡½æ•°
log_message() {
    local level=$1
    local message=$2
    echo "$(date '+%Y-%m-%d %H:%M:%S') [$level] $message" | tee -a $MONITORING_LOG
}

# å‘é€å‘Šè­¦
send_alert() {
    local title=$1
    local message=$2
    local severity=$3
    
    # å‘é€Slacké€šçŸ¥
    curl -X POST $ALERT_WEBHOOK \
         -H 'Content-Type: application/json' \
         -d "{
           \"text\": \"$title\",
           \"attachments\": [{
             \"color\": \"danger\",
             \"fields\": [{
               \"title\": \"è¯¦ç»†ä¿¡æ¯\",
               \"value\": \"$message\",
               \"short\": false
             }]
           }]
         }"
    
    # å‘é€é‚®ä»¶é€šçŸ¥
    echo "$message" | mail -s "$title" $EMAIL_RECIPIENTS
    
    log_message "ALERT" "$title: $message"
}

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
check_services() {
    local failed_services=()
    
    services=("nginx" "postgresql" "redis-server" "docker")
    
    for service in "${services[@]}"; do
        if ! systemctl is-active --quiet $service; then
            failed_services+=($service)
        fi
    done
    
    if [ ${#failed_services[@]} -gt 0 ]; then
        send_alert "æœåŠ¡çŠ¶æ€å¼‚å¸¸" "ä»¥ä¸‹æœåŠ¡æœªè¿è¡Œ: ${failed_services[*]}" "HIGH"
        return 1
    else
        log_message "INFO" "æ‰€æœ‰å…³é”®æœåŠ¡è¿è¡Œæ­£å¸¸"
        return 0
    fi
}

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
check_system_resources() {
    local alerts=()
    
    # æ£€æŸ¥CPUä½¿ç”¨ç‡
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    cpu_usage=${cpu_usage%.*}  # å»æ‰å°æ•°éƒ¨åˆ†
    
    if [ "$cpu_usage" -gt 85 ]; then
        alerts+=("CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu_usage}%")
    fi
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
    mem_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    mem_usage=${mem_usage%.*}
    
    if [ "$mem_usage" -gt 90 ]; then
        alerts+=("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${mem_usage}%")
    fi
    
    # æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
    while read line; do
        usage=$(echo $line | awk '{print $5}' | sed 's/%//')
        filesystem=$(echo $line | awk '{print $6}')
        
        if [ "$usage" -gt 85 ]; then
            alerts+=("ç£ç›˜ç©ºé—´ä¸è¶³: $filesystem ä½¿ç”¨ç‡ ${usage}%")
        fi
    done < <(df -h | grep -vE '^Filesystem|tmpfs|cdrom')
    
    # å‘é€å‘Šè­¦
    if [ ${#alerts[@]} -gt 0 ]; then
        alert_message=$(printf '%s\n' "${alerts[@]}")
        send_alert "ç³»ç»Ÿèµ„æºå‘Šè­¦" "$alert_message" "MEDIUM"
        return 1
    else
        log_message "INFO" "ç³»ç»Ÿèµ„æºä½¿ç”¨æ­£å¸¸"
        return 0
    fi
}

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
check_network_connectivity() {
    local failed_hosts=()
    
    # å…³é”®ä¸»æœºåˆ—è¡¨
    hosts=("192.168.1.1" "192.168.10.21" "192.168.10.22" "8.8.8.8")
    
    for host in "${hosts[@]}"; do
        if ! ping -c 3 -W 5 $host >/dev/null 2>&1; then
            failed_hosts+=($host)
        fi
    done
    
    if [ ${#failed_hosts[@]} -gt 0 ]; then
        send_alert "ç½‘ç»œè¿é€šæ€§å¼‚å¸¸" "æ— æ³•è¿æ¥åˆ°ä¸»æœº: ${failed_hosts[*]}" "HIGH"
        return 1
    else
        log_message "INFO" "ç½‘ç»œè¿é€šæ€§æ­£å¸¸"
        return 0
    fi
}

# æ£€æŸ¥Webåº”ç”¨å¥åº·çŠ¶æ€
check_web_health() {
    local failed_checks=()
    
    # Webå¥åº·æ£€æŸ¥ç«¯ç‚¹
    endpoints=(
        "https://app.company.local/health"
        "https://api.company.local/health"
        "https://admin.company.local/health"
    )
    
    for endpoint in "${endpoints[@]}"; do
        response=$(curl -s -w "%{http_code}" -o /dev/null --max-time 10 $endpoint)
        
        if [ "$response" != "200" ]; then
            failed_checks+=("$endpoint è¿”å›çŠ¶æ€ç : $response")
        fi
    done
    
    if [ ${#failed_checks[@]} -gt 0 ]; then
        alert_message=$(printf '%s\n' "${failed_checks[@]}")
        send_alert "Webåº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥" "$alert_message" "HIGH"
        return 1
    else
        log_message "INFO" "Webåº”ç”¨å¥åº·æ£€æŸ¥é€šè¿‡"
        return 0
    fi
}

# æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
check_database_status() {
    local db_alerts=()
    
    # æ£€æŸ¥PostgreSQLä¸»åº“
    if ! sudo -u postgres psql -c "SELECT 1;" >/dev/null 2>&1; then
        db_alerts+=("PostgreSQLä¸»åº“è¿æ¥å¤±è´¥")
    fi
    
    # æ£€æŸ¥PostgreSQLä»åº“
    if ! sudo -u postgres psql -h 192.168.10.22 -c "SELECT 1;" >/dev/null 2>&1; then
        db_alerts+=("PostgreSQLä»åº“è¿æ¥å¤±è´¥")
    fi
    
    # æ£€æŸ¥Redis
    if ! redis-cli -a "RedisPassword123!" ping >/dev/null 2>&1; then
        db_alerts+=("Redisè¿æ¥å¤±è´¥")
    fi
    
    if [ ${#db_alerts[@]} -gt 0 ]; then
        alert_message=$(printf '%s\n' "${db_alerts[@]}")
        send_alert "æ•°æ®åº“çŠ¶æ€å¼‚å¸¸" "$alert_message" "HIGH"
        return 1
    else
        log_message "INFO" "æ•°æ®åº“çŠ¶æ€æ­£å¸¸"
        return 0
    fi
}

# ç”Ÿæˆç›‘æ§æŠ¥å‘Š
generate_monitoring_report() {
    local report_date=$(date '+%Y-%m-%d')
    local report_file="/var/log/daily-monitoring-report-$report_date.txt"
    
    cat > $report_file << EOF
=====================================
ITåŸºç¡€è®¾æ–½æ—¥å¸¸ç›‘æ§æŠ¥å‘Š
æ—¥æœŸ: $report_date
=====================================

ç³»ç»Ÿè¿è¡Œæ—¶é—´:
$(uptime)

ç³»ç»Ÿè´Ÿè½½:
$(cat /proc/loadavg)

å†…å­˜ä½¿ç”¨æƒ…å†µ:
$(free -h)

ç£ç›˜ä½¿ç”¨æƒ…å†µ:
$(df -h)

ç½‘ç»œæ¥å£çŠ¶æ€:
$(ip addr show | grep -E "inet |state ")

æœåŠ¡è¿è¡ŒçŠ¶æ€:
$(systemctl is-active nginx postgresql redis-server docker)

æœ€è¿‘é”™è¯¯æ—¥å¿— (æœ€æ–°10æ¡):
$(tail -10 /var/log/syslog | grep -i error)

æ•°æ®åº“è¿æ¥æ•°:
PostgreSQL: $(sudo -u postgres psql -t -c "SELECT count(*) FROM pg_stat_activity;")
Redis: $(redis-cli -a "RedisPassword123!" info clients | grep connected_clients)

å¤‡ä»½çŠ¶æ€:
$(ls -la /backup/ | tail -5)

=====================================
EOF
    
    log_message "INFO" "ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file"
    
    # å‘é€æŠ¥å‘Šé‚®ä»¶
    mail -s "ITåŸºç¡€è®¾æ–½æ—¥å¸¸ç›‘æ§æŠ¥å‘Š - $report_date" $EMAIL_RECIPIENTS < $report_file
}

# ä¸»ç›‘æ§å‡½æ•°
main_monitoring() {
    log_message "INFO" "å¼€å§‹ç³»ç»Ÿç›‘æ§æ£€æŸ¥..."
    
    local check_results=0
    
    check_services || ((check_results++))
    check_system_resources || ((check_results++))
    check_network_connectivity || ((check_results++))
    check_web_health || ((check_results++))
    check_database_status || ((check_results++))
    
    if [ $check_results -eq 0 ]; then
        log_message "INFO" "æ‰€æœ‰ç›‘æ§æ£€æŸ¥é€šè¿‡"
    else
        log_message "WARNING" "å‘ç° $check_results ä¸ªé—®é¢˜"
    fi
    
    # ç”Ÿæˆæ—¥æŠ¥å‘Š (æ¯å¤©23:50æ‰§è¡Œ)
    if [ "$(date '+%H:%M')" = "23:50" ]; then
        generate_monitoring_report
    fi
}

# æ‰§è¡Œç›‘æ§
main_monitoring
```

### 2.2 æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

#### æ€§èƒ½åŸºçº¿å»ºç«‹
```python
#!/usr/bin/env python3
# æ€§èƒ½åŸºçº¿å»ºç«‹è„šæœ¬

import psutil
import requests
import time
import json
import statistics
from datetime import datetime, timedelta
import mysql.connector
import psycopg2

class PerformanceBaseline:
    def __init__(self):
        self.metrics = {
            'system': {},
            'application': {},
            'database': {},
            'network': {}
        }
        
    def collect_system_metrics(self, duration_minutes=60):
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        
        print(f"å¼€å§‹æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ï¼ŒæŒç»­ {duration_minutes} åˆ†é’Ÿ...")
        
        cpu_samples = []
        memory_samples = []
        disk_samples = []
        
        samples = duration_minutes * 6  # æ¯10ç§’é‡‡æ ·ä¸€æ¬¡
        
        for i in range(samples):
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_samples.append(cpu_percent)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_samples.append(memory.percent)
            
            # ç£ç›˜I/O
            disk_io = psutil.disk_io_counters()
            disk_samples.append({
                'read_bytes': disk_io.read_bytes,
                'write_bytes': disk_io.write_bytes,
                'read_time': disk_io.read_time,
                'write_time': disk_io.write_time
            })
            
            if i % 60 == 0:  # æ¯10åˆ†é’Ÿæ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"å·²å®Œæˆ {i/samples*100:.1f}% ç³»ç»ŸæŒ‡æ ‡æ”¶é›†")
            
            time.sleep(10)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.metrics['system'] = {
            'cpu': {
                'mean': statistics.mean(cpu_samples),
                'median': statistics.median(cpu_samples),
                'max': max(cpu_samples),
                'min': min(cpu_samples),
                'stdev': statistics.stdev(cpu_samples) if len(cpu_samples) > 1 else 0
            },
            'memory': {
                'mean': statistics.mean(memory_samples),
                'median': statistics.median(memory_samples),
                'max': max(memory_samples),
                'min': min(memory_samples),
                'stdev': statistics.stdev(memory_samples) if len(memory_samples) > 1 else 0
            },
            'disk_io': {
                'avg_read_bytes_per_sec': statistics.mean([s['read_bytes'] for s in disk_samples]),
                'avg_write_bytes_per_sec': statistics.mean([s['write_bytes'] for s in disk_samples]),
                'avg_read_time': statistics.mean([s['read_time'] for s in disk_samples]),
                'avg_write_time': statistics.mean([s['write_time'] for s in disk_samples])
            }
        }
        
        print("âœ“ ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å®Œæˆ")
    
    def collect_application_metrics(self, duration_minutes=60):
        """æ”¶é›†åº”ç”¨æ€§èƒ½æŒ‡æ ‡"""
        
        print(f"å¼€å§‹æ”¶é›†åº”ç”¨æŒ‡æ ‡ï¼ŒæŒç»­ {duration_minutes} åˆ†é’Ÿ...")
        
        response_times = []
        error_rates = []
        throughput_samples = []
        
        test_urls = [
            'https://app.company.local/api/health',
            'https://app.company.local/api/users',
            'https://app.company.local/api/dashboard'
        ]
        
        samples = duration_minutes * 6
        
        for i in range(samples):
            request_count = 0
            error_count = 0
            total_response_time = 0
            
            # å¯¹æ¯ä¸ªURLå‘é€è¯·æ±‚
            for url in test_urls:
                try:
                    start_time = time.time()
                    response = requests.get(url, timeout=30)
                    end_time = time.time()
                    
                    response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    response_times.append(response_time)
                    total_response_time += response_time
                    request_count += 1
                    
                    if response.status_code >= 400:
                        error_count += 1
                        
                except Exception as e:
                    error_count += 1
                    request_count += 1
            
            # è®¡ç®—å½“å‰å‘¨æœŸçš„é”™è¯¯ç‡å’Œååé‡
            if request_count > 0:
                error_rate = (error_count / request_count) * 100
                error_rates.append(error_rate)
                
                throughput = request_count / 10  # æ¯ç§’è¯·æ±‚æ•°
                throughput_samples.append(throughput)
            
            if i % 60 == 0:
                print(f"å·²å®Œæˆ {i/samples*100:.1f}% åº”ç”¨æŒ‡æ ‡æ”¶é›†")
            
            time.sleep(10)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.metrics['application'] = {
            'response_time': {
                'mean': statistics.mean(response_times),
                'median': statistics.median(response_times),
                'p95': sorted(response_times)[int(len(response_times) * 0.95)],
                'p99': sorted(response_times)[int(len(response_times) * 0.99)],
                'max': max(response_times),
                'min': min(response_times)
            },
            'error_rate': {
                'mean': statistics.mean(error_rates),
                'max': max(error_rates),
                'min': min(error_rates)
            },
            'throughput': {
                'mean': statistics.mean(throughput_samples),
                'max': max(throughput_samples),
                'min': min(throughput_samples)
            }
        }
        
        print("âœ“ åº”ç”¨æŒ‡æ ‡æ”¶é›†å®Œæˆ")
    
    def collect_database_metrics(self, duration_minutes=60):
        """æ”¶é›†æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
        
        print(f"å¼€å§‹æ”¶é›†æ•°æ®åº“æŒ‡æ ‡ï¼ŒæŒç»­ {duration_minutes} åˆ†é’Ÿ...")
        
        pg_metrics = []
        redis_metrics = []
        
        samples = duration_minutes * 6
        
        for i in range(samples):
            try:
                # PostgreSQLæŒ‡æ ‡
                pg_conn = psycopg2.connect(
                    host='192.168.10.21',
                    database='companydb',
                    user='postgres',
                    password='postgres_password'
                )
                pg_cursor = pg_conn.cursor()
                
                # æŸ¥è¯¢å½“å‰è¿æ¥æ•°
                pg_cursor.execute("SELECT count(*) FROM pg_stat_activity;")
                active_connections = pg_cursor.fetchone()[0]
                
                # æŸ¥è¯¢æ•°æ®åº“å¤§å°
                pg_cursor.execute("SELECT pg_database_size('companydb');")
                db_size = pg_cursor.fetchone()[0]
                
                # æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç‡
                pg_cursor.execute("""
                    SELECT sum(blks_hit) * 100.0 / sum(blks_hit + blks_read) as cache_hit_ratio
                    FROM pg_stat_database;
                """)
                cache_hit_ratio = pg_cursor.fetchone()[0] or 0
                
                pg_metrics.append({
                    'active_connections': active_connections,
                    'db_size': db_size,
                    'cache_hit_ratio': float(cache_hit_ratio)
                })
                
                pg_conn.close()
                
                # RedisæŒ‡æ ‡
                import redis
                redis_client = redis.Redis(
                    host='192.168.10.40',
                    port=6379,
                    password='RedisPassword123!',
                    decode_responses=True
                )
                
                redis_info = redis_client.info()
                redis_metrics.append({
                    'connected_clients': redis_info['connected_clients'],
                    'used_memory': redis_info['used_memory'],
                    'total_commands_processed': redis_info['total_commands_processed'],
                    'keyspace_hits': redis_info.get('keyspace_hits', 0),
                    'keyspace_misses': redis_info.get('keyspace_misses', 0)
                })
                
            except Exception as e:
                print(f"æ•°æ®åº“æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
            
            if i % 60 == 0:
                print(f"å·²å®Œæˆ {i/samples*100:.1f}% æ•°æ®åº“æŒ‡æ ‡æ”¶é›†")
            
            time.sleep(10)
        
        # è®¡ç®—PostgreSQLç»Ÿè®¡ä¿¡æ¯
        if pg_metrics:
            self.metrics['database']['postgresql'] = {
                'avg_connections': statistics.mean([m['active_connections'] for m in pg_metrics]),
                'avg_cache_hit_ratio': statistics.mean([m['cache_hit_ratio'] for m in pg_metrics]),
                'db_size_gb': pg_metrics[-1]['db_size'] / (1024**3)
            }
        
        # è®¡ç®—Redisç»Ÿè®¡ä¿¡æ¯
        if redis_metrics:
            hit_rates = []
            for m in redis_metrics:
                total = m['keyspace_hits'] + m['keyspace_misses']
                if total > 0:
                    hit_rate = (m['keyspace_hits'] / total) * 100
                    hit_rates.append(hit_rate)
            
            self.metrics['database']['redis'] = {
                'avg_connected_clients': statistics.mean([m['connected_clients'] for m in redis_metrics]),
                'avg_memory_usage_mb': statistics.mean([m['used_memory'] for m in redis_metrics]) / (1024**2),
                'avg_hit_rate': statistics.mean(hit_rates) if hit_rates else 0
            }
        
        print("âœ“ æ•°æ®åº“æŒ‡æ ‡æ”¶é›†å®Œæˆ")
    
    def generate_baseline_report(self):
        """ç”Ÿæˆæ€§èƒ½åŸºçº¿æŠ¥å‘Š"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f'performance_baseline_{timestamp}.json'
        
        baseline_data = {
            'timestamp': datetime.now().isoformat(),
            'collection_duration': '60 minutes',
            'metrics': self.metrics,
            'thresholds': {
                'system': {
                    'cpu_warning': self.metrics['system']['cpu']['mean'] + 2 * self.metrics['system']['cpu']['stdev'],
                    'cpu_critical': self.metrics['system']['cpu']['mean'] + 3 * self.metrics['system']['cpu']['stdev'],
                    'memory_warning': min(85, self.metrics['system']['memory']['mean'] + 10),
                    'memory_critical': min(95, self.metrics['system']['memory']['mean'] + 20)
                },
                'application': {
                    'response_time_warning': self.metrics['application']['response_time']['p95'] * 1.5,
                    'response_time_critical': self.metrics['application']['response_time']['p95'] * 2,
                    'error_rate_warning': max(5, self.metrics['application']['error_rate']['mean'] * 2),
                    'error_rate_critical': max(10, self.metrics['application']['error_rate']['mean'] * 3)
                }
            }
        }
        
        # ä¿å­˜åŸºçº¿æ•°æ®
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        readable_report = f'performance_baseline_report_{timestamp}.txt'
        with open(readable_report, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("ITåŸºç¡€è®¾æ–½æ€§èƒ½åŸºçº¿æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®æ”¶é›†å‘¨æœŸ: 60åˆ†é’Ÿ\n\n")
            
            # ç³»ç»ŸæŒ‡æ ‡
            f.write("ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡:\n")
            f.write("-"*30 + "\n")
            f.write(f"CPUä½¿ç”¨ç‡ - å¹³å‡: {self.metrics['system']['cpu']['mean']:.2f}%, ")
            f.write(f"æœ€å¤§: {self.metrics['system']['cpu']['max']:.2f}%, ")
            f.write(f"æ ‡å‡†å·®: {self.metrics['system']['cpu']['stdev']:.2f}%\n")
            f.write(f"å†…å­˜ä½¿ç”¨ç‡ - å¹³å‡: {self.metrics['system']['memory']['mean']:.2f}%, ")
            f.write(f"æœ€å¤§: {self.metrics['system']['memory']['max']:.2f}%\n\n")
            
            # åº”ç”¨æŒ‡æ ‡
            f.write("åº”ç”¨æ€§èƒ½æŒ‡æ ‡:\n")
            f.write("-"*30 + "\n")
            f.write(f"å“åº”æ—¶é—´ - å¹³å‡: {self.metrics['application']['response_time']['mean']:.2f}ms, ")
            f.write(f"P95: {self.metrics['application']['response_time']['p95']:.2f}ms, ")
            f.write(f"P99: {self.metrics['application']['response_time']['p99']:.2f}ms\n")
            f.write(f"é”™è¯¯ç‡ - å¹³å‡: {self.metrics['application']['error_rate']['mean']:.4f}%\n")
            f.write(f"ååé‡ - å¹³å‡: {self.metrics['application']['throughput']['mean']:.2f} req/s\n\n")
            
            # æ•°æ®åº“æŒ‡æ ‡
            if 'postgresql' in self.metrics['database']:
                f.write("PostgreSQLæŒ‡æ ‡:\n")
                f.write("-"*30 + "\n")
                f.write(f"å¹³å‡è¿æ¥æ•°: {self.metrics['database']['postgresql']['avg_connections']:.0f}\n")
                f.write(f"ç¼“å­˜å‘½ä¸­ç‡: {self.metrics['database']['postgresql']['avg_cache_hit_ratio']:.2f}%\n")
                f.write(f"æ•°æ®åº“å¤§å°: {self.metrics['database']['postgresql']['db_size_gb']:.2f} GB\n\n")
            
            if 'redis' in self.metrics['database']:
                f.write("RedisæŒ‡æ ‡:\n")
                f.write("-"*30 + "\n")
                f.write(f"å¹³å‡è¿æ¥æ•°: {self.metrics['database']['redis']['avg_connected_clients']:.0f}\n")
                f.write(f"å†…å­˜ä½¿ç”¨: {self.metrics['database']['redis']['avg_memory_usage_mb']:.2f} MB\n")
                f.write(f"ç¼“å­˜å‘½ä¸­ç‡: {self.metrics['database']['redis']['avg_hit_rate']:.2f}%\n\n")
            
            # å‘Šè­¦é˜ˆå€¼
            f.write("å»ºè®®å‘Šè­¦é˜ˆå€¼:\n")
            f.write("-"*30 + "\n")
            f.write(f"CPUä½¿ç”¨ç‡å‘Šè­¦: {baseline_data['thresholds']['system']['cpu_warning']:.1f}%\n")
            f.write(f"CPUä½¿ç”¨ç‡ä¸¥é‡: {baseline_data['thresholds']['system']['cpu_critical']:.1f}%\n")
            f.write(f"å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦: {baseline_data['thresholds']['system']['memory_warning']:.1f}%\n")
            f.write(f"å“åº”æ—¶é—´å‘Šè­¦: {baseline_data['thresholds']['application']['response_time_warning']:.0f}ms\n")
            f.write(f"é”™è¯¯ç‡å‘Šè­¦: {baseline_data['thresholds']['application']['error_rate_warning']:.2f}%\n")
        
        print(f"âœ“ æ€§èƒ½åŸºçº¿æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  - JSONæ•°æ®: {report_file}")
        print(f"  - å¯è¯»æŠ¥å‘Š: {readable_report}")
        
        return baseline_data

    def run_baseline_collection(self):
        """æ‰§è¡Œå®Œæ•´çš„åŸºçº¿æ”¶é›†"""
        
        print("å¼€å§‹ITåŸºç¡€è®¾æ–½æ€§èƒ½åŸºçº¿å»ºç«‹...")
        print("é¢„è®¡æ€»è€—æ—¶: 3å°æ—¶ (æ¯ç±»æŒ‡æ ‡1å°æ—¶)")
        
        # å¹¶è¡Œæ”¶é›†ä¸åŒç±»å‹çš„æŒ‡æ ‡å¯èƒ½ä¼šäº’ç›¸å½±å“ï¼Œæ‰€ä»¥ä¸²è¡Œæ‰§è¡Œ
        self.collect_system_metrics(60)
        self.collect_application_metrics(60)
        self.collect_database_metrics(60)
        
        baseline_data = self.generate_baseline_report()
        
        print("âœ“ æ€§èƒ½åŸºçº¿å»ºç«‹å®Œæˆï¼")
        return baseline_data

if __name__ == "__main__":
    baseline = PerformanceBaseline()
    baseline.run_baseline_collection()
```

## 3. é—®é¢˜å¤„ç†ç®¡ç†

### 3.1 æ•…éšœå“åº”æµç¨‹

#### æ•…éšœåˆ†çº§ä¸å“åº”
```yaml
æ•…éšœåˆ†çº§æ ‡å‡†:
  P0çº§æ•…éšœ (ç´§æ€¥):
    å®šä¹‰: æ ¸å¿ƒä¸šåŠ¡ç³»ç»Ÿå®Œå…¨ä¸å¯ç”¨
    å½±å“: è¶…è¿‡50%ç”¨æˆ·æ— æ³•æ­£å¸¸å·¥ä½œ
    å“åº”æ—¶é—´: 15åˆ†é’Ÿå†…
    è§£å†³æ—¶é—´: 4å°æ—¶å†…
    é€šçŸ¥å¯¹è±¡: 
      - æŠ€æœ¯æ€»ç›‘
      - è¿ç»´ä¸»ç®¡
      - å€¼ç­å·¥ç¨‹å¸ˆ
      - ä¸šåŠ¡éƒ¨é—¨è´Ÿè´£äºº
    
  P1çº§æ•…éšœ (é«˜):
    å®šä¹‰: æ ¸å¿ƒåŠŸèƒ½å—åˆ°ä¸¥é‡å½±å“
    å½±å“: 20-50%ç”¨æˆ·å·¥ä½œå—å½±å“
    å“åº”æ—¶é—´: 30åˆ†é’Ÿå†…
    è§£å†³æ—¶é—´: 8å°æ—¶å†…
    é€šçŸ¥å¯¹è±¡:
      - è¿ç»´ä¸»ç®¡
      - å€¼ç­å·¥ç¨‹å¸ˆ
      - ç›¸å…³ä¸šåŠ¡è´Ÿè´£äºº
    
  P2çº§æ•…éšœ (ä¸­):
    å®šä¹‰: éƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸æˆ–æ€§èƒ½ä¸‹é™
    å½±å“: å°‘äº20%ç”¨æˆ·å·¥ä½œå—å½±å“
    å“åº”æ—¶é—´: 2å°æ—¶å†…
    è§£å†³æ—¶é—´: 24å°æ—¶å†…
    é€šçŸ¥å¯¹è±¡:
      - å€¼ç­å·¥ç¨‹å¸ˆ
      - ç›¸å…³æŠ€æœ¯äººå‘˜
    
  P3çº§æ•…éšœ (ä½):
    å®šä¹‰: è½»å¾®é—®é¢˜æˆ–åŠŸèƒ½ç¼ºé™·
    å½±å“: ä¸å½±å“æ­£å¸¸ä¸šåŠ¡
    å“åº”æ—¶é—´: 4å°æ—¶å†…
    è§£å†³æ—¶é—´: 72å°æ—¶å†…
    é€šçŸ¥å¯¹è±¡:
      - ç›¸å…³æŠ€æœ¯äººå‘˜

æ•…éšœå¤„ç†æµç¨‹:
  1. æ•…éšœå‘ç°:
     - ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨å‘ç°
     - ç”¨æˆ·æŠ¥å‘Š
     - å·¡æ£€å‘ç°
     
  2. æ•…éšœç¡®è®¤:
     - åˆæ­¥è¯Šæ–­
     - å½±å“èŒƒå›´è¯„ä¼°
     - æ•…éšœåˆ†çº§
     
  3. åº”æ€¥å“åº”:
     - é€šçŸ¥ç›¸å…³äººå‘˜
     - å¯åŠ¨åº”æ€¥é¢„æ¡ˆ
     - ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
     
  4. æ ¹å› åˆ†æ:
     - è¯¦ç»†è°ƒæŸ¥
     - ç¡®å®šæ ¹æœ¬åŸå› 
     - åˆ¶å®šæ°¸ä¹…è§£å†³æ–¹æ¡ˆ
     
  5. é—®é¢˜ä¿®å¤:
     - å®æ–½ä¿®å¤æ–¹æ¡ˆ
     - æµ‹è¯•éªŒè¯
     - æ¢å¤æ­£å¸¸æœåŠ¡
     
  6. æ€»ç»“æ”¹è¿›:
     - æ•…éšœæŠ¥å‘Šç¼–å†™
     - æµç¨‹æ”¹è¿›å»ºè®®
     - é¢„é˜²æªæ–½åˆ¶å®š
```

#### è‡ªåŠ¨åŒ–æ•…éšœå¤„ç†è„šæœ¬
```python
#!/usr/bin/env python3
# è‡ªåŠ¨åŒ–æ•…éšœå¤„ç†ç³»ç»Ÿ

import json
import time
import requests
import subprocess
import logging
from datetime import datetime
from enum import Enum

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/incident-response.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IncidentSeverity(Enum):
    P0 = "P0-ç´§æ€¥"
    P1 = "P1-é«˜"
    P2 = "P2-ä¸­" 
    P3 = "P3-ä½"

class IncidentStatus(Enum):
    NEW = "æ–°å»º"
    ACKNOWLEDGED = "å·²ç¡®è®¤"
    IN_PROGRESS = "å¤„ç†ä¸­"
    RESOLVED = "å·²è§£å†³"
    CLOSED = "å·²å…³é—­"

class AutoIncidentResponse:
    def __init__(self):
        self.slack_webhook = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
        self.email_api = "https://api.company.local/send-email"
        self.api_token = "your_api_token"
        
    def create_incident(self, alert_data):
        """åˆ›å»ºæ•…éšœå·¥å•"""
        
        incident = {
            'id': f"INC-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'title': alert_data.get('title', 'æœªçŸ¥æ•…éšœ'),
            'description': alert_data.get('description', ''),
            'severity': self.determine_severity(alert_data),
            'status': IncidentStatus.NEW,
            'created_at': datetime.now().isoformat(),
            'assigned_to': self.get_on_call_engineer(),
            'affected_services': alert_data.get('services', []),
            'metrics': alert_data.get('metrics', {})
        }
        
        logger.info(f"åˆ›å»ºæ•…éšœå·¥å•: {incident['id']} - {incident['title']}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“/æ–‡ä»¶
        self.save_incident(incident)
        
        # å‘é€é€šçŸ¥
        self.notify_incident_created(incident)
        
        # æ‰§è¡Œè‡ªåŠ¨åŒ–å“åº”
        self.execute_auto_response(incident)
        
        return incident
    
    def determine_severity(self, alert_data):
        """æ ¹æ®å‘Šè­¦æ•°æ®ç¡®å®šæ•…éšœçº§åˆ«"""
        
        alert_type = alert_data.get('type', '')
        metrics = alert_data.get('metrics', {})
        
        # P0çº§æ•…éšœæ¡ä»¶
        if (alert_type == 'service_down' and 
            'web-server' in alert_data.get('services', [])):
            return IncidentSeverity.P0
        
        if (alert_type == 'database_down' or
            metrics.get('error_rate', 0) > 50):
            return IncidentSeverity.P0
        
        # P1çº§æ•…éšœæ¡ä»¶  
        if (metrics.get('response_time', 0) > 10000 or
            metrics.get('error_rate', 0) > 20):
            return IncidentSeverity.P1
        
        if alert_type == 'high_cpu' and metrics.get('cpu_usage', 0) > 95:
            return IncidentSeverity.P1
        
        # P2çº§æ•…éšœæ¡ä»¶
        if (metrics.get('response_time', 0) > 5000 or
            metrics.get('error_rate', 0) > 5):
            return IncidentSeverity.P2
        
        # é»˜è®¤P3çº§
        return IncidentSeverity.P3
    
    def get_on_call_engineer(self):
        """è·å–å½“å‰å€¼ç­å·¥ç¨‹å¸ˆ"""
        
        # è¿™é‡Œå¯ä»¥é›†æˆå€¼ç­è½®æ›¿ç³»ç»Ÿ
        # ç®€åŒ–å®ç°ï¼Œè¿”å›å›ºå®šå€¼ç­è¡¨
        hour = datetime.now().hour
        
        if 9 <= hour < 18:  # å·¥ä½œæ—¶é—´
            return "day-shift@company.local"
        else:  # éå·¥ä½œæ—¶é—´
            return "night-shift@company.local"
    
    def execute_auto_response(self, incident):
        """æ‰§è¡Œè‡ªåŠ¨åŒ–å“åº”æªæ–½"""
        
        logger.info(f"æ‰§è¡Œè‡ªåŠ¨åŒ–å“åº”: {incident['id']}")
        
        if incident['severity'] == IncidentSeverity.P0:
            self.handle_p0_incident(incident)
        elif incident['severity'] == IncidentSeverity.P1:
            self.handle_p1_incident(incident)
        elif incident['severity'] == IncidentSeverity.P2:
            self.handle_p2_incident(incident)
        else:
            self.handle_p3_incident(incident)
    
    def handle_p0_incident(self, incident):
        """å¤„ç†P0çº§æ•…éšœ"""
        
        logger.critical(f"å¤„ç†P0çº§æ•…éšœ: {incident['title']}")
        
        # 1. ç«‹å³é€šçŸ¥å…³é”®äººå‘˜
        self.send_critical_notification(incident)
        
        # 2. å¯åŠ¨åº”æ€¥å“åº”
        if 'web-server' in incident['affected_services']:
            self.restart_web_services()
            
        if 'database' in incident['affected_services']:
            self.check_database_status()
            
        # 3. å¯ç”¨å¤‡ç”¨ç³»ç»Ÿ
        if 'web-server' in incident['affected_services']:
            self.activate_backup_servers()
        
        # 4. åˆ›å»ºä½œæˆ˜å®¤
        self.create_war_room(incident)
    
    def handle_p1_incident(self, incident):
        """å¤„ç†P1çº§æ•…éšœ"""
        
        logger.error(f"å¤„ç†P1çº§æ•…éšœ: {incident['title']}")
        
        # 1. é€šçŸ¥ç›¸å…³äººå‘˜
        self.send_high_priority_notification(incident)
        
        # 2. æ‰§è¡ŒåŸºç¡€æ¢å¤æ“ä½œ
        if 'high_cpu' in incident['title'].lower():
            self.investigate_high_cpu()
            
        if 'slow_response' in incident['title'].lower():
            self.optimize_performance()
    
    def handle_p2_incident(self, incident):
        """å¤„ç†P2çº§æ•…éšœ"""
        
        logger.warning(f"å¤„ç†P2çº§æ•…éšœ: {incident['title']}")
        
        # æ”¶é›†è¯¦ç»†ä¿¡æ¯ç”¨äºåç»­åˆ†æ
        self.collect_diagnostic_data(incident)
        
        # é€šçŸ¥å€¼ç­å·¥ç¨‹å¸ˆ
        self.send_standard_notification(incident)
    
    def handle_p3_incident(self, incident):
        """å¤„ç†P3çº§æ•…éšœ"""
        
        logger.info(f"å¤„ç†P3çº§æ•…éšœ: {incident['title']}")
        
        # ä»…è®°å½•ï¼Œå·¥ä½œæ—¶é—´å¤„ç†
        self.log_for_business_hours(incident)
    
    def restart_web_services(self):
        """é‡å¯WebæœåŠ¡"""
        
        try:
            logger.info("å°è¯•é‡å¯WebæœåŠ¡...")
            
            # é‡å¯Nginx
            subprocess.run(['sudo', 'systemctl', 'restart', 'nginx'], 
                         check=True, timeout=30)
            
            # é‡å¯åº”ç”¨æœåŠ¡
            subprocess.run(['sudo', 'systemctl', 'restart', 'webapp'], 
                         check=True, timeout=60)
            
            # éªŒè¯æœåŠ¡çŠ¶æ€
            time.sleep(10)
            response = requests.get('https://app.company.local/health', timeout=10)
            
            if response.status_code == 200:
                logger.info("âœ“ WebæœåŠ¡é‡å¯æˆåŠŸ")
                return True
            else:
                logger.error(f"âœ— WebæœåŠ¡é‡å¯åå¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"WebæœåŠ¡é‡å¯å¤±è´¥: {str(e)}")
            return False
    
    def check_database_status(self):
        """æ£€æŸ¥æ•°æ®åº“çŠ¶æ€"""
        
        try:
            import psycopg2
            
            # æ£€æŸ¥ä¸»æ•°æ®åº“
            conn = psycopg2.connect(
                host='192.168.10.21',
                database='companydb',
                user='postgres',
                password='postgres_password',
                connect_timeout=10
            )
            
            cursor = conn.cursor()
            cursor.execute('SELECT 1;')
            result = cursor.fetchone()
            conn.close()
            
            if result:
                logger.info("âœ“ ä¸»æ•°æ®åº“è¿æ¥æ­£å¸¸")
                return True
            else:
                logger.error("âœ— ä¸»æ•°æ®åº“æŸ¥è¯¢å¼‚å¸¸")
                return False
                
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            
            # å°è¯•åˆ‡æ¢åˆ°ä»æ•°æ®åº“
            self.failover_to_slave_db()
            return False
    
    def activate_backup_servers(self):
        """æ¿€æ´»å¤‡ç”¨æœåŠ¡å™¨"""
        
        try:
            # é€šè¿‡è´Ÿè½½å‡è¡¡å™¨APIæ¿€æ´»å¤‡ç”¨æœåŠ¡å™¨
            response = requests.post(
                'https://lb.company.local/api/activate-backup',
                headers={'Authorization': f'Bearer {self.api_token}'},
                json={'backup_servers': ['192.168.10.50', '192.168.10.51']},
                timeout=30
            )
            
            if response.status_code == 200:
                logger.info("âœ“ å¤‡ç”¨æœåŠ¡å™¨å·²æ¿€æ´»")
                return True
            else:
                logger.error(f"æ¿€æ´»å¤‡ç”¨æœåŠ¡å™¨å¤±è´¥: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"æ¿€æ´»å¤‡ç”¨æœåŠ¡å™¨å¼‚å¸¸: {str(e)}")
            return False
    
    def send_critical_notification(self, incident):
        """å‘é€ç´§æ€¥é€šçŸ¥"""
        
        message = f"""
ğŸš¨ P0çº§æ•…éšœå‘Šè­¦ ğŸš¨

æ•…éšœID: {incident['id']}
æ•…éšœæ ‡é¢˜: {incident['title']}
å½±å“æœåŠ¡: {', '.join(incident['affected_services'])}
åˆ›å»ºæ—¶é—´: {incident['created_at']}
å€¼ç­å·¥ç¨‹å¸ˆ: {incident['assigned_to']}

è¯·ç«‹å³å¤„ç†ï¼
        """
        
        # å‘é€Slacké€šçŸ¥
        self.send_slack_notification(message, urgent=True)
        
        # å‘é€é‚®ä»¶é€šçŸ¥
        recipients = [
            'cto@company.local',
            'ops-manager@company.local',
            incident['assigned_to']
        ]
        self.send_email_notification(recipients, f"P0çº§æ•…éšœ: {incident['title']}", message)
        
        # å‘é€çŸ­ä¿¡é€šçŸ¥ (å¦‚æœé…ç½®)
        self.send_sms_notification(incident)
    
    def send_slack_notification(self, message, urgent=False):
        """å‘é€Slacké€šçŸ¥"""
        
        color = "danger" if urgent else "warning"
        
        payload = {
            "text": "æ•…éšœå‘Šè­¦",
            "attachments": [{
                "color": color,
                "text": message,
                "fields": [{
                    "title": "å¤„ç†è¦æ±‚",
                    "value": "è¯·ç«‹å³å“åº”" if urgent else "è¯·åŠæ—¶å¤„ç†",
                    "short": True
                }]
            }]
        }
        
        try:
            response = requests.post(self.slack_webhook, json=payload, timeout=10)
            if response.status_code == 200:
                logger.info("âœ“ Slacké€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"Slacké€šçŸ¥å‘é€å¤±è´¥: {response.text}")
        except Exception as e:
            logger.error(f"Slacké€šçŸ¥å‘é€å¼‚å¸¸: {str(e)}")
    
    def collect_diagnostic_data(self, incident):
        """æ”¶é›†è¯Šæ–­æ•°æ®"""
        
        logger.info(f"æ”¶é›†æ•…éšœè¯Šæ–­æ•°æ®: {incident['id']}")
        
        diagnostic_data = {
            'timestamp': datetime.now().isoformat(),
            'system_info': {},
            'application_logs': {},
            'performance_metrics': {}
        }
        
        try:
            # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
            diagnostic_data['system_info'] = {
                'uptime': subprocess.getoutput('uptime'),
                'memory': subprocess.getoutput('free -h'),
                'disk': subprocess.getoutput('df -h'),
                'processes': subprocess.getoutput('ps aux --sort=-%cpu | head -10')
            }
            
            # æ”¶é›†åº”ç”¨æ—¥å¿—
            diagnostic_data['application_logs'] = {
                'nginx_error': subprocess.getoutput('tail -50 /var/log/nginx/error.log'),
                'application': subprocess.getoutput('tail -50 /var/log/webapp/app.log'),
                'system': subprocess.getoutput('tail -50 /var/log/syslog')
            }
            
            # ä¿å­˜è¯Šæ–­æ•°æ®
            filename = f"/var/log/diagnostics/incident_{incident['id']}.json"
            with open(filename, 'w') as f:
                json.dump(diagnostic_data, f, indent=2)
            
            logger.info(f"âœ“ è¯Šæ–­æ•°æ®å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"æ”¶é›†è¯Šæ–­æ•°æ®å¤±è´¥: {str(e)}")
    
    def save_incident(self, incident):
        """ä¿å­˜æ•…éšœå·¥å•"""
        
        # ç®€åŒ–å®ç°ï¼Œä¿å­˜åˆ°JSONæ–‡ä»¶
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“
        filename = f"/var/log/incidents/{incident['id']}.json"
        
        try:
            with open(filename, 'w') as f:
                json.dump(incident, f, indent=2, default=str)
            logger.info(f"æ•…éšœå·¥å•å·²ä¿å­˜: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ•…éšœå·¥å•å¤±è´¥: {str(e)}")

# ä½¿ç”¨ç¤ºä¾‹
def handle_alert(alert_data):
    """å¤„ç†å‘Šè­¦æ•°æ®"""
    
    response_system = AutoIncidentResponse()
    incident = response_system.create_incident(alert_data)
    
    return incident

# ç¤ºä¾‹å‘Šè­¦æ•°æ®
if __name__ == "__main__":
    sample_alert = {
        'type': 'service_down',
        'title': 'WebæœåŠ¡å™¨å®•æœº',
        'description': 'ä¸»WebæœåŠ¡å™¨æ— å“åº”ï¼Œç”¨æˆ·æ— æ³•è®¿é—®åº”ç”¨',
        'services': ['web-server'],
        'metrics': {
            'error_rate': 100,
            'response_time': 0
        }
    }
    
    incident = handle_alert(sample_alert)
    print(f"æ•…éšœå¤„ç†å®Œæˆ: {incident['id']}")
```

---
*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*  
*åˆ›å»ºæ—¥æœŸï¼š2025å¹´8æœˆ*  
*è´Ÿè´£äººï¼šè¿ç»´æ”¯æŒå›¢é˜Ÿ*